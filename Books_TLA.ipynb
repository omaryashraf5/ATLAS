{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import graphlab\n",
    "import pandas\n",
    "import sklearn\n",
    "import scipy\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import string\n",
    "#xml_data = open('/home/omar/data/train-data/kitchen/positive.review').read()\n",
    "\n",
    "def xml2df(xml_data):\n",
    "    root = ET.XML(xml_data) # element tree\n",
    "    all_records = []\n",
    "    for i, child in enumerate(root):\n",
    "        record = {}\n",
    "        for subchild in child:\n",
    "            record[subchild.tag] = subchild.text\n",
    "            all_records.append(record)\n",
    "    return pd.DataFrame(all_records)\n",
    "from pathlib2 import Path\n",
    "def extract(xml_data):\n",
    "    #/home/omar/data/train-data/books/positive.review\n",
    "    #\"/home/omar/data/train-data/books/unlabeled.review\"\n",
    "    with open(xml_data) as f:\n",
    "        File = f.readlines()\n",
    "    data=[]\n",
    "    datacheck=[]\n",
    "    revtext=\"\"\n",
    "    inreview=False\n",
    "    review = False\n",
    "    #contents = Path(xml_data).read_text()\n",
    "    for l in File:  #File\n",
    "        l = l.replace('\\n', '')\n",
    "        #l = l.replace(\"'\\u001a'\",'')\n",
    "        \n",
    "    \n",
    "        #l = l.replace('\\t', '')\n",
    "        #l = l.replace('\"', '')\n",
    "        if review and \"</review_text>\" not in l:\n",
    "            l = l.replace('<Text>', '')\n",
    "            revtext+=l\n",
    "            #data.append(l)\n",
    "            \n",
    "        \n",
    "        if \"<review_text>\" in l:\n",
    "            review = True\n",
    "        if \"</review_text>\" in l:\n",
    "            data.append(revtext)\n",
    "            revtext=\"\"\n",
    "            review = False\n",
    "        \n",
    "    return data\n",
    "        \n",
    "\n",
    "\n",
    "def extract_summarized(xml_data):\n",
    "    #/home/omar/data/train-data/books/positive.review\n",
    "    #\"/home/omar/data/train-data/books/unlabeled.review\"\n",
    "    with open(xml_data) as f:\n",
    "        File = f.readlines()\n",
    "    data=[]\n",
    "    revtext=\"\"\n",
    "    inreview=False\n",
    "    for l in File:\n",
    "        l = l.replace('\\n', '')\n",
    "        \n",
    "        \n",
    "        ##if '<' not in l and '>' not in l and not l.isdigit():\n",
    "            ##data.append(l)\n",
    "        ##if review:\n",
    "            ##data.append(l)\n",
    "            ##review = False\n",
    "        if inreview:\n",
    "            data.append(l)\n",
    "            inreview=False\n",
    "        #\"xxxxABCDyyyy\".find(\"ABCD\") != -1:\n",
    "        if l.find(\"<review_text>\")!=-1:\n",
    "            inreview=True\n",
    "            \n",
    "        #if \"</review_text>\" in l:\n",
    "            #data.append(revtext)\n",
    "            #revtext=\"\"\n",
    "            #inreview=False\n",
    "    return data\n",
    "\n",
    "def Advanced_Extract(xml_data):\n",
    "    #/home/omar/data/train-data/books/positive.review\n",
    "    #\"/home/omar/data/train-data/books/unlabeled.review\"\n",
    "    with open(xml_data) as f:\n",
    "        File = f.readlines()\n",
    "    data=[]\n",
    "    revtext=\"\"\n",
    "    inreview=False\n",
    "    for l in File:\n",
    "        revtext+=l\n",
    "    subtext=\"\"\n",
    "    for word in revtext.split(\" \"):\n",
    "        if inreview:\n",
    "            subtext+=word + \" \"\n",
    "        if \"<review_text> \" in word:\n",
    "            inreview=True\n",
    "        if \"</review_text> \" in word:\n",
    "            inreview=False\n",
    "            data.append(subtext)\n",
    "            subtext=\"\"\n",
    "    return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\kitechen_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\kitechen_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 1.27326 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 1.27326 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\kitechen_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\kitechen_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 19856 lines in 0.904474 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 19856 lines in 0.904474 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\books_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\books_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.36278 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.36278 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 54835 lines. Lines per second: 74344.8</pre>"
      ],
      "text/plain": [
       "Read 54835 lines. Lines per second: 74344.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"I remember listening to this audio CD about a year ago on the way home from a friend's home in the back country late at night.  I figured what harm could there be... it's about cats... what could Stephen King do there?  Who was I kidding?!  Suffice it to ...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"I remember listening to this audio CD about a year ago on the way home from a friend's home in the back country late at night.  I figured what harm could there be... it's about cats... what could Stephen King do there?  Who was I kidding?!  Suffice it to ...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"I remember listening to this audio CD about a year ago on the way home from a friend's home in the back country late at night.  I figured what harm could there be... it's about cats... what could Stephen King do there?  Who was I kidding?!  Suffice it to ...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"I remember listening to this audio CD about a year ago on the way home from a friend's home in the back country late at night.  I figured what harm could there be... it's about cats... what could Stephen King do there?  Who was I kidding?!  Suffice it to ...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"A most circumspect, articulate and historically (non-revisionist) accurate evaluaton of our political, environmental and practical lives in the US of A today.Important and enlightening info in an age of increasing civil rights suppression and heightened f...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"A most circumspect, articulate and historically (non-revisionist) accurate evaluaton of our political, environmental and practical lives in the US of A today.Important and enlightening info in an age of increasing civil rights suppression and heightened f...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"I loved this story all the way through!  I \"\"read\"\" while I drive, so I did the audio CD, (unabridged), and it was delightful!  The storline was so fun and quirky and did have me laughing at so many parts... I do like Maz!  This is a woman we all know!  S...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"I loved this story all the way through!  I \"\"read\"\" while I drive, so I did the audio CD, (unabridged), and it was delightful!  The storline was so fun and quirky and did have me laughing at so many parts... I do like Maz!  This is a woman we all know!  S...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"I thoroughly enjoyed this book and had a hard time putting it down.  All of the characters were very enjoyable and Natalie aka: Cressida was a gas!  Such a girlfriend!  She was believable, at least in my world!, and I could totally see myself falling into...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"I thoroughly enjoyed this book and had a hard time putting it down.  All of the characters were very enjoyable and Natalie aka: Cressida was a gas!  Such a girlfriend!  She was believable, at least in my world!, and I could totally see myself falling into...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 932044 lines. Lines per second: 130373</pre>"
      ],
      "text/plain": [
       "Read 932044 lines. Lines per second: 130373"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>5 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "5 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\books_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\books_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 975189 lines in 7.37198 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 975189 lines in 7.37198 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\electronics_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\electronics_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.10393 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.10393 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\electronics_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\electronics_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 23009 lines in 0.136922 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 23009 lines in 0.136922 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\dvd_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\dvd_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.355783 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.355783 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\dvd_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\dvd_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 124438 lines in 0.815627 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 124438 lines in 0.815627 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\apparel_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\apparel_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.037899 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.037899 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\apparel_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\apparel_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 9252 lines in 0.043974 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 9252 lines in 0.043974 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\automotive_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\automotive_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.01899 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.01899 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\automotive_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\automotive_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 184 lines in 0.020988 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 184 lines in 0.020988 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\baby_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\baby_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.039977 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.039977 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\baby_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\baby_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 3955 lines in 0.034981 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 3955 lines in 0.034981 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\beauty_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\beauty_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.03098 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.03098 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\beauty_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\beauty_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 2884 lines in 0.026982 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 2884 lines in 0.026982 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\cameraphoto_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\cameraphoto_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.052981 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.052981 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\cameraphoto_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\cameraphoto_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 7408 lines in 0.052986 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 7408 lines in 0.052986 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"This is a really fun game. It's got quite a lot. Graphics, length, characters, story...Graphics: 8/10 Dayum! Reminds me of Dating Sims...But, nice. The characters are crisp and have the special ingredient known as anime eyes!Sound: 7/10 The voices (or lac...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"This is a really fun game. It's got quite a lot. Graphics, length, characters, story...Graphics: 8/10 Dayum! Reminds me of Dating Sims...But, nice. The characters are crisp and have the special ingredient known as anime eyes!Sound: 7/10 The voices (or lac...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"This is a really fun game. It's got quite a lot. Graphics, length, characters, story...Graphics: 8/10 Dayum! Reminds me of Dating Sims...But, nice. The characters are crisp and have the special ingredient known as anime eyes!Sound: 7/10 The voices (or lac...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"This is a really fun game. It's got quite a lot. Graphics, length, characters, story...Graphics: 8/10 Dayum! Reminds me of Dating Sims...But, nice. The characters are crisp and have the special ingredient known as anime eyes!Sound: 7/10 The voices (or lac...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>2 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "2 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\computervideo_games_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\computervideo_games_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.035963 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.035963 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"This is a really fun game. It's got quite a lot. Graphics, length, characters, story...Graphics: 8/10 Dayum! Reminds me of Dating Sims...But, nice. The characters are crisp and have the special ingredient known as anime eyes!Sound: 7/10 The voices (or lac...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"This is a really fun game. It's got quite a lot. Graphics, length, characters, story...Graphics: 8/10 Dayum! Reminds me of Dating Sims...But, nice. The characters are crisp and have the special ingredient known as anime eyes!Sound: 7/10 The voices (or lac...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"This is a really fun game. It's got quite a lot. Graphics, length, characters, story...Graphics: 8/10 Dayum! Reminds me of Dating Sims...But, nice. The characters are crisp and have the special ingredient known as anime eyes!Sound: 7/10 The voices (or lac...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"This is a really fun game. It's got quite a lot. Graphics, length, characters, story...Graphics: 8/10 Dayum! Reminds me of Dating Sims...But, nice. The characters are crisp and have the special ingredient known as anime eyes!Sound: 7/10 The voices (or lac...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>2 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "2 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\computervideo_games_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\computervideo_games_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 2769 lines in 0.04299 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 2769 lines in 0.04299 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\gourmet_food_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\gourmet_food_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.021987 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.021987 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\gourmet_food_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\gourmet_food_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 1575 lines in 0.022 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 1575 lines in 0.022 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\grocery_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\grocery_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.023985 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.023985 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\grocery_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\grocery_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 2632 lines in 0.023988 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 2632 lines in 0.023988 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\healthpersonal_care_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\healthpersonal_care_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.039976 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.039976 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\healthpersonal_care_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\healthpersonal_care_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 7225 lines in 0.043975 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 7225 lines in 0.043975 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\jewelrywatches_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\jewelrywatches_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.024976 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.024976 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\jewelrywatches_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\jewelrywatches_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 1981 lines in 0.021987 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 1981 lines in 0.021987 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "domains = defaultdict(str)\n",
    "domainnames = ['booksgl','electronicsgl','dvdgl','apparelgl','automotivegl','babygl','beautygl','cameraphotogl','computervideo_gamesgl',\n",
    "           'gourmet_foodgl','grocerygl','healthpersonal_caregl','jewelrywatchesgl']\n",
    "\n",
    "import graphlab as gl\n",
    "\n",
    "kitchengl = gl.SFrame('kitechen_reviews.csv')\n",
    "booksgl = gl.SFrame('books_reviews.csv')\n",
    "domains['booksgl'] = booksgl\n",
    "electronicsgl = gl.SFrame('electronics_reviews.csv')\n",
    "domains['electronicsgl'] = electronicsgl\n",
    "dvdgl = gl.SFrame('dvd_reviews.csv')\n",
    "domains['dvdgl'] = dvdgl\n",
    "\n",
    "apparelgl = gl.SFrame('apparel_reviews.csv')\n",
    "domains['apparelgl'] = apparelgl\n",
    "automotivegl = gl.SFrame('automotive_reviews.csv')\n",
    "domains['automotivegl'] = automotivegl\n",
    "babygl = gl.SFrame('baby_reviews.csv')\n",
    "domains['babygl'] = babygl\n",
    "\n",
    "beautygl = gl.SFrame('beauty_reviews.csv')\n",
    "domains['beautygl'] = beautygl\n",
    "cameraphotogl = gl.SFrame('cameraphoto_reviews.csv')\n",
    "domains['cameraphotogl'] = cameraphotogl\n",
    "computervideo_gamesgl = gl.SFrame('computervideo_games_reviews.csv')\n",
    "domains['computervideo_gamesgl'] = computervideo_gamesgl\n",
    "\n",
    "gourmet_foodgl = gl.SFrame('gourmet_food_reviews.csv')\n",
    "domains['gourmet_foodgl'] = gourmet_foodgl\n",
    "grocerygl = gl.SFrame('grocery_reviews.csv')\n",
    "domains['grocerygl'] = grocerygl\n",
    "healthpersonal_caregl = gl.SFrame('healthpersonal_care_reviews.csv')\n",
    "domains['healthpersonal_caregl'] = healthpersonal_caregl\n",
    "\n",
    "jewelrywatchesgl = gl.SFrame('jewelrywatches_reviews.csv')\n",
    "domains['jewelrywatchesgl'] = jewelrywatchesgl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to oaabde01@louisville.edu and will expire on September 24, 2019.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: C:\\Users\\Omar\\AppData\\Local\\Temp\\graphlab_server_1537929466.log.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\kitechen_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\kitechen_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.062484 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.062484 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\kitechen_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\kitechen_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 19856 lines in 0.073566 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 19856 lines in 0.073566 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\books_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\books_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.328355 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.328355 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 54835 lines. Lines per second: 121024</pre>"
      ],
      "text/plain": [
       "Read 54835 lines. Lines per second: 121024"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"I remember listening to this audio CD about a year ago on the way home from a friend's home in the back country late at night.  I figured what harm could there be... it's about cats... what could Stephen King do there?  Who was I kidding?!  Suffice it to ...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"I remember listening to this audio CD about a year ago on the way home from a friend's home in the back country late at night.  I figured what harm could there be... it's about cats... what could Stephen King do there?  Who was I kidding?!  Suffice it to ...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"I remember listening to this audio CD about a year ago on the way home from a friend's home in the back country late at night.  I figured what harm could there be... it's about cats... what could Stephen King do there?  Who was I kidding?!  Suffice it to ...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"I remember listening to this audio CD about a year ago on the way home from a friend's home in the back country late at night.  I figured what harm could there be... it's about cats... what could Stephen King do there?  Who was I kidding?!  Suffice it to ...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"A most circumspect, articulate and historically (non-revisionist) accurate evaluaton of our political, environmental and practical lives in the US of A today.Important and enlightening info in an age of increasing civil rights suppression and heightened f...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"A most circumspect, articulate and historically (non-revisionist) accurate evaluaton of our political, environmental and practical lives in the US of A today.Important and enlightening info in an age of increasing civil rights suppression and heightened f...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"I loved this story all the way through!  I \"\"read\"\" while I drive, so I did the audio CD, (unabridged), and it was delightful!  The storline was so fun and quirky and did have me laughing at so many parts... I do like Maz!  This is a woman we all know!  S...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"I loved this story all the way through!  I \"\"read\"\" while I drive, so I did the audio CD, (unabridged), and it was delightful!  The storline was so fun and quirky and did have me laughing at so many parts... I do like Maz!  This is a woman we all know!  S...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"I thoroughly enjoyed this book and had a hard time putting it down.  All of the characters were very enjoyable and Natalie aka: Cressida was a gas!  Such a girlfriend!  She was believable, at least in my world!, and I could totally see myself falling into...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"I thoroughly enjoyed this book and had a hard time putting it down.  All of the characters were very enjoyable and Natalie aka: Cressida was a gas!  Such a girlfriend!  She was believable, at least in my world!, and I could totally see myself falling into...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 932044 lines. Lines per second: 173783</pre>"
      ],
      "text/plain": [
       "Read 932044 lines. Lines per second: 173783"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>5 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "5 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\books_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\books_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 975189 lines in 5.5275 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 975189 lines in 5.5275 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\electronics_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\electronics_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.093743 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.093743 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\electronics_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\electronics_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 23009 lines in 0.103979 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 23009 lines in 0.103979 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\dvd_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\dvd_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.38784 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.38784 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\dvd_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\dvd_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 124438 lines in 0.811242 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 124438 lines in 0.811242 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\apparel_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\apparel_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.046883 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.046883 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\apparel_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\apparel_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 9252 lines in 0.037042 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 9252 lines in 0.037042 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\automotive_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\automotive_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.022693 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.022693 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\automotive_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\automotive_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 736 lines in 0.019995 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 736 lines in 0.019995 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\baby_reviewsUpdated.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\baby_reviewsUpdated.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.031991 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.031991 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\baby_reviewsUpdated.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\baby_reviewsUpdated.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 4256 lines in 0.035989 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 4256 lines in 0.035989 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\beauty_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\beauty_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.028011 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.028011 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\beauty_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\beauty_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 2884 lines in 0.023994 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 2884 lines in 0.023994 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\cameraphoto_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\cameraphoto_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.054911 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.054911 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\cameraphoto_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\cameraphoto_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 7408 lines in 0.050939 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 7408 lines in 0.050939 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\gourmet_food_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\gourmet_food_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.03125 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.03125 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\gourmet_food_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\gourmet_food_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 1575 lines in 0.023994 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 1575 lines in 0.023994 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\grocery_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\grocery_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.020399 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.020399 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\grocery_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\grocery_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 2632 lines in 0.016644 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 2632 lines in 0.016644 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\healthpersonal_care_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\healthpersonal_care_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.045046 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.045046 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\healthpersonal_care_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\healthpersonal_care_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 7225 lines in 0.032594 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 7225 lines in 0.032594 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\jewelrywatches_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\jewelrywatches_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.028515 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.028515 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\jewelrywatches_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\jewelrywatches_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 1981 lines in 0.019995 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 1981 lines in 0.019995 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "##original\n",
    "import pandas as pd\n",
    "import graphlab as gl\n",
    "domains = defaultdict(str)\n",
    "domainnames = ['booksgl','electronicsgl','dvdgl','apparelgl','automotivegl','babygl','beautygl','cameraphotogl','computervideo_gamesgl',\n",
    "           'gourmet_foodgl','grocerygl','healthpersonal_caregl','jewelrywatchesgl']\n",
    "\n",
    "import graphlab as gl\n",
    "\n",
    "kitchengl = gl.SFrame('kitechen_reviews.csv')\n",
    "booksgl = gl.SFrame('books_reviews.csv')\n",
    "domains['booksgl'] = booksgl\n",
    "electronicsgl = gl.SFrame('electronics_reviews.csv')\n",
    "domains['electronicsgl'] = electronicsgl\n",
    "dvdgl = gl.SFrame('dvd_reviews.csv')\n",
    "domains['dvdgl'] = dvdgl\n",
    "\n",
    "apparelgl = gl.SFrame('apparel_reviews.csv')\n",
    "domains['apparelgl'] = apparelgl\n",
    "automotivegl = gl.SFrame('automotive_reviews.csv')\n",
    "domains['automotivegl'] = automotivegl\n",
    "babygl = gl.SFrame('baby_reviewsUpdated.csv')\n",
    "domains['babygl'] = babygl\n",
    "\n",
    "beautygl = gl.SFrame('beauty_reviews.csv')\n",
    "domains['beautygl'] = beautygl\n",
    "cameraphotogl = gl.SFrame('cameraphoto_reviews.csv')\n",
    "domains['cameraphotogl'] = cameraphotogl\n",
    "#computervideo_gamesgl = gl.SFrame('computervideo_games_reviews.csv')\n",
    "computervideo_gamesdf = pd.read_csv('computervideo_games_reviews.csv')\n",
    "computervideo_gamesgl = gl.SFrame(data=computervideo_gamesdf)\n",
    "domains['computervideo_gamesgl'] = computervideo_gamesgl\n",
    "\n",
    "gourmet_foodgl = gl.SFrame('gourmet_food_reviews.csv')\n",
    "domains['gourmet_foodgl'] = gourmet_foodgl\n",
    "grocerygl = gl.SFrame('grocery_reviews.csv')\n",
    "domains['grocerygl'] = grocerygl\n",
    "healthpersonal_caregl = gl.SFrame('healthpersonal_care_reviews.csv')\n",
    "domains['healthpersonal_caregl'] = healthpersonal_caregl\n",
    "\n",
    "jewelrywatchesgl = gl.SFrame('jewelrywatches_reviews.csv')\n",
    "domains['jewelrywatchesgl'] = jewelrywatchesgl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: C:\\Users\\Omar\\AppData\\Local\\Temp\\graphlab_server_1538019963.log.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to oaabde01@louisville.edu and will expire on September 24, 2019.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\books_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\books_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.39777 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.39777 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,long]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 54835 lines. Lines per second: 117487</pre>"
      ],
      "text/plain": [
       "Read 54835 lines. Lines per second: 117487"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"I remember listening to this audio CD about a year ago on the way home from a friend's home in the back country late at night.  I figured what harm could there be... it's about cats... what could Stephen King do there?  Who was I kidding?!  Suffice it to ...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"I remember listening to this audio CD about a year ago on the way home from a friend's home in the back country late at night.  I figured what harm could there be... it's about cats... what could Stephen King do there?  Who was I kidding?!  Suffice it to ...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"I remember listening to this audio CD about a year ago on the way home from a friend's home in the back country late at night.  I figured what harm could there be... it's about cats... what could Stephen King do there?  Who was I kidding?!  Suffice it to ...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"I remember listening to this audio CD about a year ago on the way home from a friend's home in the back country late at night.  I figured what harm could there be... it's about cats... what could Stephen King do there?  Who was I kidding?!  Suffice it to ...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"A most circumspect, articulate and historically (non-revisionist) accurate evaluaton of our political, environmental and practical lives in the US of A today.Important and enlightening info in an age of increasing civil rights suppression and heightened f...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"A most circumspect, articulate and historically (non-revisionist) accurate evaluaton of our political, environmental and practical lives in the US of A today.Important and enlightening info in an age of increasing civil rights suppression and heightened f...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"I loved this story all the way through!  I \"\"read\"\" while I drive, so I did the audio CD, (unabridged), and it was delightful!  The storline was so fun and quirky and did have me laughing at so many parts... I do like Maz!  This is a woman we all know!  S...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"I loved this story all the way through!  I \"\"read\"\" while I drive, so I did the audio CD, (unabridged), and it was delightful!  The storline was so fun and quirky and did have me laughing at so many parts... I do like Maz!  This is a woman we all know!  S...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"I thoroughly enjoyed this book and had a hard time putting it down.  All of the characters were very enjoyable and Natalie aka: Cressida was a gas!  Such a girlfriend!  She was believable, at least in my world!, and I could totally see myself falling into...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"I thoroughly enjoyed this book and had a hard time putting it down.  All of the characters were very enjoyable and Natalie aka: Cressida was a gas!  Such a girlfriend!  She was believable, at least in my world!, and I could totally see myself falling into...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 932044 lines. Lines per second: 161346</pre>"
      ],
      "text/plain": [
       "Read 932044 lines. Lines per second: 161346"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>5 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "5 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file C:\\Users\\Omar\\books_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file C:\\Users\\Omar\\books_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 975189 lines in 5.93558 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 975189 lines in 5.93558 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import graphlab as gl\n",
    "booksgl = gl.SFrame('books_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "booksglposneg = booksgl[booksgl['label']!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(booksglposneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "booksglpos = booksglposneg[booksglposneg['label']==1]\n",
    "booksglneg = booksglposneg[booksglposneg['label']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "booksglpos_500 = booksglpos[:500]\n",
    "booksglneg_500 = booksglneg[:500]\n",
    "\n",
    "booksglpos_500.save('booksglpos_500.csv')\n",
    "booksglneg_500.save('booksglneg_500.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "booksglpos_500pd = pd.read_csv('booksglpos_500.csv')\n",
    "booksglneg_500pd = pd.read_csv('booksglneg_500.csv')\n",
    "booksglpos_500pd.to_csv('booksglpos_500pd.txt',index=False)\n",
    "booksglneg_500pd.to_csv('booksglneg_500pd.txt',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apparelgl['word_count'] = gl.text_analytics.count_words(apparelgl['review'])\n",
    "automotivegl['word_count'] = gl.text_analytics.count_words(automotivegl['review'])\n",
    "babygl['word_count'] = gl.text_analytics.count_words(babygl['review'])\n",
    "beautygl['word_count'] = gl.text_analytics.count_words(beautygl['review'])\n",
    "cameraphotogl['word_count'] = gl.text_analytics.count_words(cameraphotogl['review'])\n",
    "computervideo_gamesgl['word_count'] = gl.text_analytics.count_words(computervideo_gamesgl['review'])\n",
    "gourmet_foodgl['word_count'] = gl.text_analytics.count_words(gourmet_foodgl['review'])\n",
    "grocerygl['word_count'] = gl.text_analytics.count_words(grocerygl['review'])\n",
    "\n",
    "healthpersonal_caregl['word_count'] = gl.text_analytics.count_words(healthpersonal_caregl['review'])\n",
    "jewelrywatchesgl['word_count'] = gl.text_analytics.count_words(jewelrywatchesgl['review'])\n",
    "\n",
    "import graphlab\n",
    "apparelgl['3-grams'] = graphlab.text_analytics.count_ngrams(apparelgl['review'], 3)\n",
    "apparelgl['2-grams'] = graphlab.text_analytics.count_ngrams(apparelgl['review'], 2)\n",
    "apparelgl['1-grams'] = graphlab.text_analytics.count_ngrams(apparelgl['review'], 1)\n",
    "\n",
    "automotivegl['3-grams'] = graphlab.text_analytics.count_ngrams(automotivegl['review'], 3)\n",
    "automotivegl['2-grams'] = graphlab.text_analytics.count_ngrams(automotivegl['review'], 2)\n",
    "automotivegl['1-grams'] = graphlab.text_analytics.count_ngrams(automotivegl['review'], 1)\n",
    "\n",
    "babygl['3-grams'] = graphlab.text_analytics.count_ngrams(babygl['review'], 3)\n",
    "babygl['2-grams'] = graphlab.text_analytics.count_ngrams(babygl['review'], 2)\n",
    "babygl['1-grams'] = graphlab.text_analytics.count_ngrams(babygl['review'], 1)\n",
    "\n",
    "beautygl['3-grams'] = graphlab.text_analytics.count_ngrams(beautygl['review'], 3)\n",
    "beautygl['2-grams'] = graphlab.text_analytics.count_ngrams(beautygl['review'], 2)\n",
    "beautygl['1-grams'] = graphlab.text_analytics.count_ngrams(beautygl['review'], 1)\n",
    "\n",
    "cameraphotogl['3-grams'] = graphlab.text_analytics.count_ngrams(cameraphotogl['review'], 3)\n",
    "cameraphotogl['2-grams'] = graphlab.text_analytics.count_ngrams(cameraphotogl['review'], 2)\n",
    "cameraphotogl['1-grams'] = graphlab.text_analytics.count_ngrams(cameraphotogl['review'], 1)\n",
    "\n",
    "computervideo_gamesgl['3-grams'] = graphlab.text_analytics.count_ngrams(computervideo_gamesgl['review'], 3)\n",
    "computervideo_gamesgl['2-grams'] = graphlab.text_analytics.count_ngrams(computervideo_gamesgl['review'], 2)\n",
    "computervideo_gamesgl['1-grams'] = graphlab.text_analytics.count_ngrams(computervideo_gamesgl['review'], 1)\n",
    "\n",
    "gourmet_foodgl['3-grams'] = graphlab.text_analytics.count_ngrams(gourmet_foodgl['review'], 3)\n",
    "gourmet_foodgl['2-grams'] = graphlab.text_analytics.count_ngrams(gourmet_foodgl['review'], 2)\n",
    "gourmet_foodgl['1-grams'] = graphlab.text_analytics.count_ngrams(gourmet_foodgl['review'], 1)\n",
    "\n",
    "grocerygl['3-grams'] = graphlab.text_analytics.count_ngrams(grocerygl['review'], 3)\n",
    "grocerygl['2-grams'] = graphlab.text_analytics.count_ngrams(grocerygl['review'], 2)\n",
    "grocerygl['1-grams'] = graphlab.text_analytics.count_ngrams(grocerygl['review'], 1)\n",
    "\n",
    "healthpersonal_caregl['3-grams'] = graphlab.text_analytics.count_ngrams(healthpersonal_caregl['review'], 3)\n",
    "healthpersonal_caregl['2-grams'] = graphlab.text_analytics.count_ngrams(healthpersonal_caregl['review'], 2)\n",
    "healthpersonal_caregl['1-grams'] = graphlab.text_analytics.count_ngrams(healthpersonal_caregl['review'], 1)\n",
    "\n",
    "jewelrywatchesgl['3-grams'] = graphlab.text_analytics.count_ngrams(jewelrywatchesgl['review'], 3)\n",
    "jewelrywatchesgl['2-grams'] = graphlab.text_analytics.count_ngrams(jewelrywatchesgl['review'], 2)\n",
    "jewelrywatchesgl['1-grams'] = graphlab.text_analytics.count_ngrams(jewelrywatchesgl['review'], 1)\n",
    "\n",
    "\n",
    "apparelgl['tfidf'] = gl.text_analytics.tf_idf(apparelgl['word_count'])\n",
    "\n",
    "automotivegl['tfidf'] = gl.text_analytics.tf_idf(automotivegl['word_count'])\n",
    "babygl['tfidf'] = gl.text_analytics.tf_idf(babygl['word_count'])\n",
    "beautygl['tfidf'] = gl.text_analytics.tf_idf(beautygl['word_count'])\n",
    "\n",
    "cameraphotogl['tfidf'] = gl.text_analytics.tf_idf(cameraphotogl['word_count'])\n",
    "\n",
    "computervideo_gamesgl['tfidf'] = gl.text_analytics.tf_idf(computervideo_gamesgl['word_count'])\n",
    "gourmet_foodgl['tfidf'] = gl.text_analytics.tf_idf(gourmet_foodgl['word_count'])\n",
    "grocerygl['tfidf'] = gl.text_analytics.tf_idf(grocerygl['word_count'])\n",
    "\n",
    "healthpersonal_caregl['tfidf'] = gl.text_analytics.tf_idf(healthpersonal_caregl['word_count'])\n",
    "jewelrywatchesgl['tfidf'] = gl.text_analytics.tf_idf(jewelrywatchesgl['word_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "electronicsgl['word_count'] = gl.text_analytics.count_words(electronicsgl['review'])\n",
    "kitchengl['word_count'] = gl.text_analytics.count_words(kitchengl['review'])\n",
    "booksgl['word_count'] = gl.text_analytics.count_words(booksgl['review'])\n",
    "dvdgl['word_count'] = gl.text_analytics.count_words(dvdgl['review'])\n",
    "electronicsgl['3-grams'] = graphlab.text_analytics.count_ngrams(electronicsgl['review'], 3)\n",
    "electronicsgl['2-grams'] = graphlab.text_analytics.count_ngrams(electronicsgl['review'], 2)\n",
    "electronicsgl['1-grams'] = graphlab.text_analytics.count_ngrams(electronicsgl['review'], 1)\n",
    "\n",
    "kitchengl['3-grams'] = graphlab.text_analytics.count_ngrams(kitchengl['review'], 3)\n",
    "kitchengl['2-grams'] = graphlab.text_analytics.count_ngrams(kitchengl['review'], 2)\n",
    "kitchengl['1-grams'] = graphlab.text_analytics.count_ngrams(kitchengl['review'], 1)\n",
    "\n",
    "booksgl['3-grams'] = graphlab.text_analytics.count_ngrams(booksgl['review'], 3)\n",
    "booksgl['2-grams'] = graphlab.text_analytics.count_ngrams(booksgl['review'], 2)\n",
    "booksgl['1-grams'] = graphlab.text_analytics.count_ngrams(booksgl['review'], 1)\n",
    "\n",
    "dvdgl['3-grams'] = graphlab.text_analytics.count_ngrams(dvdgl['review'], 3)\n",
    "dvdgl['2-grams'] = graphlab.text_analytics.count_ngrams(dvdgl['review'], 2)\n",
    "dvdgl['1-grams'] = graphlab.text_analytics.count_ngrams(dvdgl['review'], 1)\n",
    "\n",
    "electronicsgl['tfidf'] = gl.text_analytics.tf_idf(electronicsgl['word_count'])\n",
    "\n",
    "kitchengl['tfidf'] = gl.text_analytics.tf_idf(kitchengl['review'])\n",
    "booksgl['tfidf'] = gl.text_analytics.tf_idf(booksgl['review'])\n",
    "dvdgl['tfidf'] = gl.text_analytics.tf_idf(dvdgl['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#electronicsgl\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "def reviewcleaner(text):\n",
    "    #postags = ['JJ', 'JJR', 'JJS','VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ','RB', 'RBR', 'RBS']\n",
    "    postags = ['JJ','VB','RB']\n",
    "    #Nountags = ['NN', 'NNP', 'NNS']\n",
    "    Nountags = ['NN']\n",
    "    #text = \"I love books in general\"\n",
    "\n",
    "    #print words\n",
    "    text = preprocess(text)\n",
    "    raw_words = text.split(\" \")\n",
    "    raw_words2 = text.split(\" \")\n",
    "    #nltk.word_tokenize(text)\n",
    "    try:\n",
    "        words = nltk.pos_tag(raw_words)\n",
    "    except:\n",
    "        return \"\"\n",
    "    \n",
    "    newsentence=[]\n",
    "    for i in range(len(raw_words)):\n",
    "\n",
    "        #print raw_words[i]\n",
    "        #print words[i][1]\n",
    "        #print words\n",
    "        #print raw_words\n",
    "        ##if len(raw_words) != len(words):\n",
    "            ##print words\n",
    "            ##print raw_words\n",
    "            ##break\n",
    "        if words[i][1] in postags:\n",
    "            try:\n",
    "                sentiments = swn.senti_synsets(str(raw_words[i]))\n",
    "                if len(list(sentiments))>0:\n",
    "                    hsentiments0 = list(sentiments)[0]\n",
    "                    if hsentiments0.pos_score > 0.80 or hsentiments0.neg_score > 0.80:\n",
    "                        newsentence.append(raw_words[i])\n",
    "            except:\n",
    "                continue\n",
    "            ##newsentence.append(raw_words[i])\n",
    "        elif words[i][1] in Nountags:\n",
    "            try:\n",
    "                sentiments = swn.senti_synsets(str(raw_words[i]))\n",
    "                if len(list(sentiments))>0:\n",
    "                    hsentiments0 = list(sentiments)[0]\n",
    "                    if hsentiments0.pos_score > 0.80 or hsentiments0.neg_score > 0.80:\n",
    "                        newsentence.append(raw_words[i])\n",
    "            except:\n",
    "                continue\n",
    "    return  (str(\" \".join(newsentence)))\n",
    "\n",
    "def word_counter(textfilepath):\n",
    "    \n",
    "    \n",
    "    #'your_file.txt'\n",
    "    with open(textfilepath) as f:\n",
    "        passage = f.readlines()\n",
    "\n",
    "    words = re.findall(r'\\w+', str(passage))\n",
    "\n",
    "    cap_words = [word.upper() for word in words]\n",
    "\n",
    "    word_counts = Counter(cap_words)\n",
    "    return word_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received kingston 256mb sd card advertised unit came mail exactly 2 days iordered worked perfectly satisfied\n",
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    #sentence = \" \".join(w for w in nltk.wordpunct_tokenize(sentence) if w.lower() in words or not w.isalpha())\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    filtered_words = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    ##sentence = \" \".join(filtered_words)\n",
    "    #filtered_words = \" \".join(w for w in nltk.wordpunct_tokenize(sentence) if w.lower() in words or not w.isalpha())\n",
    "    #filtered_words = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    ##return filtered_words\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "\n",
    "#sentence = \"At eight o'clock on Thursday morning Arthur didn't feel very good. French-Fries\"\n",
    "sentence = \"I received my Kingston 256MB SD card just as advertised.The unit came in the mail exactly 2 days after Iordered. Worked perfectly and I'm very satisfied\"\n",
    "#sentence = \"like many\"\n",
    "print preprocess(sentence)\n",
    "text = preprocess(sentence)\n",
    "raw_words = text.split(\" \")\n",
    "words = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "print len(raw_words)\n",
    "print len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "booksgl_unlabeled = booksgl[booksgl['label']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "booksgl_unlabeled['clean_review'] = booksgl_unlabeled['review'].apply(lambda x: reviewcleaner(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "booksgl_unlabeled['clean_review'] .save(\"booksglcleanreview_compact_unlabeled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "increments = {}\n",
    "\n",
    "counter=0\n",
    "\n",
    "for i in range(48):\n",
    "    increments[i] = booksgl_unlabeled['clean_review'][counter:counter+20000]\n",
    "    counter+=20000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#booksgl_unlabeled_text = list(booksgl_unlabeled['clean_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"booksglcleanreview_compact_unlabeled.csv\", \"w+\") as f:\n",
    "    for row in increments[0]:\n",
    "        f.write(row + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"booksglcleanreview_compact_unlabeled.csv\", \"a\") as f:\n",
    "    for x in list(booksgl_unlabeled['clean_review'][50000:100000]):\n",
    "        f.write(x + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"booksglcleanreview_compact_unlabeled.csv\", \"a\") as f:\n",
    "    for x in list(booksgl_unlabeled['clean_review'][100000:150000]):\n",
    "        f.write(x + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"booksglcleanreview_compact_unlabeled.csv\", \"a\") as f:\n",
    "    for x in list(booksgl_unlabeled['clean_review'][150000:200000]):\n",
    "        f.write(x + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"booksglcleanreview_compact_unlabeled.csv\", \"a\") as f:\n",
    "    for x in list(booksgl_unlabeled['clean_review'][200000:250000]):\n",
    "        f.write(x + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"booksglcleanreview_compact_unlabeled.csv\", \"a\") as f:\n",
    "    for x in list(booksgl_unlabeled['clean_review'][250000:300000]):\n",
    "        f.write(x + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"booksglcleanreview_compact_unlabeled.csv\", \"a\") as f:\n",
    "    for x in list(booksgl_unlabeled['clean_review'][300000:350000]):\n",
    "        f.write(x + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"booksglcleanreview_compact_unlabeled.csv\", \"a\") as f:\n",
    "    for x in list(booksgl_unlabeled['clean_review'][350000:400000]):\n",
    "        f.write(x + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"booksglcleanreview_compact_unlabeled.csv\", \"w\") as f:\n",
    "    for x in booksgl_unlabeled_text:\n",
    "        f.write(x + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "electronics_wordcount = word_counter(\"electronicglcleanreview.csv\")\n",
    "dvdgl_wordcount = word_counter(\"dvdglcleanreview.csv\")\n",
    "kitchengl_wordcount = word_counter(\"kitchenglcleanreview.csv\")\n",
    "booksgl_wordcount = word_counter(\"booksglcleanreview.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "electronics_wordcount = word_counter(\"electronicsgl_compact_unlabeled.csv\")\n",
    "#\n",
    "dvdgl_wordcount = word_counter(\"dvdglcleanreview_compact_unlabeled.csv\")\n",
    "kitchengl_wordcount = word_counter(\"kitchenglcleanreview_compact_unlabeled.csv\")\n",
    "booksgl_wordcount = word_counter(\"booksglcleanreview_compact_unlabeled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "apparelgl_wordcount = word_counter(\"apparelglcleanreview_compact2.csv\")\n",
    "automotivegl_wordcount = word_counter(\"automotiveglcleanreview_compact2.csv\")\n",
    "babygl_wordcount = word_counter(\"babyglcleanreview_compact2.csv\")\n",
    "beautygl_wordcount = word_counter(\"beautyglcleanreview_compact2.csv\")\n",
    "cameraphotogl_wordcount = word_counter(\"cameraphotoglcleanreview_compact2.csv\")\n",
    "computervideo_gamesgl_wordcount = word_counter(\"computervideo_gamesglcleanreview_compact2.csv\")\n",
    "gourmet_foodgl_wordcount = word_counter(\"gourmet_foodglcleanreview_compact2.csv\")\n",
    "grocerygl_wordcount = word_counter(\"groceryglcleanreview_compact2.csv\")\n",
    "healthpersonal_caregl_wordcount = word_counter(\"healthpersonal_careglcleanreview_compact2.csv\")\n",
    "jewelrywatchesgl_wordcount = word_counter(\"jewelrywatchesglcleanreview_compact2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to oaabde01@louisville.edu and will expire on September 24, 2019.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: C:\\Users\\Omar\\AppData\\Local\\Temp\\graphlab_server_1537985711.log.0\n"
     ]
    }
   ],
   "source": [
    "#dvd_electronics_distance = gl.distances.cosine(electronics_wordcount,dvdgl_wordcount)\n",
    "import graphlab as gl\n",
    "fscore={}\n",
    "accuracies={}\n",
    "euclidean_distances_tobook={}\n",
    "#kitchen_electronics_distance = gl.distances.euclidean(electronics_wordcount,kitchengl_wordcount)\n",
    "euclidean_distances_tobook['electronics'] = gl.distances.euclidean(booksgl_wordcount,electronics_wordcount)\n",
    "#dvd_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,dvdgl_wordcount)\n",
    "euclidean_distances_tobook['dvd'] = gl.distances.euclidean(booksgl_wordcount,dvdgl_wordcount)\n",
    "#books_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,booksgl_wordcount)\n",
    "euclidean_distances_tobook['kitchen'] = gl.distances.euclidean(booksgl_wordcount,kitchengl_wordcount)\n",
    "\n",
    "#apparel_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,apparelgl_wordcount)\n",
    "euclidean_distances_tobook['apparel'] = gl.distances.euclidean(booksgl_wordcount,apparelgl_wordcount)\n",
    "#automotive_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,automotivegl_wordcount)\n",
    "euclidean_distances_tobook['automotive'] = gl.distances.euclidean(booksgl_wordcount,automotivegl_wordcount)\n",
    "#baby_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,babygl_wordcount)\n",
    "euclidean_distances_tobook['baby'] = gl.distances.euclidean(booksgl_wordcount,babygl_wordcount)\n",
    "#beauty_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,beautygl_wordcount)\n",
    "euclidean_distances_tobook['beauty'] = gl.distances.euclidean(booksgl_wordcount,beautygl_wordcount)\n",
    "#cameraphoto_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,cameraphotogl_wordcount)\n",
    "euclidean_distances_tobook['cameraphoto'] = gl.distances.euclidean(booksgl_wordcount,cameraphotogl_wordcount)\n",
    "#computervideo_games_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,computervideo_gamesgl_wordcount)\n",
    "euclidean_distances_tobook['computervideo'] = gl.distances.euclidean(booksgl_wordcount,computervideo_gamesgl_wordcount)\n",
    "#gourmet_food_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,gourmet_foodgl_wordcount)\n",
    "euclidean_distances_tobook['gourmet'] = gl.distances.euclidean(booksgl_wordcount,gourmet_foodgl_wordcount)\n",
    "#grocery_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,grocerygl_wordcount)\n",
    "euclidean_distances_tobook['grocery'] = gl.distances.euclidean(booksgl_wordcount,grocerygl_wordcount)\n",
    "#healthpersonal_care_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,healthpersonal_caregl_wordcount)\n",
    "euclidean_distances_tobook['healthpersonal'] = gl.distances.euclidean(booksgl_wordcount,healthpersonal_caregl_wordcount)\n",
    "#jewelrywatches_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,jewelrywatchesgl_wordcount)\n",
    "euclidean_distances_tobook['jewelrywatches'] = gl.distances.euclidean(booksgl_wordcount,jewelrywatchesgl_wordcount)\n",
    "\n",
    "book_dist_domains = ['electronics','dvd','kitchen','apparel','automotive','baby','beauty','cameraphoto','computervideo','gourmet','grocery','healthpersonal','jewelrywatches']\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bookstestgl = graphlab.SFrame('bookstestgl_predictions3')\n",
    "#bookstestgl.save('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">review</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">label</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">word_count</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">3-grams</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">One of the most<br>interesting books I've ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 1L, 'all': 1L,<br>'everyone': 1L, 'is': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'is very compelling':<br>1L, 'around you from': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">In the 1970's, Peter<br>Singer came out with a ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'a': 1L, 'singer': 1L,<br>'animal': 1L, 'field': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'was new in': 1L, 'the<br>field of': 1L, 'radical ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">If you're into reading<br>memoirs, this is ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'enjoy': 1L, 'and': 3L,<br>'into': 2L, 'feelings': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'the difference of': 1L,<br>'great read text': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">This book explains in<br>simple terms all the ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'all': 1L, \"don't\": 1L,<br>'simple': 1L, 'one': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'all the stuff': 1L,<br>'books on photography': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Geez.  Its a book folks.<br>Nobody said it was going ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'sentence': 1L, 'money':<br>1L, 'perfect.': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'in a world': 1L, 'your<br>inner child': 1L, 's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">I'm a little late in<br>reading this book.  I am ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'enjoy': 1L, 'and': 1L,<br>'am': 1L, 'books': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'trying to pace': 1L,<br>'pace myself between': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">After reading this<br>account of the acension ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'account': 1L, 'would':<br>1L, 'this': 1L, 'of': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'after reading this':<br>1L, 'of the acension': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">There is about as much<br>additional science in ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 1L, 'already':<br>1L, 'out.&lt;/text&gt;': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'intelligence as you':<br>1L, 'note that this': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">I am a big Tananarive Due<br>fan because she never ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 2L, 'because':<br>1L, 'characters,': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'over a great': 1L, 'and<br>unites them': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">This book explains in<br>simple terms all the ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'all': 1L, \"don't\": 1L,<br>'simple': 1L, 'one': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'all the stuff': 1L,<br>'books on photography': ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "<table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">2-grams</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">1-grams</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">tfidf</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">kitchen_predictions</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">dvd_predictions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'could not': 1L, 'that<br>is': 1L, 'economics ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 1L, 'all': 1L,<br>'everyone': 1L, 've': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and':<br>0.2613647641344075, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'ethics animal': 1L, 's<br>peter': 1L, 'radical ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'a': 1L, 'singer': 1L,<br>'which': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'a':<br>0.31129193809091477, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'if you': 1L, 'my<br>opinion': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'enjoy': 1L, 'and': 3L,<br>'memoirs': 1L, 'text': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'enjoy':<br>3.4265151896464454, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'on photography': 1L,<br>'this book': 1L, 'hands ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'all': 1L, 'simple': 1L,<br>'text': 1L, 'it': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'all':<br>1.4696759700589417, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'if you': 2L, 'see<br>what': 1L, 'its a': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'sentence': 1L, 'text':<br>1L, 'kill': 1L, 'go': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'sentence':<br>5.991464547107982, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'i am': 1L, 'this book':<br>1L, 'can enjoy': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'enjoy': 1L, 'and': 1L,<br>'text': 1L, 'year': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'enjoy':<br>3.4265151896464454, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'anybody would': 1L,<br>'after reading': 1L, 'of ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'account': 1L, 'would':<br>1L, 'this': 1L, 'text': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'account':<br>4.382026634673881, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'pay his': 1L, 'write<br>on': 1L, 'them more': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 1L, 'secondly':<br>1L, 'already': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and':<br>0.2613647641344075, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'as she': 1L, 'that is':<br>1L, 'plot storyline': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 2L, 'because':<br>1L, 'fascinating': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and':<br>0.522729528268815, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'on photography': 1L,<br>'this book': 1L, 'hands ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'all': 1L, 'simple': 1L,<br>'text': 1L, 'it': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'all':<br>1.4696759700589417, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "</table>\n",
       "<table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">electronics_predictions</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">apparel_predictions</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">automotive_predictions</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">baby_predictions</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">beauty_predictions</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">cameraphoto_predictions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "</table>\n",
       "<table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">computervideo_predictions</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">gourmet_predictions</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">grocery_predictions</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">healthpersonal_prediction<br>s ...</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">jewelrywatches_prediction<br>s ...</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[400 rows x 20 columns]<br/>Note: Only the head of the SFrame is printed.<br/>You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\treview\tstr\n",
       "\tlabel\tint\n",
       "\tword_count\tdict\n",
       "\t3-grams\tdict\n",
       "\t2-grams\tdict\n",
       "\t1-grams\tdict\n",
       "\ttfidf\tdict\n",
       "\tkitchen_predictions\tint\n",
       "\tdvd_predictions\tint\n",
       "\telectronics_predictions\tint\n",
       "\tapparel_predictions\tint\n",
       "\tautomotive_predictions\tint\n",
       "\tbaby_predictions\tint\n",
       "\tbeauty_predictions\tint\n",
       "\tcameraphoto_predictions\tint\n",
       "\tcomputervideo_predictions\tint\n",
       "\tgourmet_predictions\tint\n",
       "\tgrocery_predictions\tint\n",
       "\thealthpersonal_predictions\tint\n",
       "\tjewelrywatches_predictions\tint\n",
       "\n",
       "Rows: 400\n",
       "\n",
       "Data:\n",
       "+-------------------------------+-------+-------------------------------+\n",
       "|             review            | label |           word_count          |\n",
       "+-------------------------------+-------+-------------------------------+\n",
       "| One of the most interestin... |   1   | {'and': 1L, 'all': 1L, 'ev... |\n",
       "| In the 1970's, Peter Singe... |   1   | {'a': 1L, 'singer': 1L, 'a... |\n",
       "| If you're into reading mem... |   1   | {'enjoy': 1L, 'and': 3L, '... |\n",
       "| This book explains in simp... |   1   | {'all': 1L, \"don't\": 1L, '... |\n",
       "| Geez.  Its a book folks.  ... |   1   | {'sentence': 1L, 'money': ... |\n",
       "| I'm a little late in readi... |   1   | {'enjoy': 1L, 'and': 1L, '... |\n",
       "| After reading this account... |   1   | {'account': 1L, 'would': 1... |\n",
       "| There is about as much add... |   1   | {'and': 1L, 'already': 1L,... |\n",
       "| I am a big Tananarive Due ... |   1   | {'and': 2L, 'because': 1L,... |\n",
       "| This book explains in simp... |   1   | {'all': 1L, \"don't\": 1L, '... |\n",
       "+-------------------------------+-------+-------------------------------+\n",
       "+-------------------------------+-------------------------------+\n",
       "|            3-grams            |            2-grams            |\n",
       "+-------------------------------+-------------------------------+\n",
       "| {'is very compelling': 1L,... | {'could not': 1L, 'that is... |\n",
       "| {'was new in': 1L, 'the fi... | {'ethics animal': 1L, 's p... |\n",
       "| {'the difference of': 1L, ... | {'if you': 1L, 'my opinion... |\n",
       "| {'all the stuff': 1L, 'boo... | {'on photography': 1L, 'th... |\n",
       "| {'in a world': 1L, 'your i... | {'if you': 2L, 'see what':... |\n",
       "| {'trying to pace': 1L, 'pa... | {'i am': 1L, 'this book': ... |\n",
       "| {'after reading this': 1L,... | {'anybody would': 1L, 'aft... |\n",
       "| {'intelligence as you': 1L... | {'pay his': 1L, 'write on'... |\n",
       "| {'over a great': 1L, 'and ... | {'as she': 1L, 'that is': ... |\n",
       "| {'all the stuff': 1L, 'boo... | {'on photography': 1L, 'th... |\n",
       "+-------------------------------+-------------------------------+\n",
       "+-------------------------------+-------------------------------+---------------------+\n",
       "|            1-grams            |             tfidf             | kitchen_predictions |\n",
       "+-------------------------------+-------------------------------+---------------------+\n",
       "| {'and': 1L, 'all': 1L, 'ev... | {'and': 0.2613647641344075... |          0          |\n",
       "| {'a': 1L, 'singer': 1L, 'w... | {'a': 0.31129193809091477,... |          0          |\n",
       "| {'enjoy': 1L, 'and': 3L, '... | {'enjoy': 3.42651518964644... |          1          |\n",
       "| {'all': 1L, 'simple': 1L, ... | {'all': 1.4696759700589417... |          1          |\n",
       "| {'sentence': 1L, 'text': 1... | {'sentence': 5.99146454710... |          1          |\n",
       "| {'enjoy': 1L, 'and': 1L, '... | {'enjoy': 3.42651518964644... |          1          |\n",
       "| {'account': 1L, 'would': 1... | {'account': 4.382026634673... |          1          |\n",
       "| {'and': 1L, 'secondly': 1L... | {'and': 0.2613647641344075... |          0          |\n",
       "| {'and': 2L, 'because': 1L,... | {'and': 0.522729528268815,... |          1          |\n",
       "| {'all': 1L, 'simple': 1L, ... | {'all': 1.4696759700589417... |          1          |\n",
       "+-------------------------------+-------------------------------+---------------------+\n",
       "+-----------------+-------------------------+---------------------+------------------------+\n",
       "| dvd_predictions | electronics_predictions | apparel_predictions | automotive_predictions |\n",
       "+-----------------+-------------------------+---------------------+------------------------+\n",
       "|        0        |            1            |          1          |           0            |\n",
       "|        1        |            0            |          0          |           0            |\n",
       "|        1        |            1            |          1          |           0            |\n",
       "|        1        |            1            |          1          |           0            |\n",
       "|        0        |            1            |          0          |           0            |\n",
       "|        1        |            1            |          1          |           0            |\n",
       "|        0        |            0            |          0          |           0            |\n",
       "|        0        |            0            |          0          |           0            |\n",
       "|        1        |            1            |          1          |           0            |\n",
       "|        1        |            1            |          1          |           0            |\n",
       "+-----------------+-------------------------+---------------------+------------------------+\n",
       "+------------------+--------------------+-------------------------+---------------------------+\n",
       "| baby_predictions | beauty_predictions | cameraphoto_predictions | computervideo_predictions |\n",
       "+------------------+--------------------+-------------------------+---------------------------+\n",
       "|        1         |         1          |            1            |             1             |\n",
       "|        1         |         1          |            0            |             1             |\n",
       "|        1         |         1          |            1            |             1             |\n",
       "|        1         |         1          |            1            |             1             |\n",
       "|        1         |         1          |            0            |             1             |\n",
       "|        1         |         1          |            0            |             1             |\n",
       "|        1         |         1          |            0            |             1             |\n",
       "|        1         |         1          |            0            |             1             |\n",
       "|        1         |         1          |            1            |             1             |\n",
       "|        1         |         1          |            1            |             1             |\n",
       "+------------------+--------------------+-------------------------+---------------------------+\n",
       "+---------------------+---------------------+----------------------------+----------------------------+\n",
       "| gourmet_predictions | grocery_predictions | healthpersonal_predictions | jewelrywatches_predictions |\n",
       "+---------------------+---------------------+----------------------------+----------------------------+\n",
       "|          1          |          1          |             0              |             1              |\n",
       "|          1          |          1          |             0              |             1              |\n",
       "|          1          |          1          |             1              |             1              |\n",
       "|          1          |          1          |             1              |             1              |\n",
       "|          1          |          1          |             1              |             1              |\n",
       "|          1          |          1          |             1              |             1              |\n",
       "|          1          |          1          |             0              |             1              |\n",
       "|          1          |          1          |             1              |             1              |\n",
       "|          1          |          1          |             0              |             1              |\n",
       "|          1          |          1          |             1              |             1              |\n",
       "+---------------------+---------------------+----------------------------+----------------------------+\n",
       "[400 rows x 20 columns]\n",
       "Note: Only the head of the SFrame is printed.\n",
       "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bookstestgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Fscores={}\n",
    "Accuracies={}\n",
    "book_dist_domains = ['electronics','dvd','kitchen','apparel','automotive','baby','beauty','cameraphoto','computervideo','gourmet','grocery','healthpersonal','jewelrywatches']\n",
    "    \n",
    "for d in book_dist_domains:\n",
    "    Fscores[d] = graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl[str(d)+'_predictions'])\n",
    "    Accuracies[d] = graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl[str(d)+'_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dvd_electronics_distance = gl.distances.cosine(electronics_wordcount,dvdgl_wordcount)\n",
    "#fscore={}\n",
    "#accuracies={}\n",
    "cosine_distances_tobook={}\n",
    "#kitchen_electronics_distance = gl.distances.euclidean(electronics_wordcount,kitchengl_wordcount)\n",
    "cosine_distances_tobook['electronics'] = gl.distances.cosine(booksgl_wordcount,electronics_wordcount)\n",
    "#dvd_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,dvdgl_wordcount)\n",
    "cosine_distances_tobook['dvd'] = gl.distances.cosine(booksgl_wordcount,dvdgl_wordcount)\n",
    "#books_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,booksgl_wordcount)\n",
    "cosine_distances_tobook['kitchen'] = gl.distances.cosine(booksgl_wordcount,kitchengl_wordcount)\n",
    "\n",
    "#apparel_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,apparelgl_wordcount)\n",
    "cosine_distances_tobook['apparel'] = gl.distances.cosine(booksgl_wordcount,apparelgl_wordcount)\n",
    "#automotive_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,automotivegl_wordcount)\n",
    "cosine_distances_tobook['automotive'] = gl.distances.cosine(booksgl_wordcount,automotivegl_wordcount)\n",
    "#baby_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,babygl_wordcount)\n",
    "cosine_distances_tobook['baby'] = gl.distances.cosine(booksgl_wordcount,babygl_wordcount)\n",
    "#beauty_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,beautygl_wordcount)\n",
    "cosine_distances_tobook['beauty'] = gl.distances.cosine(booksgl_wordcount,beautygl_wordcount)\n",
    "#cameraphoto_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,cameraphotogl_wordcount)\n",
    "cosine_distances_tobook['cameraphoto'] = gl.distances.cosine(booksgl_wordcount,cameraphotogl_wordcount)\n",
    "#computervideo_games_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,computervideo_gamesgl_wordcount)\n",
    "cosine_distances_tobook['computervideo'] = gl.distances.cosine(booksgl_wordcount,computervideo_gamesgl_wordcount)\n",
    "#gourmet_food_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,gourmet_foodgl_wordcount)\n",
    "cosine_distances_tobook['gourmet'] = gl.distances.cosine(booksgl_wordcount,gourmet_foodgl_wordcount)\n",
    "#grocery_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,grocerygl_wordcount)\n",
    "cosine_distances_tobook['grocery'] = gl.distances.cosine(booksgl_wordcount,grocerygl_wordcount)\n",
    "#healthpersonal_care_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,healthpersonal_caregl_wordcount)\n",
    "cosine_distances_tobook['healthpersonal'] = gl.distances.cosine(booksgl_wordcount,healthpersonal_caregl_wordcount)\n",
    "#jewelrywatches_kitchen_distance = gl.distances.euclidean(kitchengl_wordcount,jewelrywatchesgl_wordcount)\n",
    "cosine_distances_tobook['jewelrywatches'] = gl.distances.cosine(booksgl_wordcount,jewelrywatchesgl_wordcount)\n",
    "\n",
    "book_dist_domains = ['electronics','dvd','kitchen','apparel','automotive','baby','beauty','cameraphoto','computervideo','gourmet','grocery','healthpersonal','jewelrywatches']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies\n",
      "('gourmet', 0.5)\n",
      "('jewelrywatches', 0.5)\n",
      "('automotive', 0.5)\n",
      "('grocery', 0.5025)\n",
      "('computervideo', 0.5025)\n",
      "('beauty', 0.505)\n",
      "('baby', 0.6125)\n",
      "('kitchen', 0.645)\n",
      "('healthpersonal', 0.6475)\n",
      "('apparel', 0.6825)\n",
      "('cameraphoto', 0.685)\n",
      "('electronics', 0.69)\n",
      "('dvd', 0.765)\n",
      "fscores\n",
      "('automotive', 0.0)\n",
      "('gourmet', 0.6666666666666666)\n",
      "('jewelrywatches', 0.6666666666666666)\n",
      "('kitchen', 0.6666666666666667)\n",
      "('grocery', 0.667779632721202)\n",
      "('computervideo', 0.667779632721202)\n",
      "('beauty', 0.6688963210702341)\n",
      "('healthpersonal', 0.6713286713286714)\n",
      "('electronics', 0.69)\n",
      "('apparel', 0.6939759036144578)\n",
      "('cameraphoto', 0.6941747572815534)\n",
      "('baby', 0.7080979284369114)\n",
      "('dvd', 0.7751196172248804)\n",
      "distances\n",
      "('dvd', 156.70035098875815)\n",
      "('electronics', 214.9116097375849)\n",
      "('kitchen', 215.7683943491261)\n",
      "('healthpersonal', 223.29576798497547)\n",
      "('cameraphoto', 225.03333086456325)\n",
      "('apparel', 225.27094797154825)\n",
      "('computervideo', 226.23660181323444)\n",
      "('baby', 228.10962276940444)\n",
      "('beauty', 229.1331490640322)\n",
      "('grocery', 231.27689032845456)\n",
      "('gourmet', 231.57720095035262)\n",
      "('jewelrywatches', 233.25736858671797)\n",
      "('automotive', 234.77861912874434)\n"
     ]
    }
   ],
   "source": [
    "sorted_Fscores = sorted(Fscores.items(), key=lambda x:x[1])\n",
    "sorted_Accuracies = sorted(Accuracies.items(), key=lambda x:x[1])\n",
    "sorted_euclideandistances = sorted(euclidean_distances_tobook.items(), key=lambda x:x[1])\n",
    "print \"accuracies\"\n",
    "for d in range(len(sorted_Fscores)):\n",
    "    print sorted_Accuracies[d]\n",
    "print \"fscores\"\n",
    "for d in range(len(sorted_Fscores)):\n",
    "    print sorted_Fscores[d]\n",
    "print \"distances\"\n",
    "for d in range(len(sorted_Fscores)):\n",
    "    #print \"euclidean_distance\"\n",
    "    print sorted_euclideandistances[d]\n",
    "    #print sorted_Fscores[d]\n",
    "    #print sorted_Accuracies[d]\n",
    "    #print sorted_euclideandistances[d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from numpy import linalg as LA\n",
    "import numpy as np\n",
    "def euclidean_norm_dict(vec):\n",
    "    vector=[]\n",
    "    #for v in vec:\n",
    "        #vector.append(v)\n",
    "    enorm = LA.norm(vec)\n",
    "    #for i in vec:\n",
    "        #vec[i] = float(vec[i]/enorm)\n",
    "    return vec / enorm\n",
    "def compute_weights(vec):\n",
    "    newvec=[]\n",
    "    for v in vec:\n",
    "        e = (1-v) / v\n",
    "        weight = (1/2) * np.log(e)\n",
    "        newvec.append(weight)\n",
    "    return newvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from numpy import linalg as LA\n",
    "import numpy as np\n",
    "def euclidean_norm_dict(vec):\n",
    "    vector=[]\n",
    "    #for v in vec:\n",
    "        #vector.append(v)\n",
    "    enorm = LA.norm(vec)\n",
    "    #for i in vec:\n",
    "        #vec[i] = float(vec[i]/enorm)\n",
    "    return vec / enorm\n",
    "def compute_weights(vec):\n",
    "    newvec=[]\n",
    "    for v in vec:\n",
    "        e = (1-v) / v\n",
    "        weight = (1/2) * np.log(e)\n",
    "        newvec.append(weight)\n",
    "    return newvec\n",
    "\n",
    "##largestdistances=[0.743533140113,0.60353417065,0.584761632679,0.572767971981,0.544679039307,0.55875,0.543115351339]\n",
    "##lowestdistances=[0.339159452128,0.457601529338,0.462982751863,0.488780265727,0.491315411726,0.503708849348]\n",
    "##lowestdist_errors =[0.21,0.2125,0.255,0.225,0.20375,0.23]\n",
    "##highestdist_errors =[0.4925,0.4625,0.485,0.45125,0.44125,0.47125]\n",
    "##lowestdist_weights = compute_weights(lowestdist_errors)\n",
    "#############################\n",
    "#euclidean\n",
    "largestdistances=[234.77861912874434,233.25736858671797,231.57720095035262,231.27689032845456,229.1331490640322,228.10962276940444]\n",
    "lowestdistances=[156.70035098875815,214.9116097375849,215.7683943491261,223.29576798497547,225.03333086456325,225.27094797154825]\n",
    "\n",
    "#largestdistances=[238.9979079406345,237.49947368362734,231.57720095035262,231.27689032845456,229.1331490640322,232.38975881049493]\n",
    "#lowestdistances=[172.89881433948585,215.11624764298952,216.13190416965284,227.62688769123915,229.35343904114453,229.61707253599414]\n",
    "\n",
    "lowestdist_errors =[0.21,0.2125,0.255,0.225,0.20375,0.23]\n",
    "highestdist_errors =[0.4925,0.4625,0.485,0.45125,0.44125,0.47125]\n",
    "lowestdist_weights = compute_weights(lowestdist_errors)\n",
    "\n",
    "##############################\n",
    "highestdist_weights = compute_weights(highestdist_errors)\n",
    "Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "normalized_smallestdistances = euclidean_norm_dict(lowestdistances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#largestdistances\n",
    "#Minor_domains\n",
    "\n",
    "def weighted_predict_largedist(sframe,normalized_largestdistances,threshold):\n",
    "    weighted_pos_predictions=0\n",
    "    weighted_neg_predictions=0\n",
    "    for i in range(len(Minor_domains)):\n",
    "        weighted_pos_predictions+=normalized_largestdistances[i]*sframe[Minor_domains[i]]\n",
    "        weighted_neg_predictions+= normalized_largestdistances[i]*sframe[Minor_domains[i]]\n",
    "    weighted_neg_predictions = weighted_neg_predictions*-1\n",
    "    totalweight=sum(normalized_largestdistances)\n",
    "    if weighted_pos_predictions>(totalweight*threshold): #weighted_neg_predictions<weighted_pos_predictions:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    #Major_domains\n",
    "\n",
    "def weighted_predict_smalldist(sframe,normalized_smallestdistances,threshold):\n",
    "    weighted_pos_predictions=0\n",
    "    weighted_neg_predictions=0\n",
    "    for i in range(len(Major_domains)):\n",
    "        weighted_pos_predictions+=normalized_smallestdistances[i]*sframe[Major_domains[i]]\n",
    "        weighted_neg_predictions+= normalized_smallestdistances[i]*sframe[Major_domains[i]]\n",
    "    weighted_neg_predictions = weighted_neg_predictions*-1\n",
    "    totalweight=sum(normalized_smallestdistances)\n",
    "    if weighted_pos_predictions>(totalweight*threshold): #weighted_neg_predictions<weighted_pos_predictions:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def weighted_predict_smalldist_largedistances(sframe,normalized_smallestdistances,normalized_largestdistances,threshold):\n",
    "    weighted_pos_predictions=0\n",
    "    weighted_neg_predictions=0\n",
    "    for i in range(len(Major_domains)):\n",
    "        weighted_pos_predictions+=normalized_smallestdistances[i]*sframe[Major_domains[i]]\n",
    "        weighted_neg_predictions+= normalized_smallestdistances[i]*sframe[Major_domains[i]]\n",
    "    for i in range(len(Minor_domains)):\n",
    "        weighted_pos_predictions+=normalized_largestdistances[i]*sframe[Minor_domains[i]]\n",
    "        weighted_neg_predictions+= normalized_largestdistances[i]*sframe[Minor_domains[i]]\n",
    "    weight1=sum(normalized_smallestdistances)\n",
    "    weight2=sum(normalized_largestdistances)\n",
    "    totalweight = weight1 + weight2\n",
    "    if weighted_pos_predictions>(totalweight*threshold): #weighted_neg_predictions<weighted_pos_predictions:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority_predictions' model Fscore\n",
      "0.666666666667\n",
      "minority_predictions' model Accuracy\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "#0.5 threshold\n",
    "threshold=0.5\n",
    "bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "print \"minority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions'])\n",
    "print \"minority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority_predictions' model Fscore\n",
      "0.667779632721\n",
      "minority_predictions' model Accuracy\n",
      "0.5025\n"
     ]
    }
   ],
   "source": [
    "#0.6 threshold\n",
    "threshold=0.6\n",
    "bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "print \"minority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions'])\n",
    "print \"minority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority_predictions' model Fscore\n",
      "0.710775047259\n",
      "minority_predictions' model Accuracy\n",
      "0.6175\n"
     ]
    }
   ],
   "source": [
    "#0.7 threshold\n",
    "threshold=0.7\n",
    "bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "print \"minority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions'])\n",
    "print \"minority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority_predictions' model Fscore\n",
      "0.710775047259\n",
      "minority_predictions' model Accuracy\n",
      "0.6175\n"
     ]
    }
   ],
   "source": [
    "#0.8 threshold\n",
    "threshold=0.8\n",
    "bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "print \"minority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions'])\n",
    "print \"minority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority_predictions' model Fscore\n",
      "0.0\n",
      "minority_predictions' model Accuracy\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "#0.9 threshold\n",
    "threshold=0.9\n",
    "bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "print \"minority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions'])\n",
    "print \"minority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.717277486911\n",
      "majority_predictions' model Accuracy\n",
      "0.73\n"
     ]
    }
   ],
   "source": [
    "#0.6 threshold\n",
    "threshold=0.6\n",
    "bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.715294117647\n",
      "majority_predictions' model Accuracy\n",
      "0.6975\n"
     ]
    }
   ],
   "source": [
    "#0.5 threshold\n",
    "threshold=0.5\n",
    "bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.660714285714\n",
      "majority_predictions' model Accuracy\n",
      "0.715\n"
     ]
    }
   ],
   "source": [
    "#0.7 threshold\n",
    "threshold=0.7\n",
    "bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.646153846154\n",
      "majority_predictions' model Accuracy\n",
      "0.7125\n"
     ]
    }
   ],
   "source": [
    "#0.8 threshold\n",
    "threshold=0.8\n",
    "bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.483516483516\n",
      "majority_predictions' model Accuracy\n",
      "0.6475\n"
     ]
    }
   ],
   "source": [
    "#0.9 threshold\n",
    "threshold=0.9\n",
    "bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.773333333333\n",
      "majority_predictions' model Accuracy\n",
      "0.745\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.6 using all models\n",
    "threshold=0.6\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.727272727273\n",
      "majority_predictions' model Accuracy\n",
      "0.64\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.5 using all models\n",
    "threshold=0.5\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.716577540107\n",
      "majority_predictions' model Accuracy\n",
      "0.735\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.7 using all models\n",
    "threshold=0.7\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.637770897833\n",
      "majority_predictions' model Accuracy\n",
      "0.7075\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.8 using all models\n",
    "threshold=0.8\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.483516483516\n",
      "majority_predictions' model Accuracy\n",
      "0.6475\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.9 using all models\n",
    "threshold=0.9\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies\n",
      "('gourmet', 0.5)\n",
      "('jewelrywatches', 0.5)\n",
      "('automotive', 0.5)\n",
      "('grocery', 0.5025)\n",
      "('computervideo', 0.5025)\n",
      "('beauty', 0.505)\n",
      "('baby', 0.6125)\n",
      "('kitchen', 0.645)\n",
      "('healthpersonal', 0.6475)\n",
      "('apparel', 0.6825)\n",
      "('cameraphoto', 0.685)\n",
      "('electronics', 0.69)\n",
      "('dvd', 0.765)\n",
      "fscores\n",
      "('automotive', 0.0)\n",
      "('gourmet', 0.6666666666666666)\n",
      "('jewelrywatches', 0.6666666666666666)\n",
      "('kitchen', 0.6666666666666667)\n",
      "('grocery', 0.667779632721202)\n",
      "('computervideo', 0.667779632721202)\n",
      "('beauty', 0.6688963210702341)\n",
      "('healthpersonal', 0.6713286713286714)\n",
      "('electronics', 0.69)\n",
      "('apparel', 0.6939759036144578)\n",
      "('cameraphoto', 0.6941747572815534)\n",
      "('baby', 0.7080979284369114)\n",
      "('dvd', 0.7751196172248804)\n",
      "distances\n",
      "('dvd', 0.23901334588341028)\n",
      "('electronics', 0.5475787563806471)\n",
      "('kitchen', 0.5546099411237762)\n",
      "('healthpersonal', 0.6240552059503661)\n",
      "('cameraphoto', 0.6424281501211625)\n",
      "('apparel', 0.6449664641578016)\n",
      "('computervideo', 0.6557890706088274)\n",
      "('baby', 0.6777351856726836)\n",
      "('beauty', 0.6904890029305283)\n",
      "('grocery', 0.7193428294891278)\n",
      "('gourmet', 0.7236390000090482)\n",
      "('jewelrywatches', 0.7491597026757475)\n",
      "('automotive', 0.7749332064910006)\n"
     ]
    }
   ],
   "source": [
    "sorted_Fscores = sorted(Fscores.items(), key=lambda x:x[1])\n",
    "sorted_Accuracies = sorted(Accuracies.items(), key=lambda x:x[1])\n",
    "sorted_cosinedistances = sorted(cosine_distances_tobook.items(), key=lambda x:x[1])\n",
    "print \"accuracies\"\n",
    "for d in range(len(sorted_Fscores)):\n",
    "    print sorted_Accuracies[d]\n",
    "print \"fscores\"\n",
    "for d in range(len(sorted_Fscores)):\n",
    "    print sorted_Fscores[d]\n",
    "print \"distances\"\n",
    "for d in range(len(sorted_Fscores)):\n",
    "    #print \"euclidean_distance\"\n",
    "    print sorted_cosinedistances[d]\n",
    "    #print sorted_Fscores[d]\n",
    "    #print sorted_Accuracies[d]\n",
    "    #print sorted_euclideandistances[d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "import numpy as np\n",
    "def euclidean_norm_dict(vec):\n",
    "    vector=[]\n",
    "    #for v in vec:\n",
    "        #vector.append(v)\n",
    "    enorm = LA.norm(vec)\n",
    "    #for i in vec:\n",
    "        #vec[i] = float(vec[i]/enorm)\n",
    "    return vec / enorm\n",
    "def compute_weights(vec):\n",
    "    newvec=[]\n",
    "    for v in vec:\n",
    "        e = (1-v) / v\n",
    "        weight = (1/2) * np.log(e)\n",
    "        newvec.append(weight)\n",
    "    return newvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from numpy import linalg as LA\n",
    "import numpy as np\n",
    "def euclidean_norm_dict(vec):\n",
    "    vector=[]\n",
    "    #for v in vec:\n",
    "        #vector.append(v)\n",
    "    enorm = LA.norm(vec)\n",
    "    #for i in vec:\n",
    "        #vec[i] = float(vec[i]/enorm)\n",
    "    return vec / enorm\n",
    "def compute_weights(vec):\n",
    "    newvec=[]\n",
    "    for v in vec:\n",
    "        e = (1-v) / v\n",
    "        weight = (1/2) * np.log(e)\n",
    "        newvec.append(weight)\n",
    "    return newvec\n",
    "\n",
    "##largestdistances=[0.743533140113,0.60353417065,0.584761632679,0.572767971981,0.544679039307,0.55875,0.543115351339]\n",
    "##lowestdistances=[0.339159452128,0.457601529338,0.462982751863,0.488780265727,0.491315411726,0.503708849348]\n",
    "##lowestdist_errors =[0.21,0.2125,0.255,0.225,0.20375,0.23]\n",
    "##highestdist_errors =[0.4925,0.4625,0.485,0.45125,0.44125,0.47125]\n",
    "##lowestdist_weights = compute_weights(lowestdist_errors)\n",
    "#############################\n",
    "#euclidean\n",
    "largestdistances=[0.7749332064910006,0.7491597026757475,0.7236390000090482,0.7193428294891278,0.6904890029305283,0.6777351856726836]\n",
    "lowestdistances=[0.23901334588341028,0.5475787563806471,0.5546099411237762,0.6240552059503661, 0.6424281501211625,0.6279728206924405]\n",
    "\n",
    "#largestdistances=[0.7783551076050383,0.7529885746128953,0.7278146105451752,0.7235979596409374,0.6949575486126907,0.6821894656069148]\n",
    "#lowestdistances=[0.26233362308000074,0.5147535214228807,0.5215377483572382,0.6291649095620444, 0.6473852158466183,0.6502073539229547]\n",
    "lowestdist_errors =[0.21,0.2125,0.255,0.225,0.20375,0.23]\n",
    "highestdist_errors =[0.4925,0.4625,0.485,0.45125,0.44125,0.47125]\n",
    "lowestdist_weights = compute_weights(lowestdist_errors)\n",
    "\n",
    "##############################\n",
    "highestdist_weights = compute_weights(highestdist_errors)\n",
    "Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "normalized_smallestdistances = euclidean_norm_dict(lowestdistances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#largestdistances\n",
    "#Minor_domains\n",
    "\n",
    "def weighted_predict_largedist(sframe,normalized_largestdistances,threshold):\n",
    "    weighted_pos_predictions=0\n",
    "    weighted_neg_predictions=0\n",
    "    for i in range(len(Minor_domains)):\n",
    "        weighted_pos_predictions+=normalized_largestdistances[i]*sframe[Minor_domains[i]]\n",
    "        weighted_neg_predictions+= normalized_largestdistances[i]*sframe[Minor_domains[i]]\n",
    "    weighted_neg_predictions = weighted_neg_predictions*-1\n",
    "    totalweight=sum(normalized_largestdistances)\n",
    "    if weighted_pos_predictions>(totalweight*threshold): #weighted_neg_predictions<weighted_pos_predictions:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    #Major_domains\n",
    "\n",
    "def weighted_predict_smalldist(sframe,normalized_smallestdistances,threshold):\n",
    "    weighted_pos_predictions=0\n",
    "    weighted_neg_predictions=0\n",
    "    for i in range(len(Major_domains)):\n",
    "        weighted_pos_predictions+=normalized_smallestdistances[i]*sframe[Major_domains[i]]\n",
    "        weighted_neg_predictions+= normalized_smallestdistances[i]*sframe[Major_domains[i]]\n",
    "    weighted_neg_predictions = weighted_neg_predictions*-1\n",
    "    totalweight=sum(normalized_smallestdistances)\n",
    "    if weighted_pos_predictions>(totalweight*threshold): #weighted_neg_predictions<weighted_pos_predictions:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def weighted_predict_smalldist_largedistances(sframe,normalized_smallestdistances,normalized_largestdistances,threshold):\n",
    "    weighted_pos_predictions=0\n",
    "    weighted_neg_predictions=0\n",
    "    for i in range(len(Major_domains)):\n",
    "        weighted_pos_predictions+=normalized_smallestdistances[i]*sframe[Major_domains[i]]\n",
    "        weighted_neg_predictions+= normalized_smallestdistances[i]*sframe[Major_domains[i]]\n",
    "    for i in range(len(Minor_domains)):\n",
    "        weighted_pos_predictions+=normalized_largestdistances[i]*sframe[Minor_domains[i]]\n",
    "        weighted_neg_predictions+= normalized_largestdistances[i]*sframe[Minor_domains[i]]\n",
    "    weight1=sum(normalized_smallestdistances)\n",
    "    weight2=sum(normalized_largestdistances)\n",
    "    totalweight = weight1 + weight2\n",
    "    if weighted_pos_predictions>(totalweight*threshold): #weighted_neg_predictions<weighted_pos_predictions:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority_predictions' model Fscore\n",
      "0.666666666667\n",
      "minority_predictions' model Accuracy\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "#0.5 threshold\n",
    "threshold=0.5\n",
    "bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "print \"minority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions'])\n",
    "print \"minority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority_predictions' model Fscore\n",
      "0.667779632721\n",
      "minority_predictions' model Accuracy\n",
      "0.5025\n"
     ]
    }
   ],
   "source": [
    "#0.5 threshold\n",
    "threshold=0.6\n",
    "bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "print \"minority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions'])\n",
    "print \"minority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority_predictions' model Fscore\n",
      "0.710775047259\n",
      "minority_predictions' model Accuracy\n",
      "0.6175\n"
     ]
    }
   ],
   "source": [
    "#0.5 threshold\n",
    "threshold=0.7\n",
    "bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "print \"minority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions'])\n",
    "print \"minority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority_predictions' model Fscore\n",
      "0.710775047259\n",
      "minority_predictions' model Accuracy\n",
      "0.6175\n"
     ]
    }
   ],
   "source": [
    "#0.5 threshold\n",
    "threshold=0.8\n",
    "bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "print \"minority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions'])\n",
    "print \"minority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority_predictions' model Fscore\n",
      "0.0\n",
      "minority_predictions' model Accuracy\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "#0.5 threshold\n",
    "threshold=0.9\n",
    "bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "print \"minority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions'])\n",
    "print \"minority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.715294117647\n",
      "majority_predictions' model Accuracy\n",
      "0.6975\n"
     ]
    }
   ],
   "source": [
    "#0.5 threshold\n",
    "threshold=0.5\n",
    "bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.717277486911\n",
      "majority_predictions' model Accuracy\n",
      "0.73\n"
     ]
    }
   ],
   "source": [
    "#0.6 threshold\n",
    "threshold=0.6\n",
    "bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.660869565217\n",
      "majority_predictions' model Accuracy\n",
      "0.7075\n"
     ]
    }
   ],
   "source": [
    "#0.7 threshold\n",
    "threshold=0.7\n",
    "bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.646153846154\n",
      "majority_predictions' model Accuracy\n",
      "0.7125\n"
     ]
    }
   ],
   "source": [
    "#0.8 threshold\n",
    "threshold=0.8\n",
    "bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.487455197133\n",
      "majority_predictions' model Accuracy\n",
      "0.6425\n"
     ]
    }
   ],
   "source": [
    "#0.5 threshold\n",
    "threshold=0.9\n",
    "bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.727272727273\n",
      "majority_predictions' model Accuracy\n",
      "0.64\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.5 using all models\n",
    "threshold=0.5\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.758169934641\n",
      "majority_predictions' model Accuracy\n",
      "0.7225\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.6 using all models\n",
    "threshold=0.6\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.712401055409\n",
      "majority_predictions' model Accuracy\n",
      "0.7275\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.7 using all models\n",
    "threshold=0.7\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.637770897833\n",
      "majority_predictions' model Accuracy\n",
      "0.7075\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.8 using all models\n",
    "threshold=0.8\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.483516483516\n",
      "majority_predictions' model Accuracy\n",
      "0.6475\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.9 using all models\n",
    "threshold=0.9\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.727272727273\n",
      "majority_predictions' model Accuracy\n",
      "0.64\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.5 using all models\n",
    "threshold=0.5\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.758169934641\n",
      "majority_predictions' model Accuracy\n",
      "0.7225\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.6 using all models\n",
    "threshold=0.6\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.712401055409\n",
      "majority_predictions' model Accuracy\n",
      "0.7275\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.7 using all models\n",
    "threshold=0.7\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.637770897833\n",
      "majority_predictions' model Accuracy\n",
      "0.7075\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.8 using all models\n",
    "threshold=0.8\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.483516483516\n",
      "majority_predictions' model Accuracy\n",
      "0.6475\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.9 using all models\n",
    "threshold=0.9\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly Sampled Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bookstestgl  = gl.SFrame('booksgl_random_predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "import numpy as np\n",
    "def euclidean_norm_dict(vec):\n",
    "    vector=[]\n",
    "    #for v in vec:\n",
    "        #vector.append(v)\n",
    "    enorm = LA.norm(vec)\n",
    "    #for i in vec:\n",
    "        #vec[i] = float(vec[i]/enorm)\n",
    "    return vec / enorm\n",
    "def compute_weights(vec):\n",
    "    newvec=[]\n",
    "    for v in vec:\n",
    "        e = (1-v) / v\n",
    "        weight = (1/2) * np.log(e)\n",
    "        newvec.append(weight)\n",
    "    return newvec\n",
    "\n",
    "##largestdistances=[0.743533140113,0.60353417065,0.584761632679,0.572767971981,0.544679039307,0.55875,0.543115351339]\n",
    "##lowestdistances=[0.339159452128,0.457601529338,0.462982751863,0.488780265727,0.491315411726,0.503708849348]\n",
    "##lowestdist_errors =[0.21,0.2125,0.255,0.225,0.20375,0.23]\n",
    "##highestdist_errors =[0.4925,0.4625,0.485,0.45125,0.44125,0.47125]\n",
    "##lowestdist_weights = compute_weights(lowestdist_errors)\n",
    "#############################\n",
    "#euclidean\n",
    "largestdistances=[221.07238633533586,219.45614596087302,217.71081737019867,217.37755173890426,215.2812114421507,214.14481081735323]\n",
    "lowestdistances=[143.547901412734,199.8999749874922,200.7959162931358,209.28210625851412,210.8885961829136,211.24156787905167]\n",
    "lowestdist_errors =[0.21,0.2125,0.255,0.225,0.20375,0.23]\n",
    "highestdist_errors =[0.4925,0.4625,0.485,0.45125,0.44125,0.47125]\n",
    "lowestdist_weights = compute_weights(lowestdist_errors)\n",
    "\n",
    "##############################\n",
    "highestdist_weights = compute_weights(highestdist_errors)\n",
    "Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "normalized_smallestdistances = euclidean_norm_dict(lowestdistances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority_predictions' model Fscore\n",
      "0.667111407605\n",
      "minority_predictions' model Accuracy\n",
      "0.501\n"
     ]
    }
   ],
   "source": [
    "#0.5 threshold\n",
    "threshold=0.5\n",
    "bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "print \"minority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions'])\n",
    "print \"minority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority_predictions' model Fscore\n",
      "0.668002672011\n",
      "minority_predictions' model Accuracy\n",
      "0.503\n"
     ]
    }
   ],
   "source": [
    "#0.6 threshold\n",
    "threshold=0.6\n",
    "bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "print \"minority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions'])\n",
    "print \"minority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority_predictions' model Fscore\n",
      "0.697234352256\n",
      "minority_predictions' model Accuracy\n",
      "0.584\n"
     ]
    }
   ],
   "source": [
    "#0.7 threshold\n",
    "threshold=0.7\n",
    "bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "print \"minority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions'])\n",
    "print \"minority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority_predictions' model Fscore\n",
      "0.697234352256\n",
      "minority_predictions' model Accuracy\n",
      "0.584\n"
     ]
    }
   ],
   "source": [
    "#0.8 threshold\n",
    "threshold=0.8\n",
    "bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "print \"minority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions'])\n",
    "print \"minority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority_predictions' model Fscore\n",
      "0.0\n",
      "minority_predictions' model Accuracy\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "#0.9 threshold\n",
    "threshold=0.9\n",
    "bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "print \"minority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions'])\n",
    "print \"minority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.7734375\n",
      "majority_predictions' model Accuracy\n",
      "0.768\n"
     ]
    }
   ],
   "source": [
    "#0.5 threshold\n",
    "threshold=0.5\n",
    "bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.767950052029\n",
      "majority_predictions' model Accuracy\n",
      "0.777\n"
     ]
    }
   ],
   "source": [
    "#0.6 threshold\n",
    "threshold=0.6\n",
    "bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.71158392435\n",
      "majority_predictions' model Accuracy\n",
      "0.756\n"
     ]
    }
   ],
   "source": [
    "#0.7 threshold\n",
    "threshold=0.7\n",
    "bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.709987966306\n",
      "majority_predictions' model Accuracy\n",
      "0.759\n"
     ]
    }
   ],
   "source": [
    "#0.8 threshold\n",
    "threshold=0.8\n",
    "bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.507374631268\n",
      "majority_predictions' model Accuracy\n",
      "0.666\n"
     ]
    }
   ],
   "source": [
    "#0.9 threshold\n",
    "threshold=0.9\n",
    "bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.744006187162\n",
      "majority_predictions' model Accuracy\n",
      "0.669\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.5 using all models\n",
    "threshold=0.5\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.797421731123\n",
      "majority_predictions' model Accuracy\n",
      "0.78\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.6 using all models\n",
    "threshold=0.6\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.770680628272\n",
      "majority_predictions' model Accuracy\n",
      "0.781\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.7 using all models\n",
    "threshold=0.7\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.707021791768\n",
      "majority_predictions' model Accuracy\n",
      "0.758\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.8 using all models\n",
    "threshold=0.8\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.505169867061\n",
      "majority_predictions' model Accuracy\n",
      "0.665\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.9 using all models\n",
    "threshold=0.9\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "import numpy as np\n",
    "def euclidean_norm_dict(vec):\n",
    "    vector=[]\n",
    "    #for v in vec:\n",
    "        #vector.append(v)\n",
    "    enorm = LA.norm(vec)\n",
    "    #for i in vec:\n",
    "        #vec[i] = float(vec[i]/enorm)\n",
    "    return vec / enorm\n",
    "def compute_weights(vec):\n",
    "    newvec=[]\n",
    "    for v in vec:\n",
    "        e = (1-v) / v\n",
    "        weight = (1/2) * np.log(e)\n",
    "        newvec.append(weight)\n",
    "    return newvec\n",
    "\n",
    "##largestdistances=[0.743533140113,0.60353417065,0.584761632679,0.572767971981,0.544679039307,0.55875,0.543115351339]\n",
    "##lowestdistances=[0.339159452128,0.457601529338,0.462982751863,0.488780265727,0.491315411726,0.503708849348]\n",
    "##lowestdist_errors =[0.21,0.2125,0.255,0.225,0.20375,0.23]\n",
    "##highestdist_errors =[0.4925,0.4625,0.485,0.45125,0.44125,0.47125]\n",
    "##lowestdist_weights = compute_weights(lowestdist_errors)\n",
    "#############################\n",
    "#euclidean\n",
    "largestdistances=[0.7629636773607741,0.735536866547934,0.7089925396063376,0.7042264938811418,0.6758400856548605,0.661642545621075]\n",
    "lowestdistances=[0.220478158447052,0.5216979848670661,0.5288849650867541,0.6070688722672171, 0.6242299946258284,0.6279728206924405]\n",
    "lowestdist_errors =[0.21,0.2125,0.255,0.225,0.20375,0.23]\n",
    "highestdist_errors =[0.4925,0.4625,0.485,0.45125,0.44125,0.47125]\n",
    "lowestdist_weights = compute_weights(lowestdist_errors)\n",
    "\n",
    "##############################\n",
    "highestdist_weights = compute_weights(highestdist_errors)\n",
    "Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "normalized_smallestdistances = euclidean_norm_dict(lowestdistances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority_predictions' model Fscore\n",
      "0.667111407605\n",
      "minority_predictions' model Accuracy\n",
      "0.501\n"
     ]
    }
   ],
   "source": [
    "#0.5 threshold\n",
    "threshold=0.5\n",
    "bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "print \"minority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions'])\n",
    "print \"minority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority_predictions' model Fscore\n",
      "0.668002672011\n",
      "minority_predictions' model Accuracy\n",
      "0.503\n"
     ]
    }
   ],
   "source": [
    "#0.6 threshold\n",
    "threshold=0.6\n",
    "bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "print \"minority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions'])\n",
    "print \"minority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority_predictions' model Fscore\n",
      "0.697234352256\n",
      "minority_predictions' model Accuracy\n",
      "0.584\n"
     ]
    }
   ],
   "source": [
    "#0.7 threshold\n",
    "threshold=0.7\n",
    "bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "print \"minority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions'])\n",
    "print \"minority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority_predictions' model Fscore\n",
      "0.697234352256\n",
      "minority_predictions' model Accuracy\n",
      "0.584\n"
     ]
    }
   ],
   "source": [
    "#0.8 threshold\n",
    "threshold=0.8\n",
    "bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "print \"minority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions'])\n",
    "print \"minority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minority_predictions' model Fscore\n",
      "0.0\n",
      "minority_predictions' model Accuracy\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "#0.9 threshold\n",
    "threshold=0.9\n",
    "bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "print \"minority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions'])\n",
    "print \"minority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.7734375\n",
      "majority_predictions' model Accuracy\n",
      "0.768\n"
     ]
    }
   ],
   "source": [
    "#0.5 threshold\n",
    "threshold=0.5\n",
    "bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.762605042017\n",
      "majority_predictions' model Accuracy\n",
      "0.774\n"
     ]
    }
   ],
   "source": [
    "#0.6 threshold\n",
    "threshold=0.6\n",
    "bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.712138728324\n",
      "majority_predictions' model Accuracy\n",
      "0.751\n"
     ]
    }
   ],
   "source": [
    "#0.7 threshold\n",
    "threshold=0.7\n",
    "bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.689655172414\n",
      "majority_predictions' model Accuracy\n",
      "0.748\n"
     ]
    }
   ],
   "source": [
    "#0.8 threshold\n",
    "threshold=0.8\n",
    "bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.526466380544\n",
      "majority_predictions' model Accuracy\n",
      "0.669\n"
     ]
    }
   ],
   "source": [
    "#0.9 threshold\n",
    "threshold=0.9\n",
    "bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.749221183801\n",
      "majority_predictions' model Accuracy\n",
      "0.678\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.5 using all models\n",
    "threshold=0.5\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.787387387387\n",
      "majority_predictions' model Accuracy\n",
      "0.764\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.6 using all models\n",
    "threshold=0.6\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.772256728778\n",
      "majority_predictions' model Accuracy\n",
      "0.78\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.7 using all models\n",
    "threshold=0.7\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.707021791768\n",
      "majority_predictions' model Accuracy\n",
      "0.758\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.8 using all models\n",
    "threshold=0.8\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majority_predictions' model Fscore\n",
      "0.505169867061\n",
      "majority_predictions' model Accuracy\n",
      "0.665\n"
     ]
    }
   ],
   "source": [
    "#threshold 0.9 using all models\n",
    "threshold=0.9\n",
    "bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "print \"majority_predictions' model Fscore\"\n",
    "print graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions'])\n",
    "print \"majority_predictions' model Accuracy\"\n",
    "print graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "import numpy as np\n",
    "def euclidean_norm_dict(vec):\n",
    "    vector=[]\n",
    "    #for v in vec:\n",
    "        #vector.append(v)\n",
    "    enorm = LA.norm(vec)\n",
    "    #for i in vec:\n",
    "        #vec[i] = float(vec[i]/enorm)\n",
    "    return vec / enorm\n",
    "def compute_weights(vec):\n",
    "    newvec=[]\n",
    "    for v in vec:\n",
    "        e = (1-v) / v\n",
    "        weight = (1/2) * np.log(e)\n",
    "        newvec.append(weight)\n",
    "    return newvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for random dataset 1\n",
      "euclidean distance results for the minority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.781\n",
      "Cosine distance results for the minority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.774\n",
      "data set 1 was tested!\n",
      "results for random dataset 2\n",
      "euclidean distance results for the minority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.775\n",
      "Cosine distance results for the minority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.771\n",
      "data set 2 was tested!\n",
      "results for random dataset 3\n",
      "euclidean distance results for the minority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.782\n",
      "Cosine distance results for the minority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.775\n",
      "data set 3 was tested!\n",
      "results for random dataset 4\n",
      "euclidean distance results for the minority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.772\n",
      "Cosine distance results for the minority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.762\n",
      "data set 4 was tested!\n",
      "results for random dataset 5\n",
      "euclidean distance results for the minority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.785\n",
      "Cosine distance results for the minority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.761\n",
      "data set 5 was tested!\n",
      "max cosine acc\n",
      "[0.774, 0.771, 0.775, 0.762, 0.761]\n",
      "max euc acc\n",
      "[0.781, 0.775, 0.782, 0.772, 0.785]\n"
     ]
    }
   ],
   "source": [
    "counter=0\n",
    "max_euc_accs=[]\n",
    "max_cosine_accs=[]\n",
    "for d in range(1,6):\n",
    "    i=d\n",
    "    counter+=1\n",
    "    if i==1:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions')\n",
    "    else:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions_'+str(i))\n",
    "        \n",
    "    print \"results for random dataset \"+str(d)\n",
    "    print \"euclidean distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    largestdistances=[234.77861912874434,233.25736858671797,231.57720095035262,231.27689032845456,229.1331490640322,228.10962276940444]\n",
    "    lowestdistances=[156.70035098875815,214.9116097375849,215.7683943491261,223.29576798497547,225.03333086456325,225.27094797154825]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    thresholds = [0.5,0.6,0.7,0.8,0.9]\n",
    "    euc_Fscores = []\n",
    "    euc_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "    print \"euclidean distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    euc_major_Fscores=[]\n",
    "    euc_major_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    print \"euclidean distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    #threshold using all models\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print \"max euc acc\"\n",
    "    print max(euc_Accs)\n",
    "    max_euc_accs.append(max(euc_Accs))\n",
    "    \n",
    "    largestdistances=[0.7749332064910006,0.7491597026757475,0.7236390000090482,0.7193428294891278,0.6904890029305283,0.6777351856726836]\n",
    "    lowestdistances=[0.23901334588341028,0.5475787563806471,0.5546099411237762,0.6240552059503661, 0.6424281501211625,0.6279728206924405]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    \n",
    "    print \"Cosine distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    cosine_Accs=[]\n",
    "    cosine_Fscores=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "    \n",
    "    print \"Cosine distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    \n",
    "    print \"Cosine distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    \n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        print \"max euc acc\"\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    print max(cosine_Accs)\n",
    "    max_cosine_accs.append(max(cosine_Accs))\n",
    "    print \"data set \"+str(counter)+' was tested!'\n",
    "print \"max cosine acc\"\n",
    "print max_cosine_accs\n",
    "print \"max euc acc\"\n",
    "print max_euc_accs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_max_acc_euc = 0.7790\n",
    "\n",
    "avg_max_acc_cosine = 0.7686"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for random dataset 1\n",
      "euclidean distance results for the minority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.781\n",
      "Cosine distance results for the minority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.777\n",
      "data set 1 was tested!\n",
      "results for random dataset 2\n",
      "euclidean distance results for the minority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.775\n",
      "Cosine distance results for the minority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.774\n",
      "data set 2 was tested!\n",
      "results for random dataset 3\n",
      "euclidean distance results for the minority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.782\n",
      "Cosine distance results for the minority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.779\n",
      "data set 3 was tested!\n",
      "results for random dataset 4\n",
      "euclidean distance results for the minority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.772\n",
      "Cosine distance results for the minority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.766\n",
      "data set 4 was tested!\n",
      "results for random dataset 5\n",
      "euclidean distance results for the minority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.785\n",
      "Cosine distance results for the minority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.766\n",
      "data set 5 was tested!\n",
      "max cosine acc\n",
      "[0.777, 0.774, 0.779, 0.766, 0.766]\n",
      "max euc acc\n",
      "[0.781, 0.775, 0.782, 0.772, 0.785]\n"
     ]
    }
   ],
   "source": [
    "counter=0\n",
    "max_euc_accs=[]\n",
    "max_cosine_accs=[]\n",
    "for d in range(1,6):\n",
    "    i=d\n",
    "    counter+=1\n",
    "    if i==1:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions')\n",
    "    else:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions_'+str(i))\n",
    "        \n",
    "    print \"results for random dataset \"+str(d)\n",
    "    print \"euclidean distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    largestdistances=[234.77861912874434,233.25736858671797,231.57720095035262,231.27689032845456,229.1331490640322,228.10962276940444]\n",
    "    lowestdistances=[156.70035098875815,214.9116097375849,215.7683943491261,223.29576798497547,225.03333086456325,225.27094797154825]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    thresholds = [0.5,0.6,0.7,0.8,0.9]\n",
    "    euc_Fscores = []\n",
    "    euc_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "    print \"euclidean distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    euc_major_Fscores=[]\n",
    "    euc_major_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    print \"euclidean distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    #threshold using all models\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print \"max euc acc\"\n",
    "    print max(euc_Accs)\n",
    "    max_euc_accs.append(max(euc_Accs))\n",
    "    \n",
    "    largestdistances=[0.7749332064910006,0.7491597026757475,0.7236390000090482,0.7193428294891278,0.6904890029305283,0.6777351856726836]\n",
    "    lowestdistances=[0.23901334588341028,0.5475787563806471,0.5546099411237762,0.6240552059503661, 0.6424281501211625,0.6279728206924405]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    \n",
    "    print \"Cosine distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    cosine_Accs=[]\n",
    "    cosine_Fscores=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "    \n",
    "    print \"Cosine distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    \n",
    "    print \"Cosine distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    \n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        print \"max euc acc\"\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    print max(cosine_Accs)\n",
    "    max_cosine_accs.append(max(cosine_Accs))\n",
    "    print \"data set \"+str(counter)+' was tested!'\n",
    "print \"max cosine acc\"\n",
    "print max_cosine_accs\n",
    "print \"max euc acc\"\n",
    "print max_euc_accs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for random dataset 1\n",
      "euclidean distance results for the minority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.781\n",
      "Cosine distance results for the minority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.78\n",
      "data set 1 was tested!\n",
      "results for random dataset 2\n",
      "euclidean distance results for the minority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.775\n",
      "Cosine distance results for the minority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.774\n",
      "data set 2 was tested!\n",
      "results for random dataset 3\n",
      "euclidean distance results for the minority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.782\n",
      "Cosine distance results for the minority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.782\n",
      "data set 3 was tested!\n",
      "results for random dataset 4\n",
      "euclidean distance results for the minority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.772\n",
      "Cosine distance results for the minority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.773\n",
      "data set 4 was tested!\n",
      "results for random dataset 5\n",
      "euclidean distance results for the minority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.785\n",
      "Cosine distance results for the minority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.77\n",
      "data set 5 was tested!\n",
      "max cosine acc\n",
      "[0.78, 0.774, 0.782, 0.773, 0.77]\n",
      "max euc acc\n",
      "[0.781, 0.775, 0.782, 0.772, 0.785]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "counter=0\n",
    "max_euc_accs=[]\n",
    "max_cosine_accs=[]\n",
    "eucthresholds_accs = defaultdict(list)\n",
    "eucthresholds_fscores = defaultdict(list)\n",
    "cosinethresholds_accs = defaultdict(list)\n",
    "cosinethresholds_fscores = defaultdict(list)\n",
    "\n",
    "euc_threshold_acc = defaultdict(list)\n",
    "cosine_threshold_acc = defaultdict(list)\n",
    "\n",
    "euc_threshold_fscore = defaultdict(list)\n",
    "cosine_threshold_fscore = defaultdict(list)\n",
    "for d in range(1,6):\n",
    "    i=d\n",
    "    counter+=1\n",
    "    if i==1:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions')\n",
    "    else:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions_'+str(i))\n",
    "        \n",
    "    print \"results for random dataset \"+str(d)\n",
    "    print \"euclidean distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    largestdistances=[234.77861912874434,233.25736858671797,231.57720095035262,231.27689032845456,229.1331490640322,228.10962276940444]\n",
    "    lowestdistances=[156.70035098875815,214.9116097375849,215.7683943491261,223.29576798497547,225.03333086456325,225.27094797154825]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    thresholds = [0.5,0.6,0.7,0.8,0.9]\n",
    "    euc_Fscores = []\n",
    "    euc_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "\n",
    "    print \"euclidean distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    euc_major_Fscores=[]\n",
    "    euc_major_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    print \"euclidean distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    #threshold using all models\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print \"max euc acc\"\n",
    "    print max(euc_Accs)\n",
    "    max_euc_accs.append(max(euc_Accs))\n",
    "    \n",
    "    largestdistances=[0.7749332064910006,0.7491597026757475,0.7236390000090482,0.7193428294891278,0.6904890029305283,0.6777351856726836]\n",
    "    lowestdistances=[0.23901334588341028,0.5475787563806471,0.5546099411237762,0.6240552059503661, 0.6424281501211625,0.6279728206924405]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    \n",
    "    print \"Cosine distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    cosine_Accs=[]\n",
    "    cosine_Fscores=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "    \n",
    "    print \"Cosine distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    \n",
    "    print \"Cosine distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    \n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        print \"max euc acc\"\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        cosine_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        cosine_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print max(cosine_Accs)\n",
    "    max_cosine_accs.append(max(cosine_Accs))\n",
    "    print \"data set \"+str(counter)+' was tested!'\n",
    "print \"max cosine acc\"\n",
    "print max_cosine_accs\n",
    "print \"max euc acc\"\n",
    "print max_euc_accs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thresholds = [0.5,0.6,0.7,0.8,0.9]\n",
    "euc_avg_th_acc = {}\n",
    "cosine_avg_th_acc = {}\n",
    "euc_avg_th_fscore = {}\n",
    "cosine_avg_th_fscore = {}\n",
    "\n",
    "for t in thresholds:\n",
    "    \n",
    "    euc_avg_th_acc[t] = sum(euc_threshold_acc[t]) / len(euc_threshold_acc[t])\n",
    "    cosine_avg_th_acc[t] = sum(cosine_threshold_acc[t]) / len(cosine_threshold_acc[t])\n",
    "    euc_avg_th_fscore[t] = sum(euc_threshold_fscore[t]) / len(euc_threshold_fscore[t])\n",
    "    cosine_avg_th_fscore[t] = sum(cosine_threshold_fscore[t]) / len(cosine_threshold_fscore[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "average euclidean accuracy\n",
      "0.6608\n",
      "average euclidean fscore\n",
      "0.739252127777\n",
      "average cosine accuracy\n",
      "0.6608\n",
      "average cosine fscore\n",
      "0.739252127777\n",
      "0.6\n",
      "average euclidean accuracy\n",
      "0.7754\n",
      "average euclidean fscore\n",
      "0.795440771064\n",
      "average cosine accuracy\n",
      "0.7568\n",
      "average cosine fscore\n",
      "0.783751735546\n",
      "0.7\n",
      "average euclidean accuracy\n",
      "0.7758\n",
      "average euclidean fscore\n",
      "0.763894191114\n",
      "average cosine accuracy\n",
      "0.7756\n",
      "average cosine fscore\n",
      "0.76679314827\n",
      "0.8\n",
      "average euclidean accuracy\n",
      "0.7442\n",
      "average euclidean fscore\n",
      "0.685718710895\n",
      "average cosine accuracy\n",
      "0.7442\n",
      "average cosine fscore\n",
      "0.685718710895\n",
      "0.9\n",
      "average euclidean accuracy\n",
      "0.6578\n",
      "average euclidean fscore\n",
      "0.491135349495\n",
      "average cosine accuracy\n",
      "0.6578\n",
      "average cosine fscore\n",
      "0.491135349495\n"
     ]
    }
   ],
   "source": [
    "# across maj-min classifiers\n",
    "for t in thresholds:\n",
    "    print t\n",
    "    print \"average euclidean accuracy\"\n",
    "    print euc_avg_th_acc[t]\n",
    "    \n",
    "    print \"average euclidean fscore\"\n",
    "    print euc_avg_th_fscore[t]\n",
    "    \n",
    "    print \"average cosine accuracy\"\n",
    "    print cosine_avg_th_acc[t]\n",
    "    \n",
    "    print \"average cosine fscore\"\n",
    "    print cosine_avg_th_fscore[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier groups analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minority Max Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for random dataset 1\n",
      "euclidean distance results for the minority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.584\n",
      "Cosine distance results for the minority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.584\n",
      "data set 1 was tested!\n",
      "results for random dataset 2\n",
      "euclidean distance results for the minority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.591\n",
      "Cosine distance results for the minority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.591\n",
      "data set 2 was tested!\n",
      "results for random dataset 3\n",
      "euclidean distance results for the minority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.579\n",
      "Cosine distance results for the minority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.579\n",
      "data set 3 was tested!\n",
      "results for random dataset 4\n",
      "euclidean distance results for the minority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.582\n",
      "Cosine distance results for the minority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.582\n",
      "data set 4 was tested!\n",
      "results for random dataset 5\n",
      "euclidean distance results for the minority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.592\n",
      "Cosine distance results for the minority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.592\n",
      "data set 5 was tested!\n",
      "max cosine acc\n",
      "[0.584, 0.591, 0.579, 0.582, 0.592]\n",
      "max euc acc\n",
      "[0.584, 0.591, 0.579, 0.582, 0.592]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "counter=0\n",
    "max_euc_accs=[]\n",
    "max_cosine_accs=[]\n",
    "eucthresholds_accs = defaultdict(list)\n",
    "eucthresholds_fscores = defaultdict(list)\n",
    "cosinethresholds_accs = defaultdict(list)\n",
    "cosinethresholds_fscores = defaultdict(list)\n",
    "\n",
    "euc_threshold_acc = defaultdict(list)\n",
    "cosine_threshold_acc = defaultdict(list)\n",
    "\n",
    "euc_threshold_fscore = defaultdict(list)\n",
    "cosine_threshold_fscore = defaultdict(list)\n",
    "for d in range(1,6):\n",
    "    i=d\n",
    "    counter+=1\n",
    "    if i==1:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions')\n",
    "    else:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions_'+str(i))\n",
    "        \n",
    "    print \"results for random dataset \"+str(d)\n",
    "    print \"euclidean distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    largestdistances=[234.77861912874434,233.25736858671797,231.57720095035262,231.27689032845456,229.1331490640322,228.10962276940444]\n",
    "    lowestdistances=[156.70035098875815,214.9116097375849,215.7683943491261,223.29576798497547,225.03333086456325,225.27094797154825]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    thresholds = [0.5,0.6,0.7,0.8,0.9]\n",
    "    euc_Fscores = []\n",
    "    euc_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "\n",
    "    print \"euclidean distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    euc_major_Fscores=[]\n",
    "    euc_major_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    print \"euclidean distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    #threshold using all models\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print \"max euc acc\"\n",
    "    print max(euc_Accs)\n",
    "    max_euc_accs.append(max(euc_Accs))\n",
    "    \n",
    "    largestdistances=[0.7749332064910006,0.7491597026757475,0.7236390000090482,0.7193428294891278,0.6904890029305283,0.6777351856726836]\n",
    "    lowestdistances=[0.23901334588341028,0.5475787563806471,0.5546099411237762,0.6240552059503661, 0.6424281501211625,0.6279728206924405]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    \n",
    "    print \"Cosine distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    cosine_Accs=[]\n",
    "    cosine_Fscores=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "    \n",
    "    print \"Cosine distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    \n",
    "    print \"Cosine distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    \n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        print \"max euc acc\"\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##cosine_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print max(cosine_Accs)\n",
    "    max_cosine_accs.append(max(cosine_Accs))\n",
    "    print \"data set \"+str(counter)+' was tested!'\n",
    "print \"max cosine acc\"\n",
    "print max_cosine_accs\n",
    "print \"max euc acc\"\n",
    "print max_euc_accs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Max Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for random dataset 1\n",
      "euclidean distance results for the minority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.777\n",
      "Cosine distance results for the minority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.777\n",
      "data set 1 was tested!\n",
      "results for random dataset 2\n",
      "euclidean distance results for the minority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.774\n",
      "Cosine distance results for the minority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.774\n",
      "data set 2 was tested!\n",
      "results for random dataset 3\n",
      "euclidean distance results for the minority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.779\n",
      "Cosine distance results for the minority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.779\n",
      "data set 3 was tested!\n",
      "results for random dataset 4\n",
      "euclidean distance results for the minority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.766\n",
      "Cosine distance results for the minority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.766\n",
      "data set 4 was tested!\n",
      "results for random dataset 5\n",
      "euclidean distance results for the minority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.766\n",
      "Cosine distance results for the minority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.766\n",
      "data set 5 was tested!\n",
      "max cosine acc\n",
      "[0.777, 0.774, 0.779, 0.766, 0.766]\n",
      "max euc acc\n",
      "[0.777, 0.774, 0.779, 0.766, 0.766]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "counter=0\n",
    "max_euc_accs=[]\n",
    "max_cosine_accs=[]\n",
    "eucthresholds_accs = defaultdict(list)\n",
    "eucthresholds_fscores = defaultdict(list)\n",
    "cosinethresholds_accs = defaultdict(list)\n",
    "cosinethresholds_fscores = defaultdict(list)\n",
    "\n",
    "euc_threshold_acc = defaultdict(list)\n",
    "cosine_threshold_acc = defaultdict(list)\n",
    "\n",
    "euc_threshold_fscore = defaultdict(list)\n",
    "cosine_threshold_fscore = defaultdict(list)\n",
    "for d in range(1,6):\n",
    "    i=d\n",
    "    counter+=1\n",
    "    if i==1:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions')\n",
    "    else:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions_'+str(i))\n",
    "        \n",
    "    print \"results for random dataset \"+str(d)\n",
    "    print \"euclidean distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    largestdistances=[234.77861912874434,233.25736858671797,231.57720095035262,231.27689032845456,229.1331490640322,228.10962276940444]\n",
    "    lowestdistances=[156.70035098875815,214.9116097375849,215.7683943491261,223.29576798497547,225.03333086456325,225.27094797154825]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    thresholds = [0.5,0.6,0.7,0.8,0.9]\n",
    "    euc_Fscores = []\n",
    "    euc_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "\n",
    "    print \"euclidean distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    euc_major_Fscores=[]\n",
    "    euc_major_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    print \"euclidean distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    #threshold using all models\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print \"max euc acc\"\n",
    "    print max(euc_Accs)\n",
    "    max_euc_accs.append(max(euc_Accs))\n",
    "    \n",
    "    largestdistances=[0.7749332064910006,0.7491597026757475,0.7236390000090482,0.7193428294891278,0.6904890029305283,0.6777351856726836]\n",
    "    lowestdistances=[0.23901334588341028,0.5475787563806471,0.5546099411237762,0.6240552059503661, 0.6424281501211625,0.6279728206924405]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    \n",
    "    print \"Cosine distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    cosine_Accs=[]\n",
    "    cosine_Fscores=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "    \n",
    "    print \"Cosine distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    \n",
    "    print \"Cosine distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    \n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        print \"max euc acc\"\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print max(cosine_Accs)\n",
    "    max_cosine_accs.append(max(cosine_Accs))\n",
    "    print \"data set \"+str(counter)+' was tested!'\n",
    "print \"max cosine acc\"\n",
    "print max_cosine_accs\n",
    "print \"max euc acc\"\n",
    "print max_euc_accs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for random dataset 1\n",
      "euclidean distance results for the minority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.791\n",
      "Cosine distance results for the minority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.791\n",
      "data set 1 was tested!\n",
      "results for random dataset 2\n",
      "euclidean distance results for the minority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.79\n",
      "Cosine distance results for the minority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.79\n",
      "data set 2 was tested!\n",
      "results for random dataset 3\n",
      "euclidean distance results for the minority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.795\n",
      "Cosine distance results for the minority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.795\n",
      "data set 3 was tested!\n",
      "results for random dataset 4\n",
      "euclidean distance results for the minority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.785\n",
      "Cosine distance results for the minority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.785\n",
      "data set 4 was tested!\n",
      "results for random dataset 5\n",
      "euclidean distance results for the minority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.798\n",
      "Cosine distance results for the minority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.798\n",
      "data set 5 was tested!\n",
      "max cosine acc\n",
      "[0.791, 0.79, 0.795, 0.785, 0.798]\n",
      "max euc acc\n",
      "[0.791, 0.79, 0.795, 0.785, 0.798]\n"
     ]
    }
   ],
   "source": [
    "#inverted weights\n",
    "from collections import defaultdict\n",
    "counter=0\n",
    "max_euc_accs=[]\n",
    "max_cosine_accs=[]\n",
    "eucthresholds_accs = defaultdict(list)\n",
    "eucthresholds_fscores = defaultdict(list)\n",
    "cosinethresholds_accs = defaultdict(list)\n",
    "cosinethresholds_fscores = defaultdict(list)\n",
    "\n",
    "euc_threshold_acc = defaultdict(list)\n",
    "cosine_threshold_acc = defaultdict(list)\n",
    "\n",
    "euc_threshold_fscore = defaultdict(list)\n",
    "cosine_threshold_fscore = defaultdict(list)\n",
    "for d in range(1,6):\n",
    "    i=d\n",
    "    counter+=1\n",
    "    if i==1:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions')\n",
    "    else:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions_'+str(i))\n",
    "        \n",
    "    print \"results for random dataset \"+str(d)\n",
    "    print \"euclidean distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    largestdistances=[234.77861912874434,233.25736858671797,231.57720095035262,231.27689032845456,229.1331490640322,228.10962276940444]\n",
    "    lowestdistances=[156.70035098875815,214.9116097375849,215.7683943491261,223.29576798497547,225.03333086456325,225.27094797154825]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "\n",
    "    normalized_largestdistances_inverted = []\n",
    "    normalized_smallestdistances_inverted = []\n",
    "    for n in normalized_largestdistances:\n",
    "        normalized_largestdistances_inverted.append(1-n)\n",
    "    \n",
    "    for n in normalized_smallestdistances:\n",
    "        normalized_smallestdistances_inverted.append(1-n)\n",
    "    thresholds = [0.5,0.6,0.7,0.8,0.9]\n",
    "    euc_Fscores = []\n",
    "    euc_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "\n",
    "    print \"euclidean distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    euc_major_Fscores=[]\n",
    "    euc_major_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances_inverted,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    print \"euclidean distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    #threshold using all models\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print \"max euc acc\"\n",
    "    print max(euc_Accs)\n",
    "    max_euc_accs.append(max(euc_Accs))\n",
    "    \n",
    "    largestdistances=[0.7749332064910006,0.7491597026757475,0.7236390000090482,0.7193428294891278,0.6904890029305283,0.6777351856726836]\n",
    "    lowestdistances=[0.23901334588341028,0.5475787563806471,0.5546099411237762,0.6240552059503661, 0.6424281501211625,0.6279728206924405]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "\n",
    "    normalized_largestdistances_inverted = []\n",
    "    normalized_smallestdistances_inverted = []\n",
    "    for n in normalized_largestdistances:\n",
    "        normalized_largestdistances_inverted.append(1-n)\n",
    "    \n",
    "    for n in normalized_smallestdistances:\n",
    "        normalized_smallestdistances_inverted.append(1-n)\n",
    "    print \"Cosine distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    cosine_Accs=[]\n",
    "    cosine_Fscores=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "    \n",
    "    print \"Cosine distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances_inverted,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    \n",
    "    print \"Cosine distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    \n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        print \"max euc acc\"\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print max(cosine_Accs)\n",
    "    max_cosine_accs.append(max(cosine_Accs))\n",
    "    print \"data set \"+str(counter)+' was tested!'\n",
    "print \"max cosine acc\"\n",
    "print max_cosine_accs\n",
    "print \"max euc acc\"\n",
    "print max_euc_accs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max cosine acc\n",
    "[0.777, 0.774, 0.779, 0.766, 0.766]\n",
    "max euc acc\n",
    "[0.777, 0.774, 0.779, 0.766, 0.766]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority-Minority Max Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for random dataset 1\n",
      "euclidean distance results for the minority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.781\n",
      "Cosine distance results for the minority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.78\n",
      "data set 1 was tested!\n",
      "results for random dataset 2\n",
      "euclidean distance results for the minority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.775\n",
      "Cosine distance results for the minority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.773\n",
      "data set 2 was tested!\n",
      "results for random dataset 3\n",
      "euclidean distance results for the minority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.782\n",
      "Cosine distance results for the minority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.782\n",
      "data set 3 was tested!\n",
      "results for random dataset 4\n",
      "euclidean distance results for the minority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.772\n",
      "Cosine distance results for the minority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.773\n",
      "data set 4 was tested!\n",
      "results for random dataset 5\n",
      "euclidean distance results for the minority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.785\n",
      "Cosine distance results for the minority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.77\n",
      "data set 5 was tested!\n",
      "max cosine acc\n",
      "[0.78, 0.773, 0.782, 0.773, 0.77]\n",
      "max euc acc\n",
      "[0.781, 0.775, 0.782, 0.772, 0.785]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "counter=0\n",
    "max_euc_accs=[]\n",
    "max_cosine_accs=[]\n",
    "eucthresholds_accs = defaultdict(list)\n",
    "eucthresholds_fscores = defaultdict(list)\n",
    "cosinethresholds_accs = defaultdict(list)\n",
    "cosinethresholds_fscores = defaultdict(list)\n",
    "\n",
    "euc_threshold_acc = defaultdict(list)\n",
    "cosine_threshold_acc = defaultdict(list)\n",
    "\n",
    "euc_threshold_fscore = defaultdict(list)\n",
    "cosine_threshold_fscore = defaultdict(list)\n",
    "for d in range(1,6):\n",
    "    i=d\n",
    "    counter+=1\n",
    "    if i==1:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions')\n",
    "    else:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions_'+str(i))\n",
    "        \n",
    "    print \"results for random dataset \"+str(d)\n",
    "    print \"euclidean distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    largestdistances=[234.77861912874434,233.25736858671797,231.57720095035262,231.27689032845456,229.1331490640322,228.10962276940444]\n",
    "    lowestdistances=[156.70035098875815,214.9116097375849,215.7683943491261,223.29576798497547,225.03333086456325,225.27094797154825]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    thresholds = [0.5,0.6,0.7,0.8,0.9]\n",
    "    euc_Fscores = []\n",
    "    euc_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "\n",
    "    print \"euclidean distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    euc_major_Fscores=[]\n",
    "    euc_major_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    print \"euclidean distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    #threshold using all models\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print \"max euc acc\"\n",
    "    print max(euc_Accs)\n",
    "    max_euc_accs.append(max(euc_Accs))\n",
    "    \n",
    "    largestdistances=[0.7749332064910006,0.7491597026757475,0.7236390000090482,0.7193428294891278,0.6904890029305283,0.6777351856726836]\n",
    "    lowestdistances=[0.23901334588341028,0.5475787563806471,0.5546099411237762,0.6240552059503661, 0.6424281501211625,0.6279728206924405]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    \n",
    "    print \"Cosine distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    cosine_Accs=[]\n",
    "    cosine_Fscores=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "    \n",
    "    print \"Cosine distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    \n",
    "    print \"Cosine distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    \n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        print \"max euc acc\"\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print max(cosine_Accs)\n",
    "    max_cosine_accs.append(max(cosine_Accs))\n",
    "    print \"data set \"+str(counter)+' was tested!'\n",
    "print \"max cosine acc\"\n",
    "print max_cosine_accs\n",
    "print \"max euc acc\"\n",
    "print max_euc_accs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for random dataset 1\n",
      "euclidean distance results for the minority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.781\n",
      "Cosine distance results for the minority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.795\n",
      "data set 1 was tested!\n",
      "results for random dataset 2\n",
      "euclidean distance results for the minority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.775\n",
      "Cosine distance results for the minority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.775\n",
      "data set 2 was tested!\n",
      "results for random dataset 3\n",
      "euclidean distance results for the minority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.782\n",
      "Cosine distance results for the minority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.782\n",
      "data set 3 was tested!\n",
      "results for random dataset 4\n",
      "euclidean distance results for the minority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.774\n",
      "Cosine distance results for the minority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.786\n",
      "data set 4 was tested!\n",
      "results for random dataset 5\n",
      "euclidean distance results for the minority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.785\n",
      "Cosine distance results for the minority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.792\n",
      "data set 5 was tested!\n",
      "max cosine acc\n",
      "[0.795, 0.775, 0.782, 0.786, 0.792]\n",
      "max euc acc\n",
      "[0.781, 0.775, 0.782, 0.774, 0.785]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Inverted weights\n",
    "\n",
    "from collections import defaultdict\n",
    "counter=0\n",
    "max_euc_accs=[]\n",
    "max_cosine_accs=[]\n",
    "eucthresholds_accs = defaultdict(list)\n",
    "eucthresholds_fscores = defaultdict(list)\n",
    "cosinethresholds_accs = defaultdict(list)\n",
    "cosinethresholds_fscores = defaultdict(list)\n",
    "\n",
    "euc_threshold_acc = defaultdict(list)\n",
    "cosine_threshold_acc = defaultdict(list)\n",
    "\n",
    "euc_threshold_fscore = defaultdict(list)\n",
    "cosine_threshold_fscore = defaultdict(list)\n",
    "for d in range(1,6):\n",
    "    i=d\n",
    "    counter+=1\n",
    "    if i==1:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions')\n",
    "    else:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions_'+str(i))\n",
    "        \n",
    "    print \"results for random dataset \"+str(d)\n",
    "    print \"euclidean distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    largestdistances=[234.77861912874434,233.25736858671797,231.57720095035262,231.27689032845456,229.1331490640322,228.10962276940444]\n",
    "    lowestdistances=[156.70035098875815,214.9116097375849,215.7683943491261,223.29576798497547,225.03333086456325,225.27094797154825]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    normalized_largestdistances_inverted = []\n",
    "    normalized_smallestdistances_inverted = []\n",
    "    for n in normalized_largestdistances:\n",
    "        normalized_largestdistances_inverted.append(1-n)\n",
    "    \n",
    "    for n in normalized_smallestdistances:\n",
    "        normalized_smallestdistances_inverted.append(1-n)\n",
    "    thresholds = [0.5,0.6,0.7,0.8,0.9]\n",
    "    euc_Fscores = []\n",
    "    euc_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "\n",
    "    print \"euclidean distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    euc_major_Fscores=[]\n",
    "    euc_major_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    print \"euclidean distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    #threshold using all models\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances_inverted,normalized_largestdistances_inverted,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print \"max euc acc\"\n",
    "    print max(euc_Accs)\n",
    "    max_euc_accs.append(max(euc_Accs))\n",
    "    \n",
    "    largestdistances=[0.7749332064910006,0.7491597026757475,0.7236390000090482,0.7193428294891278,0.6904890029305283,0.6777351856726836]\n",
    "    lowestdistances=[0.23901334588341028,0.5475787563806471,0.5546099411237762,0.6240552059503661, 0.6424281501211625,0.6279728206924405]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    normalized_largestdistances_inverted = []\n",
    "    normalized_smallestdistances_inverted = []\n",
    "    for n in normalized_largestdistances:\n",
    "        normalized_largestdistances_inverted.append(1-n)\n",
    "    \n",
    "    for n in normalized_smallestdistances:\n",
    "        normalized_smallestdistances_inverted.append(1-n)\n",
    "    print \"Cosine distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    cosine_Accs=[]\n",
    "    cosine_Fscores=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "    \n",
    "    print \"Cosine distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    \n",
    "    print \"Cosine distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    \n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances_inverted,normalized_largestdistances_inverted,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        print \"max euc acc\"\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print max(cosine_Accs)\n",
    "    max_cosine_accs.append(max(cosine_Accs))\n",
    "    print \"data set \"+str(counter)+' was tested!'\n",
    "print \"max cosine acc\"\n",
    "print max_cosine_accs\n",
    "print \"max euc acc\"\n",
    "print max_euc_accs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max cosine acc\n",
    "[0.78, 0.773, 0.782, 0.773, 0.77]\n",
    "max euc acc\n",
    "[0.781, 0.775, 0.782, 0.772, 0.785]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least performing classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for random dataset 1\n",
      "euclidean distance results for the minority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.584\n",
      "Cosine distance results for the minority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.584\n",
      "data set 1 was tested!\n",
      "results for random dataset 2\n",
      "euclidean distance results for the minority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.591\n",
      "Cosine distance results for the minority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.591\n",
      "data set 2 was tested!\n",
      "results for random dataset 3\n",
      "euclidean distance results for the minority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.579\n",
      "Cosine distance results for the minority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.579\n",
      "data set 3 was tested!\n",
      "results for random dataset 4\n",
      "euclidean distance results for the minority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.582\n",
      "Cosine distance results for the minority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.582\n",
      "data set 4 was tested!\n",
      "results for random dataset 5\n",
      "euclidean distance results for the minority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.592\n",
      "Cosine distance results for the minority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.592\n",
      "data set 5 was tested!\n",
      "max cosine acc\n",
      "[0.584, 0.591, 0.579, 0.582, 0.592]\n",
      "max euc acc\n",
      "[0.584, 0.591, 0.579, 0.582, 0.592]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "counter=0\n",
    "max_euc_accs=[]\n",
    "max_cosine_accs=[]\n",
    "eucthresholds_accs = defaultdict(list)\n",
    "eucthresholds_fscores = defaultdict(list)\n",
    "cosinethresholds_accs = defaultdict(list)\n",
    "cosinethresholds_fscores = defaultdict(list)\n",
    "\n",
    "euc_threshold_acc = defaultdict(list)\n",
    "cosine_threshold_acc = defaultdict(list)\n",
    "\n",
    "euc_threshold_fscore = defaultdict(list)\n",
    "cosine_threshold_fscore = defaultdict(list)\n",
    "for d in range(1,6):\n",
    "    i=d\n",
    "    counter+=1\n",
    "    if i==1:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions')\n",
    "    else:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions_'+str(i))\n",
    "        \n",
    "    print \"results for random dataset \"+str(d)\n",
    "    print \"euclidean distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    largestdistances=[234.77861912874434,233.25736858671797,231.57720095035262,231.27689032845456,229.1331490640322,228.10962276940444]\n",
    "    lowestdistances=[156.70035098875815,214.9116097375849,215.7683943491261,223.29576798497547,225.03333086456325,225.27094797154825]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    thresholds = [0.5,0.6,0.7,0.8,0.9]\n",
    "    euc_Fscores = []\n",
    "    euc_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "\n",
    "    print \"euclidean distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    euc_major_Fscores=[]\n",
    "    euc_major_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    print \"euclidean distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    #threshold using all models\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print \"max euc acc\"\n",
    "    print max(euc_Accs)\n",
    "    max_euc_accs.append(max(euc_Accs))\n",
    "    \n",
    "    largestdistances=[0.7749332064910006,0.7491597026757475,0.7236390000090482,0.7193428294891278,0.6904890029305283,0.6777351856726836]\n",
    "    lowestdistances=[0.23901334588341028,0.5475787563806471,0.5546099411237762,0.6240552059503661, 0.6424281501211625,0.6279728206924405]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    \n",
    "    print \"Cosine distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    cosine_Accs=[]\n",
    "    cosine_Fscores=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "    \n",
    "    print \"Cosine distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    \n",
    "    print \"Cosine distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    \n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        print \"max euc acc\"\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print max(cosine_Accs)\n",
    "    max_cosine_accs.append(max(cosine_Accs))\n",
    "    print \"data set \"+str(counter)+' was tested!'\n",
    "print \"max cosine acc\"\n",
    "print max_cosine_accs\n",
    "print \"max euc acc\"\n",
    "print max_euc_accs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for random dataset 1\n",
      "euclidean distance results for the minority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.584\n",
      "Cosine distance results for the minority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.584\n",
      "data set 1 was tested!\n",
      "results for random dataset 2\n",
      "euclidean distance results for the minority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.591\n",
      "Cosine distance results for the minority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.591\n",
      "data set 2 was tested!\n",
      "results for random dataset 3\n",
      "euclidean distance results for the minority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.579\n",
      "Cosine distance results for the minority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.579\n",
      "data set 3 was tested!\n",
      "results for random dataset 4\n",
      "euclidean distance results for the minority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.582\n",
      "Cosine distance results for the minority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.582\n",
      "data set 4 was tested!\n",
      "results for random dataset 5\n",
      "euclidean distance results for the minority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.592\n",
      "Cosine distance results for the minority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "max euc acc\n",
      "0.592\n",
      "data set 5 was tested!\n",
      "max cosine acc\n",
      "[0.584, 0.591, 0.579, 0.582, 0.592]\n",
      "max euc acc\n",
      "[0.584, 0.591, 0.579, 0.582, 0.592]\n"
     ]
    }
   ],
   "source": [
    "## inverted weights\n",
    "from collections import defaultdict\n",
    "counter=0\n",
    "max_euc_accs=[]\n",
    "max_cosine_accs=[]\n",
    "eucthresholds_accs = defaultdict(list)\n",
    "eucthresholds_fscores = defaultdict(list)\n",
    "cosinethresholds_accs = defaultdict(list)\n",
    "cosinethresholds_fscores = defaultdict(list)\n",
    "\n",
    "euc_threshold_acc = defaultdict(list)\n",
    "cosine_threshold_acc = defaultdict(list)\n",
    "\n",
    "euc_threshold_fscore = defaultdict(list)\n",
    "cosine_threshold_fscore = defaultdict(list)\n",
    "for d in range(1,6):\n",
    "    i=d\n",
    "    counter+=1\n",
    "    if i==1:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions')\n",
    "    else:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions_'+str(i))\n",
    "        \n",
    "    print \"results for random dataset \"+str(d)\n",
    "    print \"euclidean distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    largestdistances=[234.77861912874434,233.25736858671797,231.57720095035262,231.27689032845456,229.1331490640322,228.10962276940444]\n",
    "    lowestdistances=[156.70035098875815,214.9116097375849,215.7683943491261,223.29576798497547,225.03333086456325,225.27094797154825]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    normalized_largestdistances_inverted = [0] * len(normalized_largestdistances)\n",
    "    normalized_smallestdistances_inverted = [0] * len(normalized_smallestdistances)\n",
    "    for i in range(len(normalized_largestdistances)):\n",
    "        normalized_largestdistances_inverted[i] = 1 - normalized_largestdistances[i]\n",
    "    \n",
    "    for j in range(len(normalized_smallestdistances)):\n",
    "        normalized_smallestdistances_inverted[j] = 1 - normalized_smallestdistances[j]\n",
    "    thresholds = [0.5,0.6,0.7,0.8,0.9]\n",
    "    euc_Fscores = []\n",
    "    euc_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances_inverted,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "\n",
    "    print \"euclidean distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    euc_major_Fscores=[]\n",
    "    euc_major_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    print \"euclidean distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    #threshold using all models\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print \"max euc acc\"\n",
    "    print max(euc_Accs)\n",
    "    max_euc_accs.append(max(euc_Accs))\n",
    "    \n",
    "    largestdistances=[0.7749332064910006,0.7491597026757475,0.7236390000090482,0.7193428294891278,0.6904890029305283,0.6777351856726836]\n",
    "    lowestdistances=[0.23901334588341028,0.5475787563806471,0.5546099411237762,0.6240552059503661, 0.6424281501211625,0.6279728206924405]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    normalized_largestdistances_inverted = [0] * len(normalized_largestdistances)\n",
    "    normalized_smallestdistances_inverted = [0] * len(normalized_smallestdistances)\n",
    "    for i in range(len(normalized_largestdistances)):\n",
    "        normalized_largestdistances_inverted[i] = 1 - normalized_largestdistances[i]\n",
    "    \n",
    "    for j in range(len(normalized_smallestdistances)):\n",
    "        normalized_smallestdistances_inverted[j] = 1 - normalized_smallestdistances[j]\n",
    "    print \"Cosine distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    cosine_Accs=[]\n",
    "    cosine_Fscores=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances_inverted,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "    \n",
    "    print \"Cosine distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    \n",
    "    print \"Cosine distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    \n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        print \"max euc acc\"\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print max(cosine_Accs)\n",
    "    max_cosine_accs.append(max(cosine_Accs))\n",
    "    print \"data set \"+str(counter)+' was tested!'\n",
    "print \"max cosine acc\"\n",
    "print max_cosine_accs\n",
    "print \"max euc acc\"\n",
    "print max_euc_accs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max cosine acc\n",
    "[0.584, 0.591, 0.579, 0.582, 0.592]\n",
    "max euc acc\n",
    "[0.584, 0.591, 0.579, 0.582, 0.592]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">review</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">label</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">word_count</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">3-grams</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">As a casual piano player<br>and a Broadway fanati ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 3L,<br>'\"manilowesque\"': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'shuffle\" \"prissy<br>sonata\"': 1L, 'as a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">This book was someone<br>else's bookclub ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'all': 1L, 'gold': 1L,<br>'bookclub': 1L, 'over': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'the prose needed': 1L,<br>'others miss yes': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">I have been studing<br>leadership for over 15 ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'consider': 1L, 'over':<br>1L, 'years': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'would have been': 1L,<br>'the concepts in': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The best book on the<br>history of World War II ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'all': 1L, 'pointing':<br>1L, 'excellent': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'meanings in addition':<br>1L, 'an excellent job': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Zadie Smith is one of the<br>most powerful authors I ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 2L, 'love': 1L,<br>'times.': 1L, \"don't\": ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'out in emotional': 1L,<br>'you enjoyed the': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">I've been very critical<br>of the fiction work of ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'all': 1L, 'anti-death':<br>1L, \"grisham's\": 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'hammer that opinion':<br>1L, 'like \"the firm\"': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Zig Ziglar has done it<br>again. As an author ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{\"gab,'\": 1L, 'and': 2L,<br>'right': 2L, 'just': 2L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'most honorable<br>profession': 1L, 'asking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">A quick search in Amazon<br>books under the name ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'government.': 1L,<br>'ever.': 1L, 'being': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'characters brands<br>brings': 1L, 'american ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Beautiful, stunning<br>absolutely gorgeous.A ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 2L, 'ordered':<br>2L, 'absolutely': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'is much cheaper': 1L,<br>'much cheaper here': 1L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">I loved this book. The<br>food was really good and ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 2L, 'because':<br>2L, 'set': 1L, 'have': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'a budget the': 1L, 'if<br>you re': 1L, 't eat it': ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "<table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">2-grams</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">1-grams</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">tfidf</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">kitchen_predictions</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">dvd_predictions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'course the': 1L, 'was<br>so': 1L, 'lewis ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 3L, 'lewis': 1L,<br>'\"manilowesque\"': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and':<br>0.3903260560410611, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'book and': 1L, 'the<br>flaws': 1L, 'this book': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'all': 1L, 'selection':<br>1L, 'gold': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'all':<br>1.171182981502945, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'this book': 1L,<br>'activity it': 1L, 'at ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'sanctuary': 1L,<br>'consider': 1L, 'over': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'consider':<br>3.8632328412587142, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'in addition': 1L, 'by<br>the': 1L, 'yanks used': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'all': 1L, 'pointing':<br>1L, 'move': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'all':<br>1.171182981502945, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'if you': 1L, 'the<br>most': 1L, '\"crash\" y ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 2L, 'love': 1L,<br>'enjoyed': 1L, 'momen ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and':<br>0.2602173706940407, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'needed to': 1L, 'this<br>book': 1L, 'his novels': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'all': 1L, 'ron': 4L,<br>'writers': 1L, 'years': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'all':<br>1.171182981502945, 'a ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'about so': 1L, 'not<br>just': 1L, 'the most': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 2L, 'right': 2L,<br>'just': 2L, 'being': 2L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{\"gab,'\":<br>6.907755278982137, 'a ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'many people': 1L,<br>'point the': 1L, 'mat ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'all': 1L, 'being': 1L,<br>'newton': 1L, 'years': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'government.':<br>6.214608098422191, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'november 1st': 1L,<br>'beautiful stunning': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'beautiful': 1L, 'and':<br>2L, 'co': 1L, 've': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and':<br>0.2602173706940407, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'spicy to': 1L,<br>'different things': 1L, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and': 2L, 'because':<br>2L, 'set': 1L, 'just': ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">{'and':<br>0.2602173706940407, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "</table>\n",
       "<table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">electronics_predictions</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">apparel_predictions</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">automotive_predictions</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">baby_predictions</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">beauty_predictions</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">cameraphoto_predictions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "</table>\n",
       "<table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">computervideo_predictions</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">gourmet_predictions</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">grocery_predictions</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">healthpersonal_prediction<br>s ...</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">jewelrywatches_prediction<br>s ...</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[1000 rows x 20 columns]<br/>Note: Only the head of the SFrame is printed.<br/>You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\treview\tstr\n",
       "\tlabel\tint\n",
       "\tword_count\tdict\n",
       "\t3-grams\tdict\n",
       "\t2-grams\tdict\n",
       "\t1-grams\tdict\n",
       "\ttfidf\tdict\n",
       "\tkitchen_predictions\tint\n",
       "\tdvd_predictions\tint\n",
       "\telectronics_predictions\tint\n",
       "\tapparel_predictions\tint\n",
       "\tautomotive_predictions\tint\n",
       "\tbaby_predictions\tint\n",
       "\tbeauty_predictions\tint\n",
       "\tcameraphoto_predictions\tint\n",
       "\tcomputervideo_predictions\tint\n",
       "\tgourmet_predictions\tint\n",
       "\tgrocery_predictions\tint\n",
       "\thealthpersonal_predictions\tint\n",
       "\tjewelrywatches_predictions\tint\n",
       "\n",
       "Rows: 1000\n",
       "\n",
       "Data:\n",
       "+-------------------------------+-------+-------------------------------+\n",
       "|             review            | label |           word_count          |\n",
       "+-------------------------------+-------+-------------------------------+\n",
       "| As a casual piano player a... |   1   | {'and': 3L, '\"manilowesque... |\n",
       "| This book was someone else... |   1   | {'all': 1L, 'gold': 1L, 'b... |\n",
       "| I have been studing leader... |   1   | {'consider': 1L, 'over': 1... |\n",
       "| The best book on the histo... |   1   | {'all': 1L, 'pointing': 1L... |\n",
       "| Zadie Smith is one of the ... |   1   | {'and': 2L, 'love': 1L, 't... |\n",
       "| I've been very critical of... |   1   | {'all': 1L, 'anti-death': ... |\n",
       "| Zig Ziglar has done it aga... |   1   | {\"gab,'\": 1L, 'and': 2L, '... |\n",
       "| A quick search in Amazon b... |   1   | {'government.': 1L, 'ever.... |\n",
       "| Beautiful, stunning absolu... |   1   | {'and': 2L, 'ordered': 2L,... |\n",
       "| I loved this book. The foo... |   1   | {'and': 2L, 'because': 2L,... |\n",
       "+-------------------------------+-------+-------------------------------+\n",
       "+-------------------------------+-------------------------------+\n",
       "|            3-grams            |            2-grams            |\n",
       "+-------------------------------+-------------------------------+\n",
       "| {'shuffle\" \"prissy sonata\"... | {'course the': 1L, 'was so... |\n",
       "| {'the prose needed': 1L, '... | {'book and': 1L, 'the flaw... |\n",
       "| {'would have been': 1L, 't... | {'this book': 1L, 'activit... |\n",
       "| {'meanings in addition': 1... | {'in addition': 1L, 'by th... |\n",
       "| {'out in emotional': 1L, '... | {'if you': 1L, 'the most':... |\n",
       "| {'hammer that opinion': 1L... | {'needed to': 1L, 'this bo... |\n",
       "| {'most honorable professio... | {'about so': 1L, 'not just... |\n",
       "| {'characters brands brings... | {'many people': 1L, 'point... |\n",
       "| {'is much cheaper': 1L, 'm... | {'november 1st': 1L, 'beau... |\n",
       "| {'a budget the': 1L, 'if y... | {'spicy to': 1L, 'differen... |\n",
       "+-------------------------------+-------------------------------+\n",
       "+-------------------------------+-------------------------------+---------------------+\n",
       "|            1-grams            |             tfidf             | kitchen_predictions |\n",
       "+-------------------------------+-------------------------------+---------------------+\n",
       "| {'and': 3L, 'lewis': 1L, '... | {'and': 0.3903260560410611... |          1          |\n",
       "| {'all': 1L, 'selection': 1... | {'all': 1.171182981502945,... |          0          |\n",
       "| {'sanctuary': 1L, 'conside... | {'consider': 3.86323284125... |          0          |\n",
       "| {'all': 1L, 'pointing': 1L... | {'all': 1.171182981502945,... |          0          |\n",
       "| {'and': 2L, 'love': 1L, 'e... | {'and': 0.2602173706940407... |          1          |\n",
       "| {'all': 1L, 'ron': 4L, 'wr... | {'all': 1.171182981502945,... |          1          |\n",
       "| {'and': 2L, 'right': 2L, '... | {\"gab,'\": 6.90775527898213... |          1          |\n",
       "| {'all': 1L, 'being': 1L, '... | {'government.': 6.21460809... |          1          |\n",
       "| {'beautiful': 1L, 'and': 2... | {'and': 0.2602173706940407... |          1          |\n",
       "| {'and': 2L, 'because': 2L,... | {'and': 0.2602173706940407... |          1          |\n",
       "+-------------------------------+-------------------------------+---------------------+\n",
       "+-----------------+-------------------------+---------------------+------------------------+\n",
       "| dvd_predictions | electronics_predictions | apparel_predictions | automotive_predictions |\n",
       "+-----------------+-------------------------+---------------------+------------------------+\n",
       "|        1        |            1            |          1          |           0            |\n",
       "|        1        |            1            |          1          |           0            |\n",
       "|        1        |            0            |          1          |           0            |\n",
       "|        1        |            1            |          0          |           0            |\n",
       "|        1        |            1            |          1          |           0            |\n",
       "|        1        |            1            |          0          |           0            |\n",
       "|        0        |            1            |          1          |           0            |\n",
       "|        1        |            0            |          1          |           0            |\n",
       "|        0        |            1            |          1          |           0            |\n",
       "|        0        |            0            |          1          |           0            |\n",
       "+-----------------+-------------------------+---------------------+------------------------+\n",
       "+------------------+--------------------+-------------------------+---------------------------+\n",
       "| baby_predictions | beauty_predictions | cameraphoto_predictions | computervideo_predictions |\n",
       "+------------------+--------------------+-------------------------+---------------------------+\n",
       "|        1         |         1          |            1            |             1             |\n",
       "|        1         |         1          |            1            |             1             |\n",
       "|        1         |         1          |            1            |             1             |\n",
       "|        1         |         1          |            1            |             1             |\n",
       "|        1         |         1          |            1            |             1             |\n",
       "|        1         |         1          |            1            |             1             |\n",
       "|        1         |         1          |            0            |             1             |\n",
       "|        1         |         1          |            1            |             1             |\n",
       "|        1         |         1          |            1            |             1             |\n",
       "|        1         |         1          |            0            |             1             |\n",
       "+------------------+--------------------+-------------------------+---------------------------+\n",
       "+---------------------+---------------------+----------------------------+----------------------------+\n",
       "| gourmet_predictions | grocery_predictions | healthpersonal_predictions | jewelrywatches_predictions |\n",
       "+---------------------+---------------------+----------------------------+----------------------------+\n",
       "|          1          |          1          |             1              |             1              |\n",
       "|          1          |          1          |             0              |             1              |\n",
       "|          1          |          1          |             0              |             1              |\n",
       "|          1          |          1          |             0              |             1              |\n",
       "|          1          |          1          |             1              |             1              |\n",
       "|          1          |          1          |             1              |             1              |\n",
       "|          1          |          1          |             0              |             1              |\n",
       "|          1          |          1          |             1              |             1              |\n",
       "|          1          |          1          |             1              |             1              |\n",
       "|          1          |          1          |             1              |             1              |\n",
       "+---------------------+---------------------+----------------------------+----------------------------+\n",
       "[1000 rows x 20 columns]\n",
       "Note: Only the head of the SFrame is printed.\n",
       "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bookstestgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "Fscoress = defaultdict(list)\n",
    "Accuraciess = defaultdict(list)\n",
    "for d in range(1,6):\n",
    "    i=d\n",
    "    counter+=1\n",
    "    if i==1:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions')\n",
    "    else:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions_'+str(i))\n",
    "    Fscores={}\n",
    "    Accuracies={}\n",
    "    book_dist_domains = ['electronics','dvd','kitchen','apparel','automotive','baby','beauty','cameraphoto','computervideo','gourmet','grocery','healthpersonal','jewelrywatches']\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    for domain in Minor_domains:\n",
    "        Fscoress[domain].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl[str(domain)]))\n",
    "        Accuraciess[domain].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl[str(domain)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average fscores for the top classifiers\n",
      "automotive_predictions\n",
      "0.0\n",
      "jewelrywatches_predictions\n",
      "0.666666666667\n",
      "gourmet_predictions\n",
      "0.666666666667\n",
      "grocery_predictions\n",
      "0.667022459417\n",
      "beauty_predictions\n",
      "0.667200474549\n",
      "baby_predictions\n",
      "0.697877391697\n"
     ]
    }
   ],
   "source": [
    "print \"average fscores for the least performing classifiers\"\n",
    "for d in Minor_domains:\n",
    "    print d\n",
    "    print sum(Fscoress[d]) / len(Fscoress[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average accuracies for the top classifiers\n",
      "automotive_predictions\n",
      "0.5\n",
      "jewelrywatches_predictions\n",
      "0.5\n",
      "gourmet_predictions\n",
      "0.5\n",
      "grocery_predictions\n",
      "0.5008\n",
      "beauty_predictions\n",
      "0.5012\n",
      "baby_predictions\n",
      "0.5856\n"
     ]
    }
   ],
   "source": [
    "print \"average accuracies for the least performing classifiers\"\n",
    "\n",
    "for d in Minor_domains:\n",
    "    print d\n",
    "    print sum(Accuraciess[d]) / len(Accuraciess[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "Fscoress = defaultdict(list)\n",
    "Accuraciess = defaultdict(list)\n",
    "for d in range(1,6):\n",
    "    i=d\n",
    "    counter+=1\n",
    "    if i==1:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions')\n",
    "    else:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions_'+str(i))\n",
    "    Fscores={}\n",
    "    Accuracies={}\n",
    "    book_dist_domains = ['electronics','dvd','kitchen','apparel','automotive','baby','beauty','cameraphoto','computervideo','gourmet','grocery','healthpersonal','jewelrywatches']\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    for domain in Major_domains:\n",
    "        Fscoress[domain].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl[str(domain)]))\n",
    "        Accuraciess[domain].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl[str(domain)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average fscores for the top classifiers\n",
      "dvd_predictions\n",
      "0.802892718853\n",
      "electronics_predictions\n",
      "0.663105111542\n",
      "kitchen_predictions\n",
      "0.708548422913\n",
      "healthpersonal_predictions\n",
      "0.739633916322\n",
      "cameraphoto_predictions\n",
      "0.698605809543\n",
      "apparel_predictions\n",
      "0.724353928676\n"
     ]
    }
   ],
   "source": [
    "print \"average fscores for the top classifiers\"\n",
    "for d in Major_domains:\n",
    "    print d\n",
    "    print sum(Fscoress[d]) / len(Fscoress[d])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average accuracies for the top classifiers\n",
      "dvd_predictions\n",
      "0.7928\n",
      "electronics_predictions\n",
      "0.6832\n",
      "kitchen_predictions\n",
      "0.7032\n",
      "healthpersonal_predictions\n",
      "0.7208\n",
      "cameraphoto_predictions\n",
      "0.688\n",
      "apparel_predictions\n",
      "0.7172\n"
     ]
    }
   ],
   "source": [
    "print \"average accuracies for the top classifiers\"\n",
    "\n",
    "for d in Major_domains:\n",
    "    print d\n",
    "    print sum(Accuraciess[d]) / len(Accuraciess[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Least Performing (th=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v2.1 started. Logging: C:\\Users\\Omar\\AppData\\Local\\Temp\\graphlab_server_1538696793.log.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create for academic use is assigned to oaabde01@louisville.edu and will expire on September 24, 2019.\n",
      "results for random dataset 1\n",
      "euclidean distance results for the minority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.502\n",
      "Cosine distance results for the minority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.502\n",
      "data set 1 was tested!\n",
      "results for random dataset 2\n",
      "euclidean distance results for the minority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.501\n",
      "Cosine distance results for the minority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.501\n",
      "data set 2 was tested!\n",
      "results for random dataset 3\n",
      "euclidean distance results for the minority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.501\n",
      "Cosine distance results for the minority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.501\n",
      "data set 3 was tested!\n",
      "results for random dataset 4\n",
      "euclidean distance results for the minority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.501\n",
      "Cosine distance results for the minority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.501\n",
      "data set 4 was tested!\n",
      "results for random dataset 5\n",
      "euclidean distance results for the minority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.501\n",
      "Cosine distance results for the minority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.501\n",
      "data set 5 was tested!\n",
      "max cosine acc\n",
      "[0.502, 0.501, 0.501, 0.501, 0.501]\n",
      "average cosine accuracy for threshold 0.5\n",
      "0.5012\n",
      "average cosine fscore for threshold 0.5\n",
      "0.667200474549\n",
      "max euc acc\n",
      "[0.502, 0.501, 0.501, 0.501, 0.501]\n",
      "average euclidean accuracy for threshold 0.5\n",
      "0.5012\n",
      "average euclidean fscore for threshold 0.5\n",
      "0.667200474549\n"
     ]
    }
   ],
   "source": [
    "## inverted weights\n",
    "from collections import defaultdict\n",
    "counter=0\n",
    "max_euc_accs=[]\n",
    "max_cosine_accs=[]\n",
    "max_cosine_fscores=[]\n",
    "max_euc_fscores=[]\n",
    "eucthresholds_accs = defaultdict(list)\n",
    "eucthresholds_fscores = defaultdict(list)\n",
    "cosinethresholds_accs = defaultdict(list)\n",
    "cosinethresholds_fscores = defaultdict(list)\n",
    "\n",
    "euc_threshold_acc = defaultdict(list)\n",
    "cosine_threshold_acc = defaultdict(list)\n",
    "\n",
    "euc_threshold_fscore = defaultdict(list)\n",
    "cosine_threshold_fscore = defaultdict(list)\n",
    "for d in range(1,6):\n",
    "    i=d\n",
    "    counter+=1\n",
    "    if i==1:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions')\n",
    "    else:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions_'+str(i))\n",
    "        \n",
    "    print \"results for random dataset \"+str(d)\n",
    "    print \"euclidean distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    largestdistances=[234.77861912874434,233.25736858671797,231.57720095035262,231.27689032845456,229.1331490640322,228.10962276940444]\n",
    "    lowestdistances=[156.70035098875815,214.9116097375849,215.7683943491261,223.29576798497547,225.03333086456325,225.27094797154825]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    normalized_largestdistances_inverted = [0] * len(normalized_largestdistances)\n",
    "    normalized_smallestdistances_inverted = [0] * len(normalized_smallestdistances)\n",
    "    for i in range(len(normalized_largestdistances)):\n",
    "        normalized_largestdistances_inverted[i] = 1 - normalized_largestdistances[i]\n",
    "    \n",
    "    for j in range(len(normalized_smallestdistances)):\n",
    "        normalized_smallestdistances_inverted[j] = 1 - normalized_smallestdistances[j]\n",
    "    thresholds = [0.5]\n",
    "    euc_Fscores = []\n",
    "    euc_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances_inverted,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "\n",
    "    print \"euclidean distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    euc_major_Fscores=[]\n",
    "    euc_major_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    print \"euclidean distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    #threshold using all models\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print \"max euc acc\"\n",
    "    print max(euc_Accs)\n",
    "    max_euc_accs.append(max(euc_Accs))\n",
    "    max_euc_fscores.append(max(euc_Fscores))\n",
    "    largestdistances=[0.7749332064910006,0.7491597026757475,0.7236390000090482,0.7193428294891278,0.6904890029305283,0.6777351856726836]\n",
    "    lowestdistances=[0.23901334588341028,0.5475787563806471,0.5546099411237762,0.6240552059503661, 0.6424281501211625,0.6279728206924405]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    normalized_largestdistances_inverted = [0] * len(normalized_largestdistances)\n",
    "    normalized_smallestdistances_inverted = [0] * len(normalized_smallestdistances)\n",
    "    for i in range(len(normalized_largestdistances)):\n",
    "        normalized_largestdistances_inverted[i] = 1 - normalized_largestdistances[i]\n",
    "    \n",
    "    for j in range(len(normalized_smallestdistances)):\n",
    "        normalized_smallestdistances_inverted[j] = 1 - normalized_smallestdistances[j]\n",
    "    print \"Cosine distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    cosine_Accs=[]\n",
    "    cosine_Fscores=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances_inverted,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "    \n",
    "    print \"Cosine distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    \n",
    "    print \"Cosine distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    \n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        print \"max euc acc\"\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print max(cosine_Accs)\n",
    "    max_cosine_accs.append(max(cosine_Accs))\n",
    "    max_cosine_fscores.append(max(cosine_Fscores))\n",
    "    print \"data set \"+str(counter)+' was tested!'\n",
    "print \"max cosine acc\"\n",
    "print max_cosine_accs\n",
    "max_cosine_accs_mean = sum(max_cosine_accs) / len(max_cosine_accs)\n",
    "print \"average cosine accuracy for threshold \" + str(thresholds[0])\n",
    "print max_cosine_accs_mean\n",
    "max_cosine_fscores_mean = sum(max_cosine_fscores) / len(max_cosine_fscores)\n",
    "print \"average cosine fscore for threshold \" + str(thresholds[0])\n",
    "print max_cosine_fscores_mean\n",
    "\n",
    "print \"max euc acc\"\n",
    "print max_euc_accs\n",
    "max_euc_accs_mean = sum(max_euc_accs) / len(max_euc_accs)\n",
    "\n",
    "print \"average euclidean accuracy for threshold \" + str(thresholds[0])\n",
    "print max_euc_accs_mean\n",
    "max_euc_fscores_mean = sum(max_euc_fscores) / len(max_euc_fscores)\n",
    "print \"average euclidean fscore for threshold \" + str(thresholds[0])\n",
    "print max_euc_fscores_mean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Top Performing (th=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for random dataset 1\n",
      "euclidean distance results for the minority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.791\n",
      "Cosine distance results for the minority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.791\n",
      "data set 1 was tested!\n",
      "results for random dataset 2\n",
      "euclidean distance results for the minority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.79\n",
      "Cosine distance results for the minority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.79\n",
      "data set 2 was tested!\n",
      "results for random dataset 3\n",
      "euclidean distance results for the minority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.795\n",
      "Cosine distance results for the minority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.795\n",
      "data set 3 was tested!\n",
      "results for random dataset 4\n",
      "euclidean distance results for the minority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.785\n",
      "Cosine distance results for the minority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.785\n",
      "data set 4 was tested!\n",
      "results for random dataset 5\n",
      "euclidean distance results for the minority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.798\n",
      "Cosine distance results for the minority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.798\n",
      "data set 5 was tested!\n",
      "max cosine acc\n",
      "[0.791, 0.79, 0.795, 0.785, 0.798]\n",
      "average cosine accuracy for threshold 0.5\n",
      "0.7918\n",
      "average cosine fscore for threshold 0.5\n",
      "0.799451049525\n",
      "max euc acc\n",
      "[0.791, 0.79, 0.795, 0.785, 0.798]\n",
      "average euclidean accuracy for threshold 0.5\n",
      "0.7918\n",
      "average euclidean fscore for threshold 0.5\n",
      "0.799451049525\n"
     ]
    }
   ],
   "source": [
    "#inverted weights\n",
    "## Top performing\n",
    "from collections import defaultdict\n",
    "counter=0\n",
    "max_euc_accs=[]\n",
    "max_cosine_accs=[]\n",
    "max_cosine_fscores=[]\n",
    "max_euc_fscores=[]\n",
    "eucthresholds_accs = defaultdict(list)\n",
    "eucthresholds_fscores = defaultdict(list)\n",
    "cosinethresholds_accs = defaultdict(list)\n",
    "cosinethresholds_fscores = defaultdict(list)\n",
    "\n",
    "euc_threshold_acc = defaultdict(list)\n",
    "cosine_threshold_acc = defaultdict(list)\n",
    "\n",
    "euc_threshold_fscore = defaultdict(list)\n",
    "cosine_threshold_fscore = defaultdict(list)\n",
    "for d in range(1,6):\n",
    "    i=d\n",
    "    counter+=1\n",
    "    if i==1:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions')\n",
    "    else:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions_'+str(i))\n",
    "        \n",
    "    print \"results for random dataset \"+str(d)\n",
    "    print \"euclidean distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    largestdistances=[234.77861912874434,233.25736858671797,231.57720095035262,231.27689032845456,229.1331490640322,228.10962276940444]\n",
    "    lowestdistances=[156.70035098875815,214.9116097375849,215.7683943491261,223.29576798497547,225.03333086456325,225.27094797154825]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "\n",
    "    normalized_largestdistances_inverted = []\n",
    "    normalized_smallestdistances_inverted = []\n",
    "    for n in normalized_largestdistances:\n",
    "        normalized_largestdistances_inverted.append(1-n)\n",
    "    \n",
    "    for n in normalized_smallestdistances:\n",
    "        normalized_smallestdistances_inverted.append(1-n)\n",
    "    thresholds = [0.5]\n",
    "    euc_Fscores = []\n",
    "    euc_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "\n",
    "    print \"euclidean distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    euc_major_Fscores=[]\n",
    "    euc_major_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances_inverted,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    print \"euclidean distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    #threshold using all models\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print \"max euc acc\"\n",
    "    print max(euc_Accs)\n",
    "    max_euc_accs.append(max(euc_Accs))\n",
    "    max_euc_fscores.append(max(euc_Fscores))\n",
    "    largestdistances=[0.7749332064910006,0.7491597026757475,0.7236390000090482,0.7193428294891278,0.6904890029305283,0.6777351856726836]\n",
    "    lowestdistances=[0.23901334588341028,0.5475787563806471,0.5546099411237762,0.6240552059503661, 0.6424281501211625,0.6279728206924405]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "\n",
    "    normalized_largestdistances_inverted = []\n",
    "    normalized_smallestdistances_inverted = []\n",
    "    for n in normalized_largestdistances:\n",
    "        normalized_largestdistances_inverted.append(1-n)\n",
    "    \n",
    "    for n in normalized_smallestdistances:\n",
    "        normalized_smallestdistances_inverted.append(1-n)\n",
    "    print \"Cosine distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    cosine_Accs=[]\n",
    "    cosine_Fscores=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "    \n",
    "    print \"Cosine distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances_inverted,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    \n",
    "    print \"Cosine distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    \n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        print \"max euc acc\"\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print max(cosine_Accs)\n",
    "    max_cosine_accs.append(max(cosine_Accs))\n",
    "    max_cosine_fscores.append(max(cosine_Fscores))\n",
    "    print \"data set \"+str(counter)+' was tested!'\n",
    "print \"max cosine acc\"\n",
    "print max_cosine_accs\n",
    "max_cosine_accs_mean = sum(max_cosine_accs) / len(max_cosine_accs)\n",
    "print \"average cosine accuracy for threshold \" + str(thresholds[0])\n",
    "print max_cosine_accs_mean\n",
    "max_cosine_fscores_mean = sum(max_cosine_fscores) / len(max_cosine_fscores)\n",
    "print \"average cosine fscore for threshold \" + str(thresholds[0])\n",
    "print max_cosine_fscores_mean\n",
    "\n",
    "print \"max euc acc\"\n",
    "print max_euc_accs\n",
    "max_euc_accs_mean = sum(max_euc_accs) / len(max_euc_accs)\n",
    "\n",
    "print \"average euclidean accuracy for threshold \" + str(thresholds[0])\n",
    "print max_euc_accs_mean\n",
    "max_euc_fscores_mean = sum(max_euc_fscores) / len(max_euc_fscores)\n",
    "print \"average euclidean fscore for threshold \" + str(thresholds[0])\n",
    "print max_euc_fscores_mean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least and Top Performing (th=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for random dataset 1\n",
      "euclidean distance results for the minority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.743\n",
      "Cosine distance results for the minority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.743\n",
      "data set 1 was tested!\n",
      "results for random dataset 2\n",
      "euclidean distance results for the minority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.72\n",
      "Cosine distance results for the minority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.72\n",
      "data set 2 was tested!\n",
      "results for random dataset 3\n",
      "euclidean distance results for the minority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.721\n",
      "Cosine distance results for the minority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.721\n",
      "data set 3 was tested!\n",
      "results for random dataset 4\n",
      "euclidean distance results for the minority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.723\n",
      "Cosine distance results for the minority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.723\n",
      "data set 4 was tested!\n",
      "results for random dataset 5\n",
      "euclidean distance results for the minority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.731\n",
      "Cosine distance results for the minority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.731\n",
      "data set 5 was tested!\n",
      "max cosine acc\n",
      "[0.743, 0.72, 0.721, 0.723, 0.731]\n",
      "average cosine accuracy for threshold 0.5\n",
      "0.7276\n",
      "average cosine fscore for threshold 0.5\n",
      "0.779722602196\n",
      "max euc acc\n",
      "[0.743, 0.72, 0.721, 0.723, 0.731]\n",
      "average euclidean accuracy for threshold 0.5\n",
      "0.7276\n",
      "average euclidean fscore for threshold 0.5\n",
      "0.779722602196\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Inverted weights\n",
    "\n",
    "from collections import defaultdict\n",
    "counter=0\n",
    "max_euc_accs=[]\n",
    "max_cosine_accs=[]\n",
    "max_cosine_fscores=[]\n",
    "max_euc_fscores=[]\n",
    "eucthresholds_accs = defaultdict(list)\n",
    "eucthresholds_fscores = defaultdict(list)\n",
    "cosinethresholds_accs = defaultdict(list)\n",
    "cosinethresholds_fscores = defaultdict(list)\n",
    "\n",
    "euc_threshold_acc = defaultdict(list)\n",
    "cosine_threshold_acc = defaultdict(list)\n",
    "\n",
    "euc_threshold_fscore = defaultdict(list)\n",
    "cosine_threshold_fscore = defaultdict(list)\n",
    "for d in range(1,6):\n",
    "    i=d\n",
    "    counter+=1\n",
    "    if i==1:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions')\n",
    "    else:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions_'+str(i))\n",
    "        \n",
    "    print \"results for random dataset \"+str(d)\n",
    "    print \"euclidean distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    largestdistances=[234.77861912874434,233.25736858671797,231.57720095035262,231.27689032845456,229.1331490640322,228.10962276940444]\n",
    "    lowestdistances=[156.70035098875815,214.9116097375849,215.7683943491261,223.29576798497547,225.03333086456325,225.27094797154825]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    normalized_largestdistances_inverted = []\n",
    "    normalized_smallestdistances_inverted = []\n",
    "    for n in normalized_largestdistances:\n",
    "        normalized_largestdistances_inverted.append(1-n)\n",
    "    \n",
    "    for n in normalized_smallestdistances:\n",
    "        normalized_smallestdistances_inverted.append(1-n)\n",
    "    thresholds = [0.5]\n",
    "    euc_Fscores = []\n",
    "    euc_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "\n",
    "    print \"euclidean distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    euc_major_Fscores=[]\n",
    "    euc_major_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    print \"euclidean distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    #threshold using all models\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances_inverted,normalized_largestdistances_inverted,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print \"max euc acc\"\n",
    "    print max(euc_Accs)\n",
    "    max_euc_accs.append(max(euc_Accs))\n",
    "    max_euc_fscores.append(max(euc_Fscores))\n",
    "    largestdistances=[0.7749332064910006,0.7491597026757475,0.7236390000090482,0.7193428294891278,0.6904890029305283,0.6777351856726836]\n",
    "    lowestdistances=[0.23901334588341028,0.5475787563806471,0.5546099411237762,0.6240552059503661, 0.6424281501211625,0.6279728206924405]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    normalized_largestdistances_inverted = []\n",
    "    normalized_smallestdistances_inverted = []\n",
    "    for n in normalized_largestdistances:\n",
    "        normalized_largestdistances_inverted.append(1-n)\n",
    "    \n",
    "    for n in normalized_smallestdistances:\n",
    "        normalized_smallestdistances_inverted.append(1-n)\n",
    "    print \"Cosine distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    cosine_Accs=[]\n",
    "    cosine_Fscores=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "    \n",
    "    print \"Cosine distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    \n",
    "    print \"Cosine distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    \n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances_inverted,normalized_largestdistances_inverted,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        print \"max euc acc\"\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print max(cosine_Accs)\n",
    "    max_cosine_accs.append(max(cosine_Accs))\n",
    "    max_cosine_fscores.append(max(cosine_Fscores))\n",
    "    print \"data set \"+str(counter)+' was tested!'\n",
    "print \"max cosine acc\"\n",
    "print max_cosine_accs\n",
    "max_cosine_accs_mean = sum(max_cosine_accs) / len(max_cosine_accs)\n",
    "print \"average cosine accuracy for threshold \" + str(thresholds[0])\n",
    "print max_cosine_accs_mean\n",
    "max_cosine_fscores_mean = sum(max_cosine_fscores) / len(max_cosine_fscores)\n",
    "print \"average cosine fscore for threshold \" + str(thresholds[0])\n",
    "print max_cosine_fscores_mean\n",
    "\n",
    "print \"max euc acc\"\n",
    "print max_euc_accs\n",
    "max_euc_accs_mean = sum(max_euc_accs) / len(max_euc_accs)\n",
    "\n",
    "print \"average euclidean accuracy for threshold \" + str(thresholds[0])\n",
    "print max_euc_accs_mean\n",
    "max_euc_fscores_mean = sum(max_euc_fscores) / len(max_euc_fscores)\n",
    "print \"average euclidean fscore for threshold \" + str(thresholds[0])\n",
    "print max_euc_fscores_mean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Least Performing (th=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for random dataset 1\n",
      "euclidean distance results for the minority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.503\n",
      "Cosine distance results for the minority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.503\n",
      "data set 1 was tested!\n",
      "results for random dataset 2\n",
      "euclidean distance results for the minority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.502\n",
      "Cosine distance results for the minority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.502\n",
      "data set 2 was tested!\n",
      "results for random dataset 3\n",
      "euclidean distance results for the minority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.502\n",
      "Cosine distance results for the minority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.502\n",
      "data set 3 was tested!\n",
      "results for random dataset 4\n",
      "euclidean distance results for the minority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.501\n",
      "Cosine distance results for the minority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.501\n",
      "data set 4 was tested!\n",
      "results for random dataset 5\n",
      "euclidean distance results for the minority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.502\n",
      "Cosine distance results for the minority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.502\n",
      "data set 5 was tested!\n",
      "max cosine acc\n",
      "[0.503, 0.502, 0.502, 0.501, 0.502]\n",
      "average cosine accuracy for threshold 0.6\n",
      "0.502\n",
      "average cosine fscore for threshold 0.6\n",
      "0.667556861317\n",
      "max euc acc\n",
      "[0.503, 0.502, 0.502, 0.501, 0.502]\n",
      "average euclidean accuracy for threshold 0.6\n",
      "0.502\n",
      "average euclidean fscore for threshold 0.6\n",
      "0.667556861317\n"
     ]
    }
   ],
   "source": [
    "## inverted weights\n",
    "from collections import defaultdict\n",
    "counter=0\n",
    "max_euc_accs=[]\n",
    "max_cosine_accs=[]\n",
    "max_cosine_fscores=[]\n",
    "\n",
    "max_euc_fscores=[]\n",
    "eucthresholds_accs = defaultdict(list)\n",
    "eucthresholds_fscores = defaultdict(list)\n",
    "cosinethresholds_accs = defaultdict(list)\n",
    "cosinethresholds_fscores = defaultdict(list)\n",
    "\n",
    "euc_threshold_acc = defaultdict(list)\n",
    "cosine_threshold_acc = defaultdict(list)\n",
    "\n",
    "euc_threshold_fscore = defaultdict(list)\n",
    "cosine_threshold_fscore = defaultdict(list)\n",
    "for d in range(1,6):\n",
    "    i=d\n",
    "    counter+=1\n",
    "    if i==1:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions')\n",
    "    else:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions_'+str(i))\n",
    "        \n",
    "    print \"results for random dataset \"+str(d)\n",
    "    print \"euclidean distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    largestdistances=[234.77861912874434,233.25736858671797,231.57720095035262,231.27689032845456,229.1331490640322,228.10962276940444]\n",
    "    lowestdistances=[156.70035098875815,214.9116097375849,215.7683943491261,223.29576798497547,225.03333086456325,225.27094797154825]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    normalized_largestdistances_inverted = [0] * len(normalized_largestdistances)\n",
    "    normalized_smallestdistances_inverted = [0] * len(normalized_smallestdistances)\n",
    "    for i in range(len(normalized_largestdistances)):\n",
    "        normalized_largestdistances_inverted[i] = 1 - normalized_largestdistances[i]\n",
    "    \n",
    "    for j in range(len(normalized_smallestdistances)):\n",
    "        normalized_smallestdistances_inverted[j] = 1 - normalized_smallestdistances[j]\n",
    "    thresholds = [0.6]\n",
    "    euc_Fscores = []\n",
    "    euc_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances_inverted,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "\n",
    "    print \"euclidean distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    euc_major_Fscores=[]\n",
    "    euc_major_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    print \"euclidean distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    #threshold using all models\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print \"max euc acc\"\n",
    "    print max(euc_Accs)\n",
    "    max_euc_accs.append(max(euc_Accs))\n",
    "    max_euc_fscores.append(max(euc_Fscores))\n",
    "    largestdistances=[0.7749332064910006,0.7491597026757475,0.7236390000090482,0.7193428294891278,0.6904890029305283,0.6777351856726836]\n",
    "    lowestdistances=[0.23901334588341028,0.5475787563806471,0.5546099411237762,0.6240552059503661, 0.6424281501211625,0.6279728206924405]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    normalized_largestdistances_inverted = [0] * len(normalized_largestdistances)\n",
    "    normalized_smallestdistances_inverted = [0] * len(normalized_smallestdistances)\n",
    "    for i in range(len(normalized_largestdistances)):\n",
    "        normalized_largestdistances_inverted[i] = 1 - normalized_largestdistances[i]\n",
    "    \n",
    "    for j in range(len(normalized_smallestdistances)):\n",
    "        normalized_smallestdistances_inverted[j] = 1 - normalized_smallestdistances[j]\n",
    "    print \"Cosine distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    cosine_Accs=[]\n",
    "    cosine_Fscores=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances_inverted,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "    \n",
    "    print \"Cosine distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    \n",
    "    print \"Cosine distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    \n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        print \"max euc acc\"\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print max(cosine_Accs)\n",
    "    max_cosine_accs.append(max(cosine_Accs))\n",
    "    max_cosine_fscores.append(max(cosine_Fscores))\n",
    "    print \"data set \"+str(counter)+' was tested!'\n",
    "print \"max cosine acc\"\n",
    "print max_cosine_accs\n",
    "max_cosine_accs_mean = sum(max_cosine_accs) / len(max_cosine_accs)\n",
    "print \"average cosine accuracy for threshold \" + str(thresholds[0])\n",
    "print max_cosine_accs_mean\n",
    "max_cosine_fscores_mean = sum(max_cosine_fscores) / len(max_cosine_fscores)\n",
    "print \"average cosine fscore for threshold \" + str(thresholds[0])\n",
    "print max_cosine_fscores_mean\n",
    "\n",
    "print \"max euc acc\"\n",
    "print max_euc_accs\n",
    "max_euc_accs_mean = sum(max_euc_accs) / len(max_euc_accs)\n",
    "\n",
    "print \"average euclidean accuracy for threshold \" + str(thresholds[0])\n",
    "print max_euc_accs_mean\n",
    "max_euc_fscores_mean = sum(max_euc_fscores) / len(max_euc_fscores)\n",
    "print \"average euclidean fscore for threshold \" + str(thresholds[0])\n",
    "print max_euc_fscores_mean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Top Performing (th=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for random dataset 1\n",
      "euclidean distance results for the minority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.777\n",
      "Cosine distance results for the minority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.777\n",
      "data set 1 was tested!\n",
      "results for random dataset 2\n",
      "euclidean distance results for the minority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.774\n",
      "Cosine distance results for the minority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.774\n",
      "data set 2 was tested!\n",
      "results for random dataset 3\n",
      "euclidean distance results for the minority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.779\n",
      "Cosine distance results for the minority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.779\n",
      "data set 3 was tested!\n",
      "results for random dataset 4\n",
      "euclidean distance results for the minority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.766\n",
      "Cosine distance results for the minority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.766\n",
      "data set 4 was tested!\n",
      "results for random dataset 5\n",
      "euclidean distance results for the minority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.766\n",
      "Cosine distance results for the minority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.766\n",
      "data set 5 was tested!\n",
      "max cosine acc\n",
      "[0.777, 0.774, 0.779, 0.766, 0.766]\n",
      "average cosine accuracy for threshold 0.6\n",
      "0.7724\n",
      "average cosine fscore for threshold 0.6\n",
      "0.762350276987\n",
      "max euc acc\n",
      "[0.777, 0.774, 0.779, 0.766, 0.766]\n",
      "average euclidean accuracy for threshold 0.6\n",
      "0.7724\n",
      "average euclidean fscore for threshold 0.6\n",
      "0.762350276987\n"
     ]
    }
   ],
   "source": [
    "#inverted weights\n",
    "## Top performing\n",
    "from collections import defaultdict\n",
    "counter=0\n",
    "max_euc_accs=[]\n",
    "max_cosine_accs=[]\n",
    "max_cosine_fscores=[]\n",
    "max_euc_fscores=[]\n",
    "eucthresholds_accs = defaultdict(list)\n",
    "eucthresholds_fscores = defaultdict(list)\n",
    "cosinethresholds_accs = defaultdict(list)\n",
    "cosinethresholds_fscores = defaultdict(list)\n",
    "\n",
    "euc_threshold_acc = defaultdict(list)\n",
    "cosine_threshold_acc = defaultdict(list)\n",
    "\n",
    "euc_threshold_fscore = defaultdict(list)\n",
    "cosine_threshold_fscore = defaultdict(list)\n",
    "for d in range(1,6):\n",
    "    i=d\n",
    "    counter+=1\n",
    "    if i==1:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions')\n",
    "    else:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions_'+str(i))\n",
    "        \n",
    "    print \"results for random dataset \"+str(d)\n",
    "    print \"euclidean distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    largestdistances=[234.77861912874434,233.25736858671797,231.57720095035262,231.27689032845456,229.1331490640322,228.10962276940444]\n",
    "    lowestdistances=[156.70035098875815,214.9116097375849,215.7683943491261,223.29576798497547,225.03333086456325,225.27094797154825]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "\n",
    "    normalized_largestdistances_inverted = []\n",
    "    normalized_smallestdistances_inverted = []\n",
    "    for n in normalized_largestdistances:\n",
    "        normalized_largestdistances_inverted.append(1-n)\n",
    "    \n",
    "    for n in normalized_smallestdistances:\n",
    "        normalized_smallestdistances_inverted.append(1-n)\n",
    "    thresholds = [0.6]\n",
    "    euc_Fscores = []\n",
    "    euc_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "\n",
    "    print \"euclidean distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    euc_major_Fscores=[]\n",
    "    euc_major_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances_inverted,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    print \"euclidean distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    #threshold using all models\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print \"max euc acc\"\n",
    "    print max(euc_Accs)\n",
    "    max_euc_accs.append(max(euc_Accs))\n",
    "    max_euc_fscores.append(max(euc_Fscores))\n",
    "    largestdistances=[0.7749332064910006,0.7491597026757475,0.7236390000090482,0.7193428294891278,0.6904890029305283,0.6777351856726836]\n",
    "    lowestdistances=[0.23901334588341028,0.5475787563806471,0.5546099411237762,0.6240552059503661, 0.6424281501211625,0.6279728206924405]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "\n",
    "    normalized_largestdistances_inverted = []\n",
    "    normalized_smallestdistances_inverted = []\n",
    "    for n in normalized_largestdistances:\n",
    "        normalized_largestdistances_inverted.append(1-n)\n",
    "    \n",
    "    for n in normalized_smallestdistances:\n",
    "        normalized_smallestdistances_inverted.append(1-n)\n",
    "    print \"Cosine distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    cosine_Accs=[]\n",
    "    cosine_Fscores=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "    \n",
    "    print \"Cosine distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances_inverted,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    \n",
    "    print \"Cosine distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    \n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        print \"max euc acc\"\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print max(cosine_Accs)\n",
    "    max_cosine_accs.append(max(cosine_Accs))\n",
    "    max_cosine_fscores.append(max(cosine_Fscores))\n",
    "    print \"data set \"+str(counter)+' was tested!'\n",
    "print \"max cosine acc\"\n",
    "print max_cosine_accs\n",
    "max_cosine_accs_mean = sum(max_cosine_accs) / len(max_cosine_accs)\n",
    "print \"average cosine accuracy for threshold \" + str(thresholds[0])\n",
    "print max_cosine_accs_mean\n",
    "max_cosine_fscores_mean = sum(max_cosine_fscores) / len(max_cosine_fscores)\n",
    "print \"average cosine fscore for threshold \" + str(thresholds[0])\n",
    "print max_cosine_fscores_mean\n",
    "\n",
    "print \"max euc acc\"\n",
    "print max_euc_accs\n",
    "max_euc_accs_mean = sum(max_euc_accs) / len(max_euc_accs)\n",
    "\n",
    "print \"average euclidean accuracy for threshold \" + str(thresholds[0])\n",
    "print max_euc_accs_mean\n",
    "max_euc_fscores_mean = sum(max_euc_fscores) / len(max_euc_fscores)\n",
    "print \"average euclidean fscore for threshold \" + str(thresholds[0])\n",
    "print max_euc_fscores_mean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least and Top Performing (th=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for random dataset 1\n",
      "euclidean distance results for the minority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.781\n",
      "Cosine distance results for the minority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.795\n",
      "data set 1 was tested!\n",
      "results for random dataset 2\n",
      "euclidean distance results for the minority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.769\n",
      "Cosine distance results for the minority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.775\n",
      "data set 2 was tested!\n",
      "results for random dataset 3\n",
      "euclidean distance results for the minority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.773\n",
      "Cosine distance results for the minority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.779\n",
      "data set 3 was tested!\n",
      "results for random dataset 4\n",
      "euclidean distance results for the minority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.774\n",
      "Cosine distance results for the minority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.786\n",
      "data set 4 was tested!\n",
      "results for random dataset 5\n",
      "euclidean distance results for the minority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.785\n",
      "Cosine distance results for the minority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.792\n",
      "data set 5 was tested!\n",
      "max cosine acc\n",
      "[0.795, 0.775, 0.779, 0.786, 0.792]\n",
      "average cosine accuracy for threshold 0.6\n",
      "0.7854\n",
      "average cosine fscore for threshold 0.6\n",
      "0.811763644557\n",
      "max euc acc\n",
      "[0.781, 0.769, 0.773, 0.774, 0.785]\n",
      "average euclidean accuracy for threshold 0.6\n",
      "0.7764\n",
      "average euclidean fscore for threshold 0.6\n",
      "0.796014688644\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Inverted weights\n",
    "\n",
    "from collections import defaultdict\n",
    "counter=0\n",
    "max_euc_accs=[]\n",
    "max_cosine_accs=[]\n",
    "max_cosine_fscores=[]\n",
    "max_euc_fscores=[]\n",
    "eucthresholds_accs = defaultdict(list)\n",
    "eucthresholds_fscores = defaultdict(list)\n",
    "cosinethresholds_accs = defaultdict(list)\n",
    "cosinethresholds_fscores = defaultdict(list)\n",
    "\n",
    "euc_threshold_acc = defaultdict(list)\n",
    "cosine_threshold_acc = defaultdict(list)\n",
    "\n",
    "euc_threshold_fscore = defaultdict(list)\n",
    "cosine_threshold_fscore = defaultdict(list)\n",
    "for d in range(1,6):\n",
    "    i=d\n",
    "    counter+=1\n",
    "    if i==1:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions')\n",
    "    else:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions_'+str(i))\n",
    "        \n",
    "    print \"results for random dataset \"+str(d)\n",
    "    print \"euclidean distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    largestdistances=[234.77861912874434,233.25736858671797,231.57720095035262,231.27689032845456,229.1331490640322,228.10962276940444]\n",
    "    lowestdistances=[156.70035098875815,214.9116097375849,215.7683943491261,223.29576798497547,225.03333086456325,225.27094797154825]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    normalized_largestdistances_inverted = []\n",
    "    normalized_smallestdistances_inverted = []\n",
    "    for n in normalized_largestdistances:\n",
    "        normalized_largestdistances_inverted.append(1-n)\n",
    "    \n",
    "    for n in normalized_smallestdistances:\n",
    "        normalized_smallestdistances_inverted.append(1-n)\n",
    "    thresholds = [0.6]\n",
    "    euc_Fscores = []\n",
    "    euc_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "\n",
    "    print \"euclidean distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    euc_major_Fscores=[]\n",
    "    euc_major_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    print \"euclidean distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    #threshold using all models\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances_inverted,normalized_largestdistances_inverted,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print \"max euc acc\"\n",
    "    print max(euc_Accs)\n",
    "    max_euc_accs.append(max(euc_Accs))\n",
    "    max_euc_fscores.append(max(euc_Fscores))\n",
    "    largestdistances=[0.7749332064910006,0.7491597026757475,0.7236390000090482,0.7193428294891278,0.6904890029305283,0.6777351856726836]\n",
    "    lowestdistances=[0.23901334588341028,0.5475787563806471,0.5546099411237762,0.6240552059503661, 0.6424281501211625,0.6279728206924405]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    normalized_largestdistances_inverted = []\n",
    "    normalized_smallestdistances_inverted = []\n",
    "    for n in normalized_largestdistances:\n",
    "        normalized_largestdistances_inverted.append(1-n)\n",
    "    \n",
    "    for n in normalized_smallestdistances:\n",
    "        normalized_smallestdistances_inverted.append(1-n)\n",
    "    print \"Cosine distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    cosine_Accs=[]\n",
    "    cosine_Fscores=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "    \n",
    "    print \"Cosine distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    \n",
    "    print \"Cosine distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    \n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances_inverted,normalized_largestdistances_inverted,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        print \"max euc acc\"\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print max(cosine_Accs)\n",
    "    max_cosine_accs.append(max(cosine_Accs))\n",
    "    max_cosine_fscores.append(max(cosine_Fscores))\n",
    "    print \"data set \"+str(counter)+' was tested!'\n",
    "print \"max cosine acc\"\n",
    "print max_cosine_accs\n",
    "max_cosine_accs_mean = sum(max_cosine_accs) / len(max_cosine_accs)\n",
    "print \"average cosine accuracy for threshold \" + str(thresholds[0])\n",
    "print max_cosine_accs_mean\n",
    "max_cosine_fscores_mean = sum(max_cosine_fscores) / len(max_cosine_fscores)\n",
    "print \"average cosine fscore for threshold \" + str(thresholds[0])\n",
    "print max_cosine_fscores_mean\n",
    "\n",
    "print \"max euc acc\"\n",
    "print max_euc_accs\n",
    "max_euc_accs_mean = sum(max_euc_accs) / len(max_euc_accs)\n",
    "\n",
    "print \"average euclidean accuracy for threshold \" + str(thresholds[0])\n",
    "print max_euc_accs_mean\n",
    "max_euc_fscores_mean = sum(max_euc_fscores) / len(max_euc_fscores)\n",
    "print \"average euclidean fscore for threshold \" + str(thresholds[0])\n",
    "print max_euc_fscores_mean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Performing (th=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for random dataset 1\n",
      "euclidean distance results for the minority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.584\n",
      "Cosine distance results for the minority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.584\n",
      "data set 1 was tested!\n",
      "results for random dataset 2\n",
      "euclidean distance results for the minority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.591\n",
      "Cosine distance results for the minority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.591\n",
      "data set 2 was tested!\n",
      "results for random dataset 3\n",
      "euclidean distance results for the minority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.579\n",
      "Cosine distance results for the minority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.579\n",
      "data set 3 was tested!\n",
      "results for random dataset 4\n",
      "euclidean distance results for the minority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.582\n",
      "Cosine distance results for the minority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.582\n",
      "data set 4 was tested!\n",
      "results for random dataset 5\n",
      "euclidean distance results for the minority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.592\n",
      "Cosine distance results for the minority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.592\n",
      "data set 5 was tested!\n",
      "max cosine acc\n",
      "[0.584, 0.591, 0.579, 0.582, 0.592]\n",
      "average cosine accuracy for threshold 0.7\n",
      "0.5856\n",
      "average cosine fscore for threshold 0.7\n",
      "0.697877391697\n",
      "max euc acc\n",
      "[0.584, 0.591, 0.579, 0.582, 0.592]\n",
      "average euclidean accuracy for threshold 0.7\n",
      "0.5856\n",
      "average euclidean fscore for threshold 0.7\n",
      "0.697877391697\n"
     ]
    }
   ],
   "source": [
    "## inverted weights\n",
    "from collections import defaultdict\n",
    "counter=0\n",
    "max_euc_accs=[]\n",
    "max_cosine_accs=[]\n",
    "max_cosine_fscores=[]\n",
    "max_euc_fscores=[]\n",
    "eucthresholds_accs = defaultdict(list)\n",
    "eucthresholds_fscores = defaultdict(list)\n",
    "cosinethresholds_accs = defaultdict(list)\n",
    "cosinethresholds_fscores = defaultdict(list)\n",
    "\n",
    "euc_threshold_acc = defaultdict(list)\n",
    "cosine_threshold_acc = defaultdict(list)\n",
    "\n",
    "euc_threshold_fscore = defaultdict(list)\n",
    "cosine_threshold_fscore = defaultdict(list)\n",
    "for d in range(1,6):\n",
    "    i=d\n",
    "    counter+=1\n",
    "    if i==1:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions')\n",
    "    else:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions_'+str(i))\n",
    "        \n",
    "    print \"results for random dataset \"+str(d)\n",
    "    print \"euclidean distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    largestdistances=[234.77861912874434,233.25736858671797,231.57720095035262,231.27689032845456,229.1331490640322,228.10962276940444]\n",
    "    lowestdistances=[156.70035098875815,214.9116097375849,215.7683943491261,223.29576798497547,225.03333086456325,225.27094797154825]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    normalized_largestdistances_inverted = [0] * len(normalized_largestdistances)\n",
    "    normalized_smallestdistances_inverted = [0] * len(normalized_smallestdistances)\n",
    "    for i in range(len(normalized_largestdistances)):\n",
    "        normalized_largestdistances_inverted[i] = 1 - normalized_largestdistances[i]\n",
    "    \n",
    "    for j in range(len(normalized_smallestdistances)):\n",
    "        normalized_smallestdistances_inverted[j] = 1 - normalized_smallestdistances[j]\n",
    "    thresholds = [0.7]\n",
    "    euc_Fscores = []\n",
    "    euc_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances_inverted,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "\n",
    "    print \"euclidean distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    euc_major_Fscores=[]\n",
    "    euc_major_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    print \"euclidean distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    #threshold using all models\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print \"max euc acc\"\n",
    "    print max(euc_Accs)\n",
    "    max_euc_accs.append(max(euc_Accs))\n",
    "    max_euc_fscores.append(max(euc_Fscores))\n",
    "    largestdistances=[0.7749332064910006,0.7491597026757475,0.7236390000090482,0.7193428294891278,0.6904890029305283,0.6777351856726836]\n",
    "    lowestdistances=[0.23901334588341028,0.5475787563806471,0.5546099411237762,0.6240552059503661, 0.6424281501211625,0.6279728206924405]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    normalized_largestdistances_inverted = [0] * len(normalized_largestdistances)\n",
    "    normalized_smallestdistances_inverted = [0] * len(normalized_smallestdistances)\n",
    "    for i in range(len(normalized_largestdistances)):\n",
    "        normalized_largestdistances_inverted[i] = 1 - normalized_largestdistances[i]\n",
    "    \n",
    "    for j in range(len(normalized_smallestdistances)):\n",
    "        normalized_smallestdistances_inverted[j] = 1 - normalized_smallestdistances[j]\n",
    "    print \"Cosine distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    cosine_Accs=[]\n",
    "    cosine_Fscores=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances_inverted,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "    \n",
    "    print \"Cosine distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    \n",
    "    print \"Cosine distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    \n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        print \"max euc acc\"\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print max(cosine_Accs)\n",
    "    max_cosine_accs.append(max(cosine_Accs))\n",
    "    max_cosine_fscores.append(max(cosine_Fscores))\n",
    "    print \"data set \"+str(counter)+' was tested!'\n",
    "print \"max cosine acc\"\n",
    "print max_cosine_accs\n",
    "max_cosine_accs_mean = sum(max_cosine_accs) / len(max_cosine_accs)\n",
    "print \"average cosine accuracy for threshold \" + str(thresholds[0])\n",
    "print max_cosine_accs_mean\n",
    "max_cosine_fscores_mean = sum(max_cosine_fscores) / len(max_cosine_fscores)\n",
    "print \"average cosine fscore for threshold \" + str(thresholds[0])\n",
    "print max_cosine_fscores_mean\n",
    "\n",
    "print \"max euc acc\"\n",
    "print max_euc_accs\n",
    "max_euc_accs_mean = sum(max_euc_accs) / len(max_euc_accs)\n",
    "\n",
    "print \"average euclidean accuracy for threshold \" + str(thresholds[0])\n",
    "print max_euc_accs_mean\n",
    "max_euc_fscores_mean = sum(max_euc_fscores) / len(max_euc_fscores)\n",
    "print \"average euclidean fscore for threshold \" + str(thresholds[0])\n",
    "print max_euc_fscores_mean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Performing (th=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for random dataset 1\n",
      "euclidean distance results for the minority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.759\n",
      "Cosine distance results for the minority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.761\n",
      "data set 1 was tested!\n",
      "results for random dataset 2\n",
      "euclidean distance results for the minority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.747\n",
      "Cosine distance results for the minority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.744\n",
      "data set 2 was tested!\n",
      "results for random dataset 3\n",
      "euclidean distance results for the minority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.744\n",
      "Cosine distance results for the minority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.75\n",
      "data set 3 was tested!\n",
      "results for random dataset 4\n",
      "euclidean distance results for the minority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.739\n",
      "Cosine distance results for the minority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.747\n",
      "data set 4 was tested!\n",
      "results for random dataset 5\n",
      "euclidean distance results for the minority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.74\n",
      "Cosine distance results for the minority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.745\n",
      "data set 5 was tested!\n",
      "max cosine acc\n",
      "[0.761, 0.744, 0.75, 0.747, 0.745]\n",
      "average cosine accuracy for threshold 0.7\n",
      "0.7494\n",
      "average cosine fscore for threshold 0.7\n",
      "0.701657965754\n",
      "max euc acc\n",
      "[0.759, 0.747, 0.744, 0.739, 0.74]\n",
      "average euclidean accuracy for threshold 0.7\n",
      "0.7458\n",
      "average euclidean fscore for threshold 0.7\n",
      "0.6892117759\n"
     ]
    }
   ],
   "source": [
    "#inverted weights\n",
    "## Top performing\n",
    "from collections import defaultdict\n",
    "counter=0\n",
    "max_euc_accs=[]\n",
    "max_cosine_accs=[]\n",
    "max_cosine_fscores=[]\n",
    "max_euc_fscores=[]\n",
    "eucthresholds_accs = defaultdict(list)\n",
    "eucthresholds_fscores = defaultdict(list)\n",
    "cosinethresholds_accs = defaultdict(list)\n",
    "cosinethresholds_fscores = defaultdict(list)\n",
    "\n",
    "euc_threshold_acc = defaultdict(list)\n",
    "cosine_threshold_acc = defaultdict(list)\n",
    "\n",
    "euc_threshold_fscore = defaultdict(list)\n",
    "cosine_threshold_fscore = defaultdict(list)\n",
    "for d in range(1,6):\n",
    "    i=d\n",
    "    counter+=1\n",
    "    if i==1:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions')\n",
    "    else:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions_'+str(i))\n",
    "        \n",
    "    print \"results for random dataset \"+str(d)\n",
    "    print \"euclidean distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    largestdistances=[234.77861912874434,233.25736858671797,231.57720095035262,231.27689032845456,229.1331490640322,228.10962276940444]\n",
    "    lowestdistances=[156.70035098875815,214.9116097375849,215.7683943491261,223.29576798497547,225.03333086456325,225.27094797154825]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "\n",
    "    normalized_largestdistances_inverted = []\n",
    "    normalized_smallestdistances_inverted = []\n",
    "    for n in normalized_largestdistances:\n",
    "        normalized_largestdistances_inverted.append(1-n)\n",
    "    \n",
    "    for n in normalized_smallestdistances:\n",
    "        normalized_smallestdistances_inverted.append(1-n)\n",
    "    thresholds = [0.7]\n",
    "    euc_Fscores = []\n",
    "    euc_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "\n",
    "    print \"euclidean distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    euc_major_Fscores=[]\n",
    "    euc_major_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances_inverted,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    print \"euclidean distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    #threshold using all models\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print \"max euc acc\"\n",
    "    print max(euc_Accs)\n",
    "    max_euc_accs.append(max(euc_Accs))\n",
    "    max_euc_fscores.append(max(euc_Fscores))\n",
    "    largestdistances=[0.7749332064910006,0.7491597026757475,0.7236390000090482,0.7193428294891278,0.6904890029305283,0.6777351856726836]\n",
    "    lowestdistances=[0.23901334588341028,0.5475787563806471,0.5546099411237762,0.6240552059503661, 0.6424281501211625,0.6279728206924405]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "\n",
    "    normalized_largestdistances_inverted = []\n",
    "    normalized_smallestdistances_inverted = []\n",
    "    for n in normalized_largestdistances:\n",
    "        normalized_largestdistances_inverted.append(1-n)\n",
    "    \n",
    "    for n in normalized_smallestdistances:\n",
    "        normalized_smallestdistances_inverted.append(1-n)\n",
    "    print \"Cosine distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    cosine_Accs=[]\n",
    "    cosine_Fscores=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "    \n",
    "    print \"Cosine distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances_inverted,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    \n",
    "    print \"Cosine distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    \n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances,normalized_largestdistances,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        print \"max euc acc\"\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print max(cosine_Accs)\n",
    "    max_cosine_accs.append(max(cosine_Accs))\n",
    "    max_cosine_fscores.append(max(cosine_Fscores))\n",
    "    print \"data set \"+str(counter)+' was tested!'\n",
    "print \"max cosine acc\"\n",
    "print max_cosine_accs\n",
    "max_cosine_accs_mean = sum(max_cosine_accs) / len(max_cosine_accs)\n",
    "print \"average cosine accuracy for threshold \" + str(thresholds[0])\n",
    "print max_cosine_accs_mean\n",
    "max_cosine_fscores_mean = sum(max_cosine_fscores) / len(max_cosine_fscores)\n",
    "print \"average cosine fscore for threshold \" + str(thresholds[0])\n",
    "print max_cosine_fscores_mean\n",
    "\n",
    "print \"max euc acc\"\n",
    "print max_euc_accs\n",
    "max_euc_accs_mean = sum(max_euc_accs) / len(max_euc_accs)\n",
    "\n",
    "print \"average euclidean accuracy for threshold \" + str(thresholds[0])\n",
    "print max_euc_accs_mean\n",
    "max_euc_fscores_mean = sum(max_euc_fscores) / len(max_euc_fscores)\n",
    "print \"average euclidean fscore for threshold \" + str(thresholds[0])\n",
    "print max_euc_fscores_mean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least and Top Performing (th=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results for random dataset 1\n",
      "euclidean distance results for the minority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority classifiers of random dataset 1\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.781\n",
      "Cosine distance results for the minority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority classifiers of random dataset 1\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 1\n",
      "max euc acc\n",
      "0.781\n",
      "data set 1 was tested!\n",
      "results for random dataset 2\n",
      "euclidean distance results for the minority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority classifiers of random dataset 2\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.775\n",
      "Cosine distance results for the minority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority classifiers of random dataset 2\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 2\n",
      "max euc acc\n",
      "0.775\n",
      "data set 2 was tested!\n",
      "results for random dataset 3\n",
      "euclidean distance results for the minority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority classifiers of random dataset 3\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.782\n",
      "Cosine distance results for the minority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority classifiers of random dataset 3\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 3\n",
      "max euc acc\n",
      "0.782\n",
      "data set 3 was tested!\n",
      "results for random dataset 4\n",
      "euclidean distance results for the minority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority classifiers of random dataset 4\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.772\n",
      "Cosine distance results for the minority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority classifiers of random dataset 4\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 4\n",
      "max euc acc\n",
      "0.772\n",
      "data set 4 was tested!\n",
      "results for random dataset 5\n",
      "euclidean distance results for the minority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority classifiers of random dataset 5\n",
      "euclidean distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.769\n",
      "Cosine distance results for the minority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority classifiers of random dataset 5\n",
      "Cosine distance results for the Majority-Minority classifiers of random dataset 5\n",
      "max euc acc\n",
      "0.769\n",
      "data set 5 was tested!\n",
      "max cosine acc\n",
      "[0.781, 0.775, 0.782, 0.772, 0.769]\n",
      "average cosine accuracy for threshold 0.7\n",
      "0.7758\n",
      "average cosine fscore for threshold 0.7\n",
      "0.763894191114\n",
      "max euc acc\n",
      "[0.781, 0.775, 0.782, 0.772, 0.769]\n",
      "average euclidean accuracy for threshold 0.7\n",
      "0.7758\n",
      "average euclidean fscore for threshold 0.7\n",
      "0.763894191114\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Inverted weights\n",
    "\n",
    "from collections import defaultdict\n",
    "counter=0\n",
    "max_euc_accs=[]\n",
    "max_cosine_accs=[]\n",
    "max_cosine_fscores=[]\n",
    "max_euc_fscores=[]\n",
    "eucthresholds_accs = defaultdict(list)\n",
    "eucthresholds_fscores = defaultdict(list)\n",
    "cosinethresholds_accs = defaultdict(list)\n",
    "cosinethresholds_fscores = defaultdict(list)\n",
    "\n",
    "euc_threshold_acc = defaultdict(list)\n",
    "cosine_threshold_acc = defaultdict(list)\n",
    "\n",
    "euc_threshold_fscore = defaultdict(list)\n",
    "cosine_threshold_fscore = defaultdict(list)\n",
    "for d in range(1,6):\n",
    "    i=d\n",
    "    counter+=1\n",
    "    if i==1:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions')\n",
    "    else:\n",
    "        bookstestgl = graphlab.SFrame('booksgl_random_predictions_'+str(i))\n",
    "        \n",
    "    print \"results for random dataset \"+str(d)\n",
    "    print \"euclidean distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    largestdistances=[234.77861912874434,233.25736858671797,231.57720095035262,231.27689032845456,229.1331490640322,228.10962276940444]\n",
    "    lowestdistances=[156.70035098875815,214.9116097375849,215.7683943491261,223.29576798497547,225.03333086456325,225.27094797154825]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    normalized_largestdistances_inverted = []\n",
    "    normalized_smallestdistances_inverted = []\n",
    "    for n in normalized_largestdistances:\n",
    "        normalized_largestdistances_inverted.append(1-n)\n",
    "    \n",
    "    for n in normalized_smallestdistances:\n",
    "        normalized_smallestdistances_inverted.append(1-n)\n",
    "    thresholds = [0.7]\n",
    "    euc_Fscores = []\n",
    "    euc_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "\n",
    "    print \"euclidean distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    euc_major_Fscores=[]\n",
    "    euc_major_Accs = []\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        ##euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    print \"euclidean distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "\n",
    "    #threshold using all models\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances_inverted,normalized_largestdistances_inverted,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        euc_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        euc_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print \"max euc acc\"\n",
    "    print max(euc_Accs)\n",
    "    max_euc_accs.append(max(euc_Accs))\n",
    "    max_euc_fscores.append(max(euc_Fscores))\n",
    "    largestdistances=[0.7749332064910006,0.7491597026757475,0.7236390000090482,0.7193428294891278,0.6904890029305283,0.6777351856726836]\n",
    "    lowestdistances=[0.23901334588341028,0.5475787563806471,0.5546099411237762,0.6240552059503661, 0.6424281501211625,0.6279728206924405]\n",
    "    Minor_domains = ['automotive_predictions','jewelrywatches_predictions','gourmet_predictions','grocery_predictions','beauty_predictions','baby_predictions']\n",
    "    Major_domains = ['dvd_predictions','electronics_predictions','kitchen_predictions','healthpersonal_predictions','cameraphoto_predictions','apparel_predictions']\n",
    "    normalized_largestdistances = euclidean_norm_dict(largestdistances)\n",
    "    normalized_smallestdistances = euclidean_norm_dict(lowestdistances)\n",
    "    normalized_largestdistances_inverted = []\n",
    "    normalized_smallestdistances_inverted = []\n",
    "    for n in normalized_largestdistances:\n",
    "        normalized_largestdistances_inverted.append(1-n)\n",
    "    \n",
    "    for n in normalized_smallestdistances:\n",
    "        normalized_smallestdistances_inverted.append(1-n)\n",
    "    print \"Cosine distance results for the minority classifiers of random dataset \"+str(d)\n",
    "    cosine_Accs=[]\n",
    "    cosine_Fscores=[]\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_largedist(x,normalized_largestdistances,threshold) )\n",
    "        #print \"minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        #print \"minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['minority_predictions'])\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['minority_predictions']))\n",
    "    \n",
    "    print \"Cosine distance results for the Majority classifiers of random dataset \"+str(d)\n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist(x,normalized_smallestdistances,threshold) )\n",
    "        #print \"majority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        #print \"majority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_predictions'])\n",
    "        ##cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "        ##cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_predictions']))\n",
    "    \n",
    "    \n",
    "    print \"Cosine distance results for the Majority-Minority classifiers of random dataset \"+str(d)\n",
    "    \n",
    "    \n",
    "    for i in range(len(thresholds)):\n",
    "        threshold=thresholds[i]\n",
    "        bookstestgl['majority_minority_predictions'] = bookstestgl.apply(lambda x: weighted_predict_smalldist_largedistances(x,normalized_smallestdistances_inverted,normalized_largestdistances_inverted,threshold) )\n",
    "\n",
    "        #print \"majority_minority_predictions' model Fscore when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.f1_score(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        #print \"majority_minority_predictions' model Accuracy when using a threshold of \"+str(thresholds[i])\n",
    "        #print graphlab.evaluation.accuracy(kitchentestgl['label'], kitchentestgl['majority_minority_predictions'])\n",
    "        print \"max euc acc\"\n",
    "        cosine_Accs.append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        cosine_Fscores.append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_acc[threshold].append(graphlab.evaluation.accuracy(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "        ##cosine_threshold_fscore[threshold].append(graphlab.evaluation.f1_score(bookstestgl['label'], bookstestgl['majority_minority_predictions']))\n",
    "    print max(cosine_Accs)\n",
    "    max_cosine_accs.append(max(cosine_Accs))\n",
    "    max_cosine_fscores.append(max(cosine_Fscores))\n",
    "    print \"data set \"+str(counter)+' was tested!'\n",
    "print \"max cosine acc\"\n",
    "print max_cosine_accs\n",
    "max_cosine_accs_mean = sum(max_cosine_accs) / len(max_cosine_accs)\n",
    "print \"average cosine accuracy for threshold \" + str(thresholds[0])\n",
    "print max_cosine_accs_mean\n",
    "max_cosine_fscores_mean = sum(max_cosine_fscores) / len(max_cosine_fscores)\n",
    "print \"average cosine fscore for threshold \" + str(thresholds[0])\n",
    "print max_cosine_fscores_mean\n",
    "\n",
    "print \"max euc acc\"\n",
    "print max_euc_accs\n",
    "max_euc_accs_mean = sum(max_euc_accs) / len(max_euc_accs)\n",
    "\n",
    "print \"average euclidean accuracy for threshold \" + str(thresholds[0])\n",
    "print max_euc_accs_mean\n",
    "max_euc_fscores_mean = sum(max_euc_fscores) / len(max_euc_fscores)\n",
    "print \"average euclidean fscore for threshold \" + str(thresholds[0])\n",
    "print max_euc_fscores_mean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
